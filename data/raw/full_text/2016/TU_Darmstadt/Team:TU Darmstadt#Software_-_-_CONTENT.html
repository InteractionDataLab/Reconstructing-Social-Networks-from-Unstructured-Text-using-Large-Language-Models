<HTML lang="en" dir="ltr" class="client-nojs">
<style type="text/css">
A:before { content:' '; } 
A:after { content:' '; } 
SPAN:before { content:' '; } 
SPAN:after { content:' '; } 
</style>
<BODY class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Team_TU_Darmstadt_Software skin-igem action-view"><DIV id="globalWrapper"><DIV id="content" class="mw-body" role="main"><DIV id="top_title"><H1 id="firstHeading" class="firstHeading"><SPAN dir="auto">Team:TU Darmstadt/Software</SPAN></H1></DIV><DIV id="HQ_page"><DIV id="bodyContent"><DIV id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><DIV class="js-message abstract">
    If you can see this message, you do not use Javascript. This Website is best to use with Javascript enabled. Without Javascript enabled, many features including the mobile version are not usable.
  </DIV><DIV class="vviki" id="vviki"><DIV id="head"><DIV class="navbar {{{extra-class}}}"><UL id="slick-menu"><LI><A href="https://2016.igem.org/Team:TU_Darmstadt/Achievements">Achievements</A></LI><LI><A href="https://2016.igem.org/Team:TU_Darmstadt/Lab">Wet Lab</A></LI><LI><A href="https://2016.igem.org/Team:TU_Darmstadt/Parts">Parts</A></LI><LI><A href="https://2016.igem.org/Team:TU_Darmstadt/Hardware">Robotics</A></LI><LI><A href="https://2016.igem.org/Team:TU_Darmstadt/Model">Modeling</A></LI><LI><A href="https://2016.igem.org/Team:TU_Darmstadt/Human_Practices">Human Practices</A></LI><LI><A href="https://2016.igem.org/Team:TU_Darmstadt/Collaborations">Collaborations</A></LI><LI><A href="https://2016.igem.org/Team:TU_Darmstadt/Notebook">Notebook</A></LI><LI><A href="https://2016.igem.org/Team:TU_Darmstadt/Results">Results</A></LI><LI><A href="https://2016.igem.org/Team:TU_Darmstadt/Team">Team</A></LI></UL></DIV><DIV id="mainHeader"><H1>ROBOTICS</H1></DIV></DIV><DIV class="page"><DIV class="abstract"><P><B>ABSTRACT</B><B>Our main task was to develop a device that measures mVenus fluorescence and adds liquids to sample containers.
           Therefore, our team decided to build a fully automatized pipetting robot that is able to locate a set of samples, detect potential light emission and pipet a specific amount of non-natural amino acid solution into the fluorescent sample.
            The foundation for the robot is a 3D-printer, due to the easy handling of movements in three dimensions. By controlling these movements with an optical system, the autonomy of the robot is further increased.</B></P></DIV><DIV class="content"><DIV class="verlinked" id="intro"><H5>INTRODUCTION</H5></DIV><H6>Development of 3D Printers &amp; Possibilities</H6>
              In the 1980s, Chuck Hull invented the first standardized 3D printer, based on a procedure which is known as stereolithography (SLA, <A href="#refe">[1]</A>). Moving from SLA to full deposit modeling (FDM) techniques, the 3D printing idea became alive in the do‑it‑yourself community. Ever since that time, simple 3D printers are accessible for little money and due to the open source idea of projects like REPRAP <A href="#refe">[2]</A> affordable for many. In last years project, iGEM TU Darmstadt has already built a fully working SLA printer, capable of being fed with biologically manufactured plastics <A href="#refe">[3]</A>.
              This year, the robotics team decided to rebuild a clone of the Ultimaker 2 FDM printer <A href="#refe">[4]</A> and exchange the extruder with a camera and a pipet to create a pipetting robot. Using several open‑source parts and software, it is the idea to establish an easy-to-handle robot to assist the daily biologist's work. 
            
              iGEM TU DARMSTADT is a young and dynamic team of interdisciplinary and motivated researchers. Our advantage is, that we can bring together synthetic biology and classic engineering sciences, for which TU Darmstadt is famous. We have the possibility, thanks to iGEM, to experiment on our own ideas and to reach for the stars. Being interested in a variety of scientific topics, we wanted to mix up different talents to create a unique project.

            <DIV class="verlinked" id="goals"><H5>GOALS</H5></DIV><P>
              The main task is to develop a machine which is capable to monitor our organisms and their health condition (encoded by fluorescence) in order to keep them alive. Therefore the machine has to measure the light emission of the organisms and needs to be able to drop liquids into sample containers. This has to be independent of the exact position of the container, which requires an automatic tracking system.
              The idea is that one places a container somewhere under the robot's working area and clicks a <I>run</I> button of a program. The robot starts its routine by tracking the new container and measuring the light emission of the organisms. Based on this measurement, the robot decides whether to feed the organisms with non‑natural amino acid solution or not. After a period of time it repeats this routine until the stop button of the program is clicked.
              These are only the minimum requirements for our project's needs. We decided to go one step further and designed our robot in such a way, that it serves as a multi‑purpose platform which is adaptable and easy to modify. The open‑source character invites other scientists to add new features or improve the robot and its capabilities.
              For example our dispensing system can be upgraded to be able to prepare 96-well plates with samples and monitor routines by using the optical system.
              Additionally, our measuring head can be changed back to a printer head which allows to 3D print with just a few changes.
              There is a vast room of possibilities, just using the concept of the accurate positioning of a sample in the 3D space.
              Due to the fact that we try to stick to widely used open-source software and standard commercial parts, our machine can be easily combined with the most DIY products, making it reusable, flexible and cheap.
              In the special case of the TU Darmstadt and the next generations of iGEM competitors, we had the idea to develop our technical equipment from year to year and, if possible, combine them. Our SLA printer from last year’s competition was upgraded and is nearly ready to use again, giving us the possibility to manufacture parts for prototyping in our lab. Also this year’s project will serve as starting point for the next year’s technical development team. New ideas and possibilities have been already discussed and we are looking forward to the next year’s competition.</P><DIV class="verlinked" id="setup"><H5>SETUP OVERVIEW</H5></DIV><CENTER><DIV class="this-is-printer"><SVG id="svg5230" xmlns:RDF="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 1866.8 2064.3" version="1.1" width="500px"><METADATA id="metadata5235"><RDF:RDF><CC:WORK rdf:about=""><DC:FORMAT>image/svg+xml</DC:FORMAT></CC:WORK></RDF:RDF></METADATA></SVG></DIV></CENTER><UL class="description-text" style="display:none;"><LI data-part="default">To get short information about our robot's single parts, click on the part of interest.</LI><LI data-part="x-y-axis">
                To realize the movement of the head in the x-y-plane, the head is mounted onto a 3D-printed connection element which is fitted between two crossing 6 mm aluminum linear rods. To ensure a smooth sliding of the connection on the linear rods, two <I>LM6UU</I> linear bearings were applied. Also, the crossing 6 mm aluminum linear rods are clipped on four <I>UM2 Ultimaker 2 Injection Sliding Blocks</I> which are themselves sliding on 8 mm steel rods. Two <I>F688ZZ</I> flanged ball bearings per 8 mm steel rod are plugged into <I>SK16</I> rod supports ensure that they are able to rotate with less friction. Furthermore, <I>GT2</I> timing belts transfer the stepper motor's rotation into a linear movement of the head. These timing belts connect the stepper motors with the 8 mm steel rods with the usage of <I>GT2</I> pulleys. Timing belts that are fixed in the sliding blocks range from one linear rod on one side of the robot to a parallel one on the opposing side. Therefore, an amount of two timing belts per axis and one timing belt per motor are necessary for a proper movement in the x‑y‑plane.
            </LI><LI data-part="z-axis">
                For the motion in z-direction, a threaded rod is coupled to a stepper motor with a <I>5‑to‑8
                 mm</I> shaft coupling and vertically mounted with two <I>KP08</I> pillow blocks. The rotation of the threaded rod is then transmitted into a vertical movement of the sample stage via a ball screw. For a greater stability and a more balanced force distribution, two additional 12 mm aluminum rods help to guide the vertical movement of the sample stage. Besides, one <I>LM12LUU</I> linear bearing per vertical axis is used for nearly frictionless motion and an even more balanced force effect on the vertical axis. The mounting for the latter is attached on the sample stage and particularly manufactured, again for a balanced force distribution. The sample stage itself consists of an aluminum framework with an infrared‑transparent Plexiglas. To minimize the weight of the sample plate, while keeping its stability, the thickness of the table's components was chosen to be 3 mm.
            </LI><LI data-part="optics">
                The main part of our optics is the combination of our lightbox, LEDs and a <I>Pi NoIR v2</I> camera. With these parts we detect flourescent light, after stimulated emission.
                The localization works with the lightbox via built‑in infrared LEDs whose light spreads inside a diffusion plate, generating an evenly distributed light source. Emerging shadows can be detected with the camera. The latter is attached to the head of the robot that additionally consists of optical components such as four high power LEDs, various filters, a lens mounting and a tip that is connected to the syringe pump. As already mentioned, the camera is in charge of detecting different sample locations and fluorescent light of the mVenus protein. While the high power LEDs ensure the excitation of the protein; the filters, that are composed of a longpass-filter and sunglasses, eliminate reflected light such that the camera only detects relevant signals. The lens mounting works as an autofocus that is able to control the sharpness of the camera. This works by applying different voltages to coils that move the lens inside a modified webcam.
            </LI><LI data-part="syringepump">
                The syringe pump is the device of the robot that feeds the bacteria with non-natural amino acid if it is necessary. Its setup is a stepper motor that squeezes the non-natural amino acid out of the syringe and a 3D-printed framework that holds the syringe. It also includes a threaded rod for transmission between the stepper motor and the syringe. To keep the whole setup at a fixed position and still pipet on the right sample, the end of the syringe is connected with the head by a flexible tube.
            </LI><LI data-part="chassis">
                The framework for the robot is built in a form of a simple cuboid constructed with 30 mm x 30 mm aluminum profiles. These profiles are connected with each other via M8 screws and drilled into ISO metric screw threads. Two extra horizontal aluminum profiles are added on the backside which hold the axes and the threaded rod needed for the vertical motion. Moreover, a thin aluminum frame is attached at the lower part of the 30 mm framework to mount the lightbox onto it.
            </LI><LI data-part="electronics">
                The electrical parts used in this robot can be separated into those that directly manage the threedimensional movements and those that are in charge of the optical localization and detection. The threedimensional movements are controlled by an <I>Arduino Mega 2560</I> microcontroller connected to a <I>RepRap Arduino Mega Pololu Shield 1.4</I> (short: <I>RAMPS 1.4</I>) which provides an intuitive connection of drivers and other electronical devices without using wires. The drivers used in this project are called <I>Pololu - DRV8825 Stepper Motor Driver Carrier</I> and allow the Arduino to let the stepper motors make a 1/32 step and therefore increases the spatial resolution. Additional parts involved in threedimensional movement are the so called endstops that assure that the head does not crash into the boundaries of the robot. The list of parts that control the optical localization and detection are comprised of a single‑board computer <I>Raspberry Pi 3</I> and controls several other electro‑optical components. All necessary power is delivered by a standard<I>ATX</I> power supply.        </LI></UL><DIV class="verlinked" id="func"><H5>FUNCTIONALITY</H5></DIV>
        The functionality of the pipetting robot comprises of the three‑dimensional agility of a 3D-printer and the possibility to pipet a specific amount of non‑natural amino acid using a syringe pump. Also it has intelligent visual object recognition so that it is able to distinguish between samples that require more non‑natural amino acid from samples that still contain a sufficient amount.
        With that said it is capable to autonomously keep alive the modified <I>E. coli</I> bacteria, given that it is activated and connected to a reliable power supply.
        To fulfil the task of keeping the bacteria alive it loops through a specially designed procedure. Initially, the robot scans the working area for samples by illuminating the downside of the sample stage using infrared LEDs and monitoring the shadows of the placed reservoirs with a camera. If the contrast is sufficiently high it is able to detect the edges of the mentioned reservoirs, fit a circle onto it and compute the distance between the reservoir and the camera itself. Furthermore it is possible to put an entire rack of reservoirs under observation due to its ability to locate every individual reservoir.
        Shortly after the detection, the distance information is sent to the 3D control program and the head of the robot moves in direction of the first reservoir. To check whether the bacteria needs more non‑natural amino acid the robot uses the fluorescence of the protein mVenus that is expressed by the bacteria. Therefore the robot excites the protein via high power LEDs and detects the emitted light. To exclude reflected light from the LEDs that would interfere with the measurement a longpass filter cuts off the spectrum below the emission peak of the protein. In dependence of the fluorescence signal, the robot decides whether it is necessary to pipet non‑natural amino acid onto the sample. If that is the case the robot moves the samples in z-range just so that the syringe reaches the sample and is able to securely add the non‑natural amino acid.
        Eventually the robot recommences the procedure described above, except for the scanning of the individual positions of the samples, which are saved temporarily until all samples are checked. As long as the robot is activated, connected to a power supply and the syringe pump does not run out on non‑natural amino acid, the robot will loop through this whole process and keep the bacteria alive without the need of a human.
        Nevertheless it is possible to check what the robot is doing via a livestream of the camera visible on a graphical user interface, since there is no other opportunity to look inside the robot itself while it is working.
        <DIV class="verlinked" id="achie"><H5>ACHIEVEMENTS</H5></DIV><P><UL style="list-style-type:square;"><LI>Successfully redesign a 3D printer chassis to meet our requirements</LI><LI>Construct a unique lightbox with integrated IR LEDs for positioning purposes</LI><LI>Design a measuring probe with a camera device with an integrated optical filter system and LEDs</LI><LI>Implementing an automatic object tracking system including a vector based feedback system for positioning</LI><LI>Construct a syringe pump system to add liquids down to microliter accuracy</LI><LI>Connecting a Raspberry Pi with an Arduino microcontroller by establishing a serial connection between the two devices, allowing a variety of different tasks</LI><LI>Data of all CADs designed by the TU Darmstadt technical department</LI><LI>A complete <A href="#build">construction tutorial</A> including a <A href="#bom">BOM</A> (Bill of Materials incl. prices)</LI></UL></P><DIV class="verlinked" id="results"><H5>RESULTS</H5><H6>Circle Detection</H6><CENTER><VIDEO width="600" controls=""><SOURCE src="https://static.igem.org/mediawiki/2016/0/07/T--TU_Darmstadt--Auto_tracking.mov" type="video/mp4">
  Your browser does not support HTML5 video.
</SOURCE></VIDEO></CENTER><P>The sample detection was facing two problems which on the first glance seem rather simple. Detect a circle and detect a rectangle. The detection of a circle is more easy due to the fact that it is a analytical function. This was already implemented by openCV and we were able to use the circle detection for our sample tracking system.
The green circles are the found objects. The red lines are vectors from the central position of our probe to the sample positions. They are used to drive the stepper motors so that our probe can move to them. You can also see, that changing the probes position are recognized and the vectors are recalculated.</P><H6>Multi-Object Detection</H6><P>
                A more delicate task was to detect round samples, stored in a rack. The difficulty is that the objects are intersecting and the algorithm needs to distinguish them somehow. You can see how the camera sees the whole object. Even with our eye it is difficult to see a rectangle. We improved the algorithm step by step using prominent features of the rack. The result is shown in the last picture.
            </P></DIV><H6>Rectangle Detection</H6><CENTER><VIDEO width="600" controls=""><SOURCE src="https://static.igem.org/mediawiki/2016/3/32/T--TU_Darmstadt--sample_tracking.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</SOURCE></VIDEO></CENTER><P>A rectangle detection is more complicated, but we were able to solve this challenge. You can see how the algorithm detect the rectangles and mark them, also tracking the single samples.</P><H6>Full Functionality</H6><CENTER><VIDEO width="600" controls=""><SOURCE src="https://static.igem.org/mediawiki/2016/6/67/T--TU_Darmstadt--finalfunct.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</SOURCE></VIDEO></CENTER><P>As shown in the video our robot starts with its routine and going from sample to sample, checking for light emission. For demonstration purposes we installed a LED light. One can see, that the optic detect the LED light and then moves down to dispense liquid into the sample, because our pipette is installed sideways on the optical head. Then its moving back to its original position and continues its routine. At the end of the video we show the possible adjustments like brightness and focus.</P><CENTER><VIDEO width="600" controls=""><SOURCE src="https://static.igem.org/mediawiki/2016/8/88/T--TU_Darmstadt--rectang.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</SOURCE></VIDEO></CENTER><P>In this video we additionally demonstrate the recognition of rectangles and circles. Using OpenCV and the Raspberry Pi Cam we can detect our glowing samples, here simulated with a green LED. After the detection of the glowing, in the case of a fluorescent protein induced by the blue high power LEDs, we pipet a drop of the respective substance (in our case nnAA) into the sample.</P><DIV class="verlinked" id="develop"><H5>FURTHER DEVELOPMENTS</H5></DIV><P>Due to a tight time schedule from the start to the end of iGEM it was not possible for us to realize all ideas and planned developments in respect of improvement of the robot itself.
            For a working process with more kinds of bacteria cultures it is absolutely indispensable to develop a system that is able to avoid all sorts of contamination between the different bacteria. Therefore it would be an option to have an extra reservoir filled with ethanol in which the tip of the syringe can be sterilized between the checks of different samples.
            Another modification that would be useful for working with individual bacteria cultures is making the power LED's changeable. This is necessary if the the wavelength of the LEDs does not overlap with the absorption spectrum of the fluorescent proteins or overlaps with a part of the spectrum that has a very low absorption efficiency.
            Moreover, apart from the latter developments it may be useful to improve the syringe pump system. Instead of using a syringe pump it would be useful to use a system with a reservoir of liquids and a pump that works continously like a turbine, for example see <A href="http://www.ardulink.org/automatic-lipid-dispensing/">http://www.ardulink.org/automatic-lipid-dispensing/</A>.
            Another useful modification of the robot would be to rebuild its foundation, namely an <I>Ultimaker 3D-printer</I> setup. Essential alterations would be to replace the sample stage with a heatbed and to replace the current head with a printhead hotend. Since the current head can be clipped it would not be too much of a challenge. Furthermore, a change of the syringe extruder is necessary, if the printer should work with plastics.
            An alternative approach is a kind of paste 3D-printer. In this case it wouldn't even be needed to change the head and the syringe, because of the already viscous properties of the paste.
        </P><DIV class="verlinked" id="optics"><H5>Optics</H5></DIV><DIV class="verlinked" id="wave"><H6>Operating Range of Wavelengths</H6></DIV>

        The robots optics consists of two big components, a camera head and a lightbox.
        The camera head is responsible for two tasks, which are the detection and localization of samples, and the fluorescence measurements.
        The light table illuminates the sample stage uniformly from below, thereby aiding the camera to reliably do the detection work.
        The fluorescence measurement and object detection are separated in terms of their operating range of wavelengths.
        All the detection occurs at wavelengths above 860nm, which is near infrared.
        The light table radiates uniform infrared light, while the camera chip is capable of capturing this wavelength.
        There is nothing special to the camera chip and in principal all commercially available CCD-Chips can potentially capture near infrared. This is usually an undesirable feature for photographic purposes,
        since it falsifies image colors. This is why camera lenses are usually equipped with an infrared filter.
        In our case, we use our own lens system, where we removed the infrared filter.
        The reason why we chose the detection to operate at infrared is because we are dealing with mVenus, a mutant of eYFP, which does not absorb infrared.
        Now, if we take a look at the spectra, one can see that the gap between the emission and absorption peaks is pretty small.
        The absorption peak occurs at wavelength of 512 nm, while the emission peak is located at a wavelength of 528 nm.
        <DIV class="verlinked" id="fluor"><H6>Fluorescence Measurement and Filtering</H6></DIV>

        For pulse measurements there is a need for advanced high frequency circuitry, which is capable of forcing the LEDs to emit pulse lengths of picoseconds.
        LEDs have rise and fall times in the nanosecond region, if they are used in a simple on/off manner.
        For continuous measurements we have to seperate the wavelength of stimulation from the wavelength of fluorescent emission. Therefore, we used a long pass filter with a cutoff wavelength of 515nm.
        It is capable of blocking most of the stimulation light, while letting most of the mVenus emission pass. The camera captures a long-exposed image to be further analyzed.
        To get rid of the residual stimulation light appearing in that image, it is digitally color-filtered, and segmented into regions of interest (ROI).
        The determination of ROIs occurs simultaneously with the detection of samples.
        The filtering is very strong, and does indeed block some of the already weak mVenus fluorescence.
        This is why we use an exposure time of 10 seconds for a fluorescence capture, to make sure enough data is collected.
<DIV class="verlinked" id="cam"><H6>Optical Hardware - Camera Head</H6></DIV>
        We are using the 8 megapixels PiCamera, because we have access to its capturing settings like framerate,
        exposure time, gains, light sensitivity etc. over an existing programming interface <A href="#refe">[5]</A>.
        This is absolutely necessary since detection and measurement have totally different requirements.
        Another benefit is, that we are able to capture directly in grayscale (Y part of YUV) for fast detection purposes, and switch to RGB when doing
        fluorescence measurements. We do not have these degrees of freedom with an ordinary USB camera. However, the disadvantage is, the stock camera itself is equipped
        with a very minimal lens system. Since our measurements do not only take place at different wavelengths, but also at different distances to the vessels
        (fluorescence images are taken from each single cuvette, while the camera head is directly placed over its opening), there is a need for adjusting the focus.
        We have therefore developed a focusing system consisting of a so called voice coil, which inhabits a 3D printed adapter socket for the PiCamera.
        The adapter socket also includes the optical longpass filter. The voice coil holds a suspended lens, which can be adjusted in its distance to the camera chip.
        This method is used in most smartphones. In our case, we took our voice coil out of an old webcam.
        The voice coil is fed with a PWM signal provided directly by the Raspberry Pi's hardware PWM channel.
        We use a simple <I>L298N H-Bridge</I> stepperdriver to amplify the PWM signal, and to decouple the Raspberry Pi's precious hardware PWM pin.
        Different duty cycles mean different focal positions. The coil current is tuned with a potentiometer.
        In this way we are able to automatically focus the lens by evaluating simple Sobel-filter-based sharpness measurements.
        Our autofocus is finding best sharpness within two seconds, using a robust global search algorithm.
        It is applied every time a new set of racks and samples is placed into the robot, i.e. prior to each new session.
        Also, to adjust the focus for individual fluorescence captures, the sharpness of the individual sample corners is considered.
        The camera head also includes the stimulating LEDs, which are 4 high Power Cree XTE, driven by a 0.9 amp
        current source and a PWM signal delivered by the Raspberry Pi.
        
We mounted the self-made autofocus in a cleanroom at <A href="http://www.idd.tu-darmstadt.de/idd/aktuelles/index.de.jsp">&quot;Institut für Druckmaschinen und Druckverfahren&quot; TU Darmstadt</A>. This was to keep the lense dustless.
We are thankful to experiment under this conditions, but for the autofocus it is not mandatory. Be just aware of a clean dustless environment! 
        <DIV class="verlinked" id="lightbox"><H6>Optical Hardware - Lightbox</H6></DIV>
        The lightbox is an essential part of the detection. All applied detection algorithms rely on thresholding the image,
        or filtered versions of it. The thresholding is basically doing a binary selection of relevant versus irrelevant image information.
        Therefore, there is always a loss of image information. If there is less clutter in the image,
        then there is no need to use strong thresholds, therefore conserving more of the image information.
        The lightbox is acting as a clean background, creating only low amounts of static noise and clutter due to its uniform radiation of light,
        allowing us to use less strict thresholds. It also emphasizes the samples' corners and enhances the detection reliability.
        The heart of the lightbox is a Plexiglas panel called “Endlighten”.
        Light is laterally injected and reflected off systematic impurities inside the panel <A href="#refe">[6]</A>.
        The light then leaves the panel uniformly in all directions.
        Light leaving the panel back side is mirrored to the front side by a white reflective Plexiglas,
        and additionally diffused by a diffusor plate, also made of Plexiglas.
        The light is injected by flat-end infrared LEDs which are mounted on 3D printed rails and tightly clamped to the sides of the
        Endlighten panel. The LEDs are driven by constant current sources to give them a long lifetime.
        <DIV class="verlinked" id="cool"><H5>Cooling</H5></DIV>
        All electronic components produce a significant amount of heat, especially the motor parts, power supply, and the Raspberry Pi.
        Since the robot chassis is meant to be completely enclosed to keep light out, heat is going to pile up in the upper part of the interior.
        To protect the samples from temperatures above room temperature, it is necessary to include a cooling system, which ensures proper air circulation, and does not let in ambient light.
        To fulfill these two requirements we decided to adapt a double-walled cooling system. The simplest implementation of it is based on the fact that warm air rises naturally, and incorporates the power supply as the air intake.
        The power supply is placed on the bottom of the robot, and draws in fresh air. The air, which is getting warmed up by the interior rises to the top and is let out by a radial fan through an extractor hood, made of a laser cutted MDF grid.
        The warm air in between the MDF grid and the outer wall is directed through a 3D printed exhaust tunnel. Thus, ambient light is being kept out.

        <DIV class="verlinked" id="softw"><H5>Software</H5></DIV><DIV class="verlinked" id="marl"><H6>Marlin</H6></DIV>
        Marlin is an Open Source C++ firmware for 3D printers available on <I>GitHub</I>. It is executed on an 8-bit micro-controller, which in our case is an Arduino Mega 2560. Due to the fact, that Marlin is adaptable to a lot of boards and different configurations it was the perfect choice for us to use it in our project.
        Nevertheless, we had to modify the firmware a little bit, so that it meets all our needs. For example we had to configure so called endstops, that ensure that the sample stage and the head won't crash at a dead end. If you're intereseted in all changes we made, click <A href="https://static.igem.org/mediawiki/2016/9/97/T--TU_Darmstadt--marlin.pdf">here</A>.
        There, you will also find a link where you can download the original, unmodified version of the marlin firmware.








        <DIV class="verlinked" id="opencv"><H6>OpenCV</H6></DIV><P><I>OpenCV</I> is a cross-platform image processing library and free for use under the open-source BSD license. Its development has been initiated by Intel in the nineties to demonstrate the capability of CPUs in executing complex image processing tasks.  <I>OpenCV</I> covers the most basic morphological image operations up to advanced machine learning algorithms.  It is written in C++, which is also its primary interface. By now most of the available features have been wrapped for other programming languages like Python, which we are going to use.  Especially what we are looking for in <I>OpenCV</I> are its feature extraction algorithms, like the <I>Hough transformation</I> (1) <A id="displayText" href="javascript:toggle();">(show details)</A>, contour and edge detection (2) <A id="displayText2" href="javascript:toggle2();">(show details)</A>, and image moment (3) <A id="displayText3" href="javascript:toggle3();">(show details)</A> extraction.  Also we make our lives easier by utilizing its implemented and optimized morphological operators and image filters (4) <A id="displayText4" href="javascript:toggle4();">(show details)</A>, like the median filter and the sobel operator.          To enhance the long-term reliability of detection we separate the detection procedure into two steps: Since racks are predominantly used to hold the sample vessels, we firstly extract and detect the rectangular shaped racks with contour and thresholding techniques. Several image shape descriptors are used to track possible positional changes. The user is then allowed to manually assign regional attributes (e.g. radii, heights) for each rack. The assignment of local radii helps the circle detection in the second step. Not only does it inhibit false detections, but also allows for a proper multithreaded apportionment of work.</P><DIV id="toggleText" style="display: none"><B>(1)</B> The <I>Hough Transformation</I> is an image processing algorithm, which transform an image in into the parameter space,  also called <I>Hough Space</I>, with respect to a parameterizable geometric shape, analogous to a fourier transformation transforming something into Fourier space with respect to sinusoidal base functions. The possible sets of correct parameters establish themselves in <I>Hough Space</I> as local maxima.</DIV><DIV id="toggleText2" style="display: none"><B>(2)</B> We use the detection of closed contours for our rectangle detection software rather than using Hough transformation, because it is faster, and in combination with the light box sufficiently reliable. The underlying edge detector uses the <I>Canny algorithm</I>.</DIV><DIV id="toggleText3" style="display: none"><B>(3)</B> A line can be described by 2 parameters, slope and offset. These two parameters are descriptors, which can be used to reconstruct the image. They contain the whole information about the line. Similarly, the same line could be expressed with a fourier series (at least partly). Image moments are still another kind of descriptors.  We use them in our tracking algorithm, to characterize detected objects and to be able to identify them in later recordings.</DIV><DIV id="toggleText4" style="display: none"><B>(4)</B> Image filters act on the image to amplify certain aspects, and attenuate others. For example, the sobel filter, which is basically a derivative operator, amplifies regions of fast intensity change (edges). The Canny edge detection algorithm is based on this filter, and returns a binarily thresholded image.</DIV><DIV class="verlinked" id="pyqt"><H6>PyQt</H6></DIV><P>Qt is a software tool to develop a GUI (Graphical User Interface)<A href="#refe">[7]</A>. It is available under a commercial and an open-source license. The software is a cross-platform application framework, which means it runs on the most computer systems like Unix or Windows. The underlying programming language is C++ and Qt can use already existing programming languages like Javascript, making it a powerful tool.
            The main idea of Qt is to use a system of signals and slots to have an easy framework to connect displayed elements with underlying functions. Also, the reusability of already existing code is enhanced. Every graphical element, for example a button, emits its own signal when it is pressed or used. The signal then can be used to trigger an action, like closing a window. If the signal is not connected to a function nothing will happen, however the signal will be emitted with no consequences. Now it is possible to connect the emitted signal with a desired action, called slot, and the program gets its specific behavior.
            Qt is widely used by companies like the European Space Agency (ESA), Samsung, DreamWorks, Volvo and many more.
            To be able to combine the possibilities of Qt with the simplicity of the Python programming language, PyQt was developed. PyQt is a binding for Python capable of translating Qt methods within the Python syntax. 
            To be able to get a direct preview of the constructed GUI, Qt Designer is a helpful tool. Basically it enables an intuitive way to build a graphical user interface without a need to explicitly coding it. To later work with the code itself, PyQt uses a method called <I>pyuic(number)</I>, which is executed through the terminal. The number in the brackets stands for the version.
            After converting the code one can open the GUI as a regular python script and work with it as usual. </P><DIV class="verlinked" id="build"><H5>BUILDING INSTRUCTIONS</H5></DIV>
        The following video shows the assembly of our robot's mechanics. Feel free to jump in the video or download it; so you can watch as many times as you like. All necessary steps are commented.
        <DIV class="verlinked" id="vid"><H6>Construction Video</H6></DIV><CENTER><VIDEO width="600" controls=""><SOURCE src="https://static.igem.org/mediawiki/2016/f/f3/T--TU_Darmstadt--construction.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</SOURCE></VIDEO></CENTER><DIV class="verlinked" id="circ"><H6>Circuit Diagram</H6></DIV><P>
The stepper motors for X and Y are plugged into the RAMPS board as shown in the graphic. There are already prepared cables available with corresponding connectors. To control and power them the stepper driver (A4988) are plugged in the RAMPS slots, marked with X and Y. On the bottom of the stepper driver it is marked which pin is ground and so on. Also on the RAMPS board it is marked where the corresponding slots for ground etc. are. The X and Y stepper motors have one endstop each to provide a stop signal for the homing routine. It is also possible for safety reasons to add one more endstop to each axis, so that the movement stops for sure at the end positons. All the endstops are plugged into the marked position on the RAMPS board. From left to right the pins are for X min and X max position, Y min and Y max position and Z min and Z max position. The pin marked with a + is reserved for the 5 V, the middle pin is the ground and the last pin marked with a S is the signal.
The Z stepper motor has two endstops and is connected like the X and Y stepper motors and endstops.
To drive our syringe pump one more stepper motor and one endstop is required and is plugged into the E0 position. It is connected like X,Y and Z.
To power the RAMPS board 12 V are needed, for example from an ATX power supply.
The Raspberry Pi3 is responsible for controlling the optics and LEDs whereas the Arduino controls the stepper motors and endstops. Arduino and Raspberry Pi are connected via one of the Pi’s USB port and the USB Type B input of the Arduino.
The lightbox is powered with 24 V from the ATX power supply. The lightbox consists of 4 LED bars, each carrying 10 infrared LEDs in series. Each bar is driven by one 50mA constant current source. These are parallel connected to the -12V of the power supply, and to the NC of one Relay. The COM port of that Relais is then connected to the +12V of the power supply.
The 2 Relay Modul serves as a Raspberry Pi controlled switch which controls the lightbox and the blue exposing LEDs. The corresponding wire’s number refer to the used GPIO pins. Be careful, the internal GPIO layout is different as the numbering on the Pi layout scheme. All numbers correspond to the latter!
The L298N are DC motor drivers which we alienated as protection circuit, to separate the 12 V from the Raspberry Pi’s GPIO. 
The blue exposing LEDs are series-connected, and controlled by the 2 Relay Modul on the first side.
</P><DIV class="verlinked" id="zips"><H6>Required Software</H6></DIV><P>In case you want to rebuild ou project, you can download all needed software in this section. The software includes the exact <A href="https://static.igem.org/mediawiki/2016/3/38/T--TU_Darmstadt--Marlin.zip">marlin firmware</A>, which we uploaded on the Arduino, as well as the software that is installed on the <A href="https://static.igem.org/mediawiki/2016/8/85/T--TU_Darmstadt--Raspberry.zip">Raspberry Pi</A> and some improved <A href="https://static.igem.org/mediawiki/2016/a/af/T--TU_Darmstadt--python.zip">python scripts</A>. Furthermore, to enable wireless support, install and configure &quot;hostapd&quot; and &quot;dnsmasq&quot; on your Raspberry Pi. We followed this external <A href="https://frillip.com/using-your-raspberry-pi-3-as-a-wifi-access-point-with-hostapd/">tutorial</A>.</P><DIV class="verlinked" id="bom"><H6>Bill Of Materials</H6></DIV>

The costs on this bill are just an example for what we paid for the items. It may be that the costs vary from ours, if you are buying them from another shop as we did. Also, it should be mentioned, that a lot of the items we used in this project are 3D-printed or were manufactured in a workshop, so that we didn't have to pay for it. So it is plausible, that the total costs for this project will rise.
Nevertheless, if you are still interested in constructing this robot, you can take a look at the bill of materials <A href="https://static.igem.org/mediawiki/2016/6/64/T--TU_Darmstadt--BOM.pdf">here</A>.
</DIV><DIV class="verlinked" id="refe"><DIV class="references"><H6>References</H6><UL><LI>[1] <A href="http://edition.cnn.com/2014/02/13/tech/innovation/the-night-i-invented-3d-printing-chuck-hall/">http://edition.cnn.com/2014/02/13/tech/innovation/the-night-i-invented-3d-printing-chuck-hall</A></LI><LI>[2] <A href="https://2015.igem.org/Team:TU_Darmstadt/Project/Tech">https://2015.igem.org/Team:TU_Darmstadt/Project/Tech</A></LI><LI>[3] <A href="http://reprap.org/wiki/About">http://reprap.org/wiki/About</A></LI><LI>[4] <A href="http://www.thingiverse.com/thing:811271">http://www.thingiverse.com/thing:811271, jasonatepaint</A></LI><LI>[5] <A href="https://picamera.readthedocs.io/en/release-1.12/">https://picamera.readthedocs.io/en/release-1.12/</A></LI><LI>[6] <A href="https://www.plexiglas-shop.com/pdfs/en/212-15-PLEXIGLAS-LED-edge-lighting-en.pdf">https://www.plexiglas-shop.com/pdfs/en/212-15-PLEXIGLAS-LED-edge-lighting-en.pdf</A></LI><LI>[7] <A href="https://en.wikipedia.org/wiki/Qt_(software)">https://en.wikipedia.org/wiki/Qt_(software)</A></LI></UL></DIV></DIV></DIV><DIV class="rechts"><DIV class="scrollbox"><DIV class="highlights"><A href="#intro">Overview</A><A href="#func">Functionality</A><A href="#results">Results</A><A href="#develop">Further Developments</A><A href="#optics">Technical Details</A><A href="#softw">Software</A></DIV><BUTTON class="back_top_full"><A href="#mainHeader">Back to the Top</A></BUTTON></DIV></DIV></DIV><DIV class="footer" style="text-decoration: none;"><CENTER><DIV id="igemTUDA"><A href="https://2016.igem.org/Team:TU_Darmstadt" style="color:white">iGEM Technische Universität Darmstadt</A></DIV></CENTER><CENTER><DIV id="footer_linear">
We thank our <A href="https://2016.igem.org/Team:TU_Darmstadt/Team/Acknowledgements" style="color:#019ac8">sponsors</A>, especially:

Email: igem@bio.tu-darmstadt.de | 
Design: Viktoria Schuster</DIV></CENTER></DIV></DIV></DIV></DIV></DIV></DIV></BODY></HTML>