<HTML lang="en" dir="ltr" class="client-nojs">
<style type="text/css">
A:before { content:' '; } 
A:after { content:' '; } 
SPAN:before { content:' '; } 
SPAN:after { content:' '; } 
</style>
<BODY class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Team_Kent_Methodology skin-igem action-view"><DIV id="globalWrapper"><DIV id="content" class="mw-body" role="main"><DIV id="top_title"><H1 id="firstHeading" class="firstHeading"><SPAN dir="auto">Team:Kent/Methodology</SPAN></H1></DIV><DIV id="HQ_page"><DIV id="bodyContent"><DIV id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><DIV class="menu_wrapper"><DIV class="nav"><LABEL for="show-menu" class="show-menu">Show Menu</LABEL><DIV class="nav_hide"><DIV class="nav_menu_item hvr-fade"><DIV class="nav_link"><A href="https://2016.igem.org/Team:Kent">Home</A></DIV></DIV><DIV class="nav_menu_item hvr-fade"><LABEL for="toggleTeam" class="toggle_nav nav_link hvr-fade">Team</LABEL><DIV class="nav_submenu" id="toggleTeam_submenu"><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Team">Meet the team</A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Attributions">Attributions</A></DIV></DIV></DIV><DIV class="nav_menu_item hvr-fade"><LABEL for="toggleProject" class="toggle_nav nav_link hvr-fade">Project</LABEL><DIV class="nav_submenu" id="toggleProject_submenu"><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Description">Description</A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Experiments">Experiments</A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Results">Results</A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Safety"> Safety </A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Achievements">Achievements</A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://igem.org/2016_Judging_Form?id=1985">Judging</A></DIV></DIV></DIV><DIV class="nav_menu_item hvr-fade"><LABEL for="toggleParts" class="toggle_nav nav_link hvr-fade">Parts</LABEL><DIV class="nav_submenu" id="toggleParts_submenu"><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Parts">Overview</A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Basic">Basic</A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Composite_Part">Composite</A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Part_Collection"> Collection</A></DIV></DIV></DIV><DIV class="nav_menu_item hvr-fade"><LABEL for="togglePractices" class="toggle_nav nav_link hvr-fade"><A href="https://2016.igem.org/Team:Kent/Human_Practices">Practices</A></LABEL><DIV class="nav_submenu" id="togglePractices_submenu"><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Human_Practices">HP</A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Collaborations">  Collaborations </A></DIV></DIV></DIV><DIV class="nav_menu_item hvr-fade"><LABEL for="toggleModelling" class="toggle_nav nav_link hvr-fade">Modelling</LABEL><DIV class="nav_submenu" id="toggleModelling_submenu"><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Model">Introduction</A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Algorithm">Algorithm</A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/Methodology">Methodology</A></DIV><DIV class="nav_menu_subitem hvr-fade"><A href="https://2016.igem.org/Team:Kent/ModelResults">Results</A></DIV></DIV></DIV><DIV class="nav_menu_item hvr-fade"><DIV class="nav_link"><A href="https://2016.igem.org/Team:Kent/Notebook">Notebook</A></DIV></DIV></DIV></DIV><DIV class="nav_replace">
    Navigation
  </DIV><UL id="accordion" class="top_menu"><LI class="menu_item"><A href="https://2016.igem.org/Team:Kent">HOME </A></LI><LI class="menu_item"> TEAM
			</LI><LI><A href=" https://2016.igem.org/Team:Kent/Team"> Team   </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Collaborations">★  Collaborations </A></LI></UL><LI class="menu_item"> PROJECT  
			</LI><LI><A href="https://2016.igem.org/Team:Kent/Description"> ★  Description </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Design"> ★ Design </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Experiments"> Experiments </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Proof"> ★ Proof of Concept </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Demonstrate"> ★ Demonstrate </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Results"> Results </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Notebook"> Notebook </A></LI><LI class="menu_item"> PARTS  
			</LI><LI><A href="https://2016.igem.org/Team:Kent/Parts">Parts </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Basic_Part"> ★ Basic Parts </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Composite_Part"> ★ Composite Parts </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Part_Collection"> ★ Part Collection </A></LI><LI class="menu_item"><A href="https://2016.igem.org/Team:Kent/Safety"> SAFETY </A></LI><LI class="menu_item"><A href="https://2016.igem.org/Team:Kent/Attributions"> ATTRIBUTIONS </A></LI><LI class="menu_item"> PRACTICES 
			</LI><LI><A href="https://2016.igem.org/Team:Kent/Human_Practices"> Human Practices </A></LI><LI><A href="https://2016.igem.org/Team:Kent/HP/Silver">★ Silver </A></LI><LI><A href="https://2016.igem.org/Team:Kent/HP/Gold">★ Gold </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Integrated_Practices"> ★ Integrated Practices </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Engagement">★ Engagement </A></LI><LI class="menu_item"> AWARDS 
			</LI><LI><A href="https://2016.igem.org/Team:Kent/Entrepreneurship"> ★ Entrepreneurship </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Hardware"> ★ Hardware </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Software">★ Software </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Measurement">★  Measurement </A></LI><LI><A href="https://2016.igem.org/Team:Kent/Model">★ Model </A></LI></DIV><DIV class="content_wrapper"><DIV class="top"><H1 class="first_title">Methodology</H1><P>MATLAB topic modelling toolbox written by Steyvers, University of California, Irvine [10] was used. This toolkit contained different types of Gibbs sampler.

We used their Gibbs Sampler HMM LDA because this algorithm is more suitable for limited
data sets than the standard LDA Gibbs sampler as there is no need to exclude stop words, such as full
stop and comma, from the corpus of text, in other words, it takes different syntactic patterns of individual words in account. For example, as the topics are generated from words whose appearance varies between documents, some terms such as “the”, “I”, “is”, etc. which tend to occur in every document without much variation are less important to the construction of topics than words whose appearance varies across documents, e.g. “environment”, “DNA”, “bacteria”, etc. Therefore, in HMM LDA model, syntactic states of tokens do influence the generation of topics [11].</P></DIV><DIV class="subs"><H2 class="sub_titles">Gibbs Sampler HMM LDA Input</H2><P>The table below provides the list of input variables that is required for the Gibbs Sampler HMM LDA and their description.</P><TABLE style="width:80%"><TBODY><TR><TH>Variable</TH><TH>Description</TH></TR><TR><TD>WS</TD><TD>1xN vector where WS(k) contains the vocabulary index of the kth word token, and  is the number of word tokens. Word indices are non-zero based unless it is a sentence marker (“.”) which has a word index of 0</TD></TR><TR><TD>DS</TD><TD>1xN  vector where DS(k) contains the document index of the kth word token, and  is the number of word tokens. Document indices is also non-zero based</TD></TR><TR><TD>WO</TD><TD>1xW  cell array of strings where WO(k) contains the kth vocabulary item and  is the number of distinctive vocabulary item. In other words, a dictionary of all the words contained in the whole corpus of text that is being analysed.</TD></TR></TBODY></TABLE>
Our code to convert text documents into this input file is available <A style="color:white" href="https://static.igem.org/mediawiki/2016/6/62/KentiGEM_code.zip">here</A>.


Here is the list of subjects that were analyse using the model:
 <TABLE style="width:80%"><TBODY><TR><TH>Source</TH><TH>Publish date of the texts</TH><TH>Total number of documents (text files)</TH></TR><TR><TD>PubMed entries with the key word “Magnetosome”</TD><TD>1980 - 2016</TD><TD>429</TD></TR><TR><TD>PubMed entries with the key word “Synthetic Biology”</TD><TD>2013 - 2016</TD><TD>9628</TD></TR><TR><TD>iGEM team abstracts</TD><TD>2013 - 2015</TD><TD>693</TD></TR><TR><TD>iGEM team Human practices/Policy and Practices</TD><TD>2013 - 2015</TD><TD>572</TD></TR></TBODY></TABLE> 


Note that the number of documents does not add up to the number of iGEM teams each year, this is because some teams did not have abstract or human practices/P&amp;P.

We ran each of the source’s texts separately.
<A style="color:white" href="https://static.igem.org/mediawiki/2016/6/62/KentiGEM_code.zip">Here</A> is our code to split PubMed text files when viewed in MEDLINE format.

Teams abstracts, Human practices and P&amp;P were scraped manually.

 <TABLE style="width:80%"><TBODY><TR><TH>Parameters</TH><TH>Description</TH><TH>Value</TH><TH>Reference</TH></TR><TR><TD>T</TD><TD>Number of topics to be extracted. This was determined by looking at final results. The model was running with 10, 15 and 25 number of topics and the best one was chosen.</TD><TD>T=15 for for all source except iGEM team abstracts.

T=25 for iGEM team abstracts</TD></TR><TR><TD>NS</TD><TD>Number of syntactic states</TD><TD>12</TD><TD>[12]</TD></TR><TR><TD>N</TD><TD>Number of iterations</TD><TD>200</TD><TD>[12]</TD></TR><TR><TD>α</TD><TD>Alpha hyperparameter</TD><TD>50/T</TD><TD>[12]</TD></TR><TR><TD>β</TD><TD>Beta hyperparameter</TD><TD>0.01</TD><TD>[12]</TD></TR><TR><TD>γ</TD><TD>Gamma hyperparameter</TD><TD>0.1</TD><TD>[12]</TD></TR><TR><TD>SEED</TD><TD>Seed for random number generator</TD><TD>2</TD><TD>[12]</TD></TR></TBODY></TABLE><H2 class="sub_titles">Gibbs Sampler HMM LDA Output</H2><P>The Gibbs Sampler HMM LDA outputs the following [10]:</P><TABLE style="width:80%"><TBODY><TR><TH>Variable</TH><TH>Description</TH></TR><TR><TD>WP</TD><TD>WxT  matrix where W(i,j)  contains the number of times words  has been assigned to topic .</TD></TR><TR><TD>DP</TD><TD>DxT matrix where DP(i,j) contains the number of times a word in document  has been assigned to topic .</TD></TR><TR><TD>MP</TD><TD>WxNS matrix where MP(i,j) contains the number of times word  has been assigned to syntactic state .</TD></TR><TR><TD>Z and X</TD><TD>Both has the size 1xn containing the topic and HMM state assignments respectively.</TD></TR></TBODY></TABLE><H2 class="sub_titles">Topic Function Input and Output</H2><P>The topic writing function reads the following inputs, which are the outputs from Gibbs Sampler HMM LDA [10]:</P><UL><LI>WP
  </LI><LI>BETA (β)
  </LI><LI>WO
</LI></UL><P>For formatting of the result in output text file, the following inputs can be added and set to user preferences.</P><TABLE style="width:80%"><TBODY><TR><TH>Variable</TH><TH>Description</TH></TR><TR><TD>K</TD><TD>The number most likely entities per topic (K) . We chose K=7.</TD></TR><TR><TD>E</TD><TD>The threshold on the topics listing in S (string) in  which is an output variable.</TD></TR><TR><TD>M</TD><TD>The number of columns in the output text file (formatting).</TD></TR></TBODY></TABLE> 
String in programming is a sequence of characters.

<H2 class="sub_titles">Visualisation - Evaluation</H2><P>The result can be evaluated either using Human-in-the-loop or metric. We used metric to assess the output of our model.
Visualize topic function reads the following outputs from Gibbs Sampler HMM LDA [10]:</P><UL><LI>DP
  </LI><LI>Alpha (α)
  </LI><LI>The topic strings S output from the topic function.
</LI></UL><P>The i and j counts in DP  are transformed to probability distributions over documents for each topic. For reminder,  DP(i,j) contains the number of times a word in document i has been assigned to topic j. The symmetrized KL- distance (Kullback-Leibler) between document distributions is calculated for each topic pair.

In general, KL-distance is difference between two probability distributions and topics are probability distributions. We have edited the code so that the visualisation function draws lines between each topic with a degree of thickness and opacity. The lines become darker and thicker with shorter distances and fainter and light with longer distances, between each topic. Furthermore, instead of just text appearing we have added circles and their size determines its probability of the topic. Bigger mean higher probably, vice-versa. The visualizer is set to only show top seven words in each topic.</P>
Our version of the topic visualizer function is available <A style="color:white" href="https://static.igem.org/mediawiki/2016/6/62/KentiGEM_code.zip">here</A>.

<H2 class="sub_titles">Other Methods for Evaluation</H2><P>For interests, results can be evaluated either using Human-in- the-loop or metric.
Human-in- the-loop means human interaction with the model and there are two methods in which the humans
can interact with this topic model:</P><UL><LI>Word intrusion

<P>One word from a cluster is swapped with another cluster (intruder) and see whether a human can
reliably tell which one is an intruder.
If humans are able to find the intruder means that the trained topic is topically coherent (good) [14].
If not the topic has no discernible theme (bad) [15].</P></LI><LI>Topic intrusion
<P>In this method, an intrusion topic is inserted to a document that contains topics that are assigned to
that document. Similar to word intrusion, humans are asked to identify the intrusion topic [13]</P></LI></UL>
Human-in-the-loop is very costly as it is time consuming.
Metrics are used to assess how good the model and there are several methods. Here is a list of some [15].
<UL><LI>Cosine similarity
  </LI><LI>Size (#tokens assigned)
  </LI><LI>Within-doc rank
  </LI><LI>Similarity to corpus-wide distribution
  </LI><LI>Locally-frequent words
  </LI><LI>Co-doc Coherence
</LI></UL></DIV></DIV></DIV></DIV></DIV></DIV></DIV></BODY></HTML>