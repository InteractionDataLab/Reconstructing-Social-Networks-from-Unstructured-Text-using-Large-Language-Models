<HTML xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
<style type="text/css">
A:before { content:' '; } 
A:after { content:' '; } 
SPAN:before { content:' '; } 
SPAN:after { content:' '; } 
</style>
<BODY class="mediawiki  ltr ns-0 ns-subject page-Team_USTC_Software_Algorithm"><DIV id="globalWrapper"><DIV id="top-section"><DIV id="p-logo"><A href="/Main_Page" title="Main Page">&quot;
	    </A></DIV><DIV id="menubar" class="left-menu"><UL><LI class="selected"><A href="/Team:USTC_Software/Algorithm">Page               </A></LI><LI class="new"><A href="/wiki/index.php?title=Talk:Team:USTC_Software/Algorithm&amp;action=edit&amp;redlink=1">Discussion               </A></LI><LI><A href="/wiki/index.php?title=Team:USTC_Software/Algorithm&amp;action=edit">View source               </A></LI><LI><A href="/wiki/index.php?title=Team:USTC_Software/Algorithm&amp;action=history">History               </A></LI><LI style="color:#808080;cursor:default">teams</LI></UL></DIV><DIV class="right-menu" id="menubar"><UL><LI id="pt-login"><A href="/wiki/index.php?title=Special:UserLogin&amp;returnto=Team:USTC_Software/Algorithm" title="You are encouraged to log in; however, it is not mandatory [o]" accesskey="o">Log in</A></LI></UL></DIV><DIV id="search-controls"><FORM action="/Special:Search" id="searchform">Â </FORM></DIV></DIV><DIV id="content"><H1 class="firstHeading">Team:USTC Software/Algorithm</H1><DIV id="bodyContent"><H3 id="siteSub">From 2009.igem.org</H3><TABLE id="toc" class="toc"><TBODY><TR><TD><DIV id="toctitle"><H2>Contents</H2></DIV><UL><LI class="toclevel-1 tocsection-1"><A href="#Genetic_Algorithm_.28GA.29"><SPAN class="tocnumber">1</SPAN><SPAN class="toctext">Genetic Algorithm (GA)</SPAN></A></LI><LI class="toclevel-2 tocsection-2"><A href="#Introduction_to_GA"><SPAN class="tocnumber">1.1</SPAN><SPAN class="toctext">Introduction to GA</SPAN></A></LI><LI class="toclevel-2 tocsection-3"><A href="#Optimization_Processes_of_GA"><SPAN class="tocnumber">1.2</SPAN><SPAN class="toctext">Optimization Processes of GA</SPAN></A></LI><LI class="toclevel-2 tocsection-4"><A href="#Parameter_For_GA"><SPAN class="tocnumber">1.3</SPAN><SPAN class="toctext">Parameter For GA</SPAN></A></LI></UL><LI class="toclevel-1 tocsection-5"><A href="#Particle_Swarm_Optimization_Algorithm_.28PSO.29"><SPAN class="tocnumber">2</SPAN><SPAN class="toctext">Particle Swarm Optimization Algorithm (PSO)</SPAN></A></LI><LI class="toclevel-2 tocsection-6"><A href="#Introduction_to_PSO"><SPAN class="tocnumber">2.1</SPAN><SPAN class="toctext">Introduction to PSO</SPAN></A></LI><LI class="toclevel-2 tocsection-7"><A href="#Basic_Formulas_of_PSO"><SPAN class="tocnumber">2.2</SPAN><SPAN class="toctext">Basic Formulas of PSO</SPAN></A></LI><LI class="toclevel-2 tocsection-8"><A href="#Performance_of_PSO"><SPAN class="tocnumber">2.3</SPAN><SPAN class="toctext">Performance of PSO</SPAN></A></LI><LI class="toclevel-2 tocsection-9"><A href="#Reference"><SPAN class="tocnumber">2.4</SPAN><SPAN class="toctext">Reference</SPAN></A></LI><LI class="toclevel-2 tocsection-10"><A href="#Data_Structure_and_Organization"><SPAN class="tocnumber">2.5</SPAN><SPAN class="toctext">Data Structure and Organization</SPAN></A></LI><LI class="toclevel-1 tocsection-11"><A href="#Global_Sensitivity_Analysis_.28GSA.29"><SPAN class="tocnumber">3</SPAN><SPAN class="toctext">Global Sensitivity Analysis (GSA)</SPAN></A></LI><LI class="toclevel-2 tocsection-12"><A href="#Introduction_to_GSA"><SPAN class="tocnumber">3.1</SPAN><SPAN class="toctext">Introduction to GSA</SPAN></A></LI><LI class="toclevel-2 tocsection-13"><A href="#Theoretical_Foundations_of_GSA"><SPAN class="tocnumber">3.2</SPAN><SPAN class="toctext">Theoretical Foundations of GSA</SPAN></A></LI><LI class="toclevel-1 tocsection-14"><A href="#Local_Sensitivity_Analysis_.28LSA.29"><SPAN class="tocnumber">4</SPAN><SPAN class="toctext">Local Sensitivity Analysis (LSA)</SPAN></A></LI><LI class="toclevel-2 tocsection-15"><A href="#Introduction_to_LSA"><SPAN class="tocnumber">4.1</SPAN><SPAN class="toctext">Introduction to LSA</SPAN></A></LI><LI class="toclevel-2 tocsection-16"><A href="#Basic_Definitions"><SPAN class="tocnumber">4.2</SPAN><SPAN class="toctext">Basic Definitions</SPAN></A></LI><LI class="toclevel-2 tocsection-17"><A href="#Computation"><SPAN class="tocnumber">4.3</SPAN><SPAN class="toctext">Computation</SPAN></A></LI></TD></TR></TBODY></TABLE><H1><SPAN class="mw-headline" id="Genetic_Algorithm_.28GA.29">Genetic Algorithm (GA)</SPAN></H1><H2><SPAN class="mw-headline" id="Introduction_to_GA">Introduction to GA</SPAN></H2><P><B> What's GA</B></P><P>Genetic algorithms are powerful methods for complex optimization problems. They are essentially evolution in a computer. A population of GA population random candidate solutions is generated and scored. These solutions are allowed to mate with each other, generating offspring that are hybrids of both parental solutions. The mating frequencies are related to a boltzmann-like probability term; akin to simulated annealing, the simulation temperature required for calculating these is high early in the run, and cools down. Since the mating probability for a given candidate solution is related to its score, better solutions mate more frequently, making their components more numerous in the offspring. This new population of candidate solutions is scored, ranked, and allowed to mate. 
</P><P><B> Why GA?</B></P><P>Genetic algorithm (GA) is utilized to identify the topology of the network. A rate term can be described as two parts: signal and function type. The former usually indicates the positive and negative interaction from reactants to products which strongly correlate with the SBGN entity relationship diagram. The later reflects the biochemical reaction type. GA searches an optimized solution by mimicking the process of evolution. Members of an initial population compete for the privilege to mate and produce offspring for successive generations. The probability of an individual mating is proportional to the fitness of the environment. The generation with better fitness will be obtained generation by generation. As the topology, the interaction forms and the parameters for each component function are all necessary in the final result, we employed a multi-level optimization. The lowest level optimization is to give the suitable parameters with the topology and the interaction forms known. For the next level, we are trying to find the interactions forms of component functions with the known topology with genetic algorithm. At last, for the upper level, the potential topologies are selected also with genetic algorithm. And the fitness functions are the similarity between the dynamic behaviors and the target, which are measured by the standard error.
</P><H2><SPAN class="mw-headline" id="Optimization_Processes_of_GA">Optimization Processes of GA</SPAN></H2><UL><LI> Step1: Randomly generate a population of topologies.
</LI></UL><UL><LI> Step2: For each topology, generate a population of networks with a serial of random interaction forms.
</LI></UL><UL><LI> Step3: For each network, obtain the fitness score by optimize the parameters.
</LI></UL><UL><LI> Step4: Obtain the fitness score of a topology by searching the most favorable network.
</LI></UL><UL><LI> Step5: Evolutes the topologies with each fitness score.
</LI></UL><H2><SPAN class="mw-headline" id="Parameter_For_GA">Parameter For GA</SPAN></H2><UL><LI> Population: the number of members in one generation
</LI></UL><UL><LI> Mutation ratio: the ratio of each gene alter its status in a generation
</LI></UL><UL><LI> Recombine ratio: the ratio that whether a gene change with its allel in mating
</LI></UL><UL><LI> Max_cycle: the maximum number of generations considered for the evolution
</LI></UL><H1><SPAN class="mw-headline" id="Particle_Swarm_Optimization_Algorithm_.28PSO.29">Particle Swarm Optimization Algorithm (PSO)</SPAN></H1><H2><SPAN class="mw-headline" id="Introduction_to_PSO">Introduction to PSO</SPAN></H2><P><B> What's PSO</B></P><P>Particle Swarm Optimization(PSO) is an optimization algorithm based on swarm intelligence theory.
Motivated by the evolution of nature, a series of evolutionary computation techniques, such as
evolutionary programming, genetic algorithms, evolutionary strategies, are proposed, in which a best
solution is evolved through the generations. In contrast to evolutionary computation techniques,
Eberhart and Kennedy developed a different algorithm through simulating social behavior of bird flocking or fish schooling. In a particle swarm optimizer, instead of using genetic operators,
individual swarms are evolved through cooperation and competition. Each particle adjusts its flying according to its
own flying experience and its companions' flying experience. The position of each particle represents a potential solution to a problem.
</P><P>The following is excerpt from&quot;<A href="http://www.swarmintelligence.org/" class="external free" rel="nofollow">http://www.swarmintelligence.org/</A>&quot;, it shows clearly how PSO works:
&quot;Each particle keeps track of its coordinates in the problem space which are associated with the best solution (fitness) it has achieved so far. (The fitness value is also stored.) This value is called pbest. Another &quot;best&quot; value that is tracked by the particle swarm optimizer is the best value, obtained so far by any particle in the neighbors of the particle. This location is called lbest. when a particle takes all the population as its topological neighbors, the best value is a global best and is called gbest.
</P><P>The particle swarm optimization concept consists of, at each time step, changing the velocity of (accelerating) each particle toward its pbest and lbest locations (local version of PSO). Acceleration is weighted by a random term, with separate random numbers being generated for acceleration toward pbest and lbest locations.&quot; (cite from the website)
</P><P><B> Why PSO?</B></P><P>In past several years, PSO has been successfully applied in many research and application areas. It is demonstrated that PSO gets better results in a faster, cheaper way compared with other methods. Compared to GA, the advantages of PSO are that PSO is easy to implement and there are few parameters to adjust. The most important reason we choose to implement this algorithm into our software is that this algorithm is easy to realize parallelization. Since the most time-consuming part in our scheme is the optimization of parameters for a given topological structure. If we cannot find a efficient optimizer, it is impossible to deal with systems contains more than five or six nodes. parallelization of the optimization process will be implemented in our next version.
</P><H2><SPAN class="mw-headline" id="Basic_Formulas_of_PSO">Basic Formulas of PSO</SPAN></H2><P>First we must be clear with several concepts:
</P><UL><LI> particle:have a position and a velocity, fly through parameter space
</LI></UL><UL><LI> velocity: velocity of a particle in the parameter space
</LI></UL><UL><LI> pbest: The best solution a particle has achieved so far
</LI></UL><UL><LI> lbest: Best solution obtained so far by any particle in the neighbors of the particle
</LI></UL><UL><LI> gbest: The best solution achieved by all particles so far
</LI></UL><P>Parameters used in PSO algorithm:
</P><UL><LI> Number of Particles:
</LI></UL><UL><LI> learning factors &lt;math&gt;c_{1}&lt;/math&gt;, &lt;math&gt;c_{2}&lt;/math&gt;</LI></UL><UL><LI> inertial factor &lt;math&gt;w&lt;/math&gt;</LI></UL><UL><LI> range of parameter and velocity on each dimension
</LI></UL><UL><LI> optimization threshold
</LI></UL><P>The particles update their positions and velocities with following equation:
</P><P>&lt;math&gt;v[] = w*v[] + c_{1}*rand()*(pbest[]-postion[]) + c_{2}*rand()*(gbest[] - position[])&lt;/math&gt;</P><P>&lt;math&gt;position[] = position[] + v[]&lt;/math&gt;</P><P>where &lt;math&gt;c_{1}&lt;/math&gt; and &lt;math&gt;c_{2}&lt;/math&gt; are two learning factors, we choose
&lt;math&gt;c_{1}=c_{2}=2&lt;/math&gt;. Particles' velocities on each dimension are clamped to a
maximum velocity &lt;math&gt;V_{\max }&lt;/math&gt;. If the sum of accelerations would
cause the velocity on that dimension to exceed &lt;math&gt;V_{\max }&lt;/math&gt;. Then the
velocity on that dimension is limited to &lt;math&gt;V_{\max }&lt;/math&gt;. If a
particle's position go beyond the range of parameter, we will set
the parameter on that dimension to the boundary condition and let
the sign of velocity on that dimension change meanwhile in order to
simulate reflection.
</P><H2><SPAN class="mw-headline" id="Performance_of_PSO">Performance of PSO</SPAN></H2><P>In contrast with simulated annealing, another method we used to optimize parameters, PSO performs
much better both on the efficiency and accuracy. Specifically, PSO is good at dealing with very
large parameters because it is a global optimization algorithm.
</P><H2><SPAN class="mw-headline" id="Reference">Reference</SPAN></H2><UL><LI><A href="http://www.swarmintelligence.org/" class="external free" rel="nofollow">http://www.swarmintelligence.org/</A></LI></UL><UL><LI> Eberhart, R.C. and Kennedy, J (1995). A new optimizer Using Particle Swarm Theory, Proc. Sixth International Symposium on Micro Machine and Human Science
</LI></UL><H2><SPAN class="mw-headline" id="Data_Structure_and_Organization">Data Structure and Organization</SPAN></H2><PRE>
struct reactor{
	int rnum;//number of reactants
	char **ract;//names of reactants
};

struct product{
	int pnum;//number of products
	char **prdt;//names of products
};

struct reaction{
	int num;//reaction number
	struct reactor RACT;//info of reactants
	struct product PRDT;//info of products
	int type;//reaction type as listed in keneticlaw.pdf
	double *para;//all parameters
	char note[MAXLEN];//self-defined notes
};
</PRE><H1><SPAN class="mw-headline" id="Global_Sensitivity_Analysis_.28GSA.29">Global Sensitivity Analysis (GSA)</SPAN></H1><P>A general objective in our modeling process is an exploration of the
high-dimensional input variable space as thoroughly as possible for
its impact on observable system behavior, often with either
optimization in mind or simply for achieving a better understanding
of the phenomena involved. Since the system input-&gt;output behavior
is typically a nonlinear relationship, simple logic suggests that
the number of runs could grow exponentially with the number of input
variables. However, an emerging family of high dimensional model
representation concepts and techniques capable of dealing with such
input-&gt;output problems in a practical fashion. RS-HDMR is a global
sensitivity analysis technique that can decompose the
high-dimensional, nonlinear contributions of each parameter to the
network properties(represented by their total sensitivity) into a
hierarchy of low-dimensional terms. Calculations by RS-HDMR require
only the ODE model, an estimate of the initial conditions and an
estimate of the dynamic range to explore for each parameter.
</P><H2><SPAN class="mw-headline" id="Introduction_to_GSA">Introduction to GSA</SPAN></H2><P><B> What's GSA?</B></P><P><B> Why GSA?</B></P><H2><SPAN class="mw-headline" id="Theoretical_Foundations_of_GSA">Theoretical Foundations of GSA</SPAN></H2><P>High dimensional model representation(HDMR) is a general set of
quantitative model assessment and analysis tools for capturing high
dimensional IO system behavior. As the impact of the multiple input
variables on the output can be independent and cooperative, it is
natural to express the model output &lt;math&gt;f(x)&lt;/math&gt; as a finite
hierarchical correlated function expansion in terms of the input
variables:
</P><P>&lt;math&gt;f(x)=f_{0}+\sum_{i=1}^{n}f_{i}(x_{i})+\sum_{1\leq i&lt;j\leq n}
f_{ij}(x_{i}, x_{j})+ \sum_{1\leq i&lt;j&lt;k\leq
n}f_{ijk}(x_{i},x_{j},x_{k})+...+\sum_{1\leq i_{1}&lt;...&lt;i_{l}\leq
n}f_{i_{1}i_{2}...i_{l}}(x_{i1},x_{i2},...,x_{il})
+...+f_{12...n}(x_{1},x_{2},...,x_{n})&lt;/math&gt;</P><P>Where the zeroth-order component function &lt;math&gt;f_{0}&lt;/math&gt; is a
constant representing the mean response to &lt;math&gt;f(x)&lt;/math&gt; , and
the first order component function &lt;math&gt;f_{i}(x_{i})&lt;/math&gt;   gives
the independent contribution to &lt;math&gt;f(x)&lt;/math&gt;  by the ith input
variable acting alone, the second order component function
&lt;math&gt;f_{ij}(x_{i},x_{j})&lt;/math&gt; gives the pair correlated
contribution to &lt;math&gt;f(x)&lt;/math&gt;  by the input variables
&lt;math&gt;x_{i}&lt;/math&gt; and &lt;math&gt;x_{j}&lt;/math&gt;, etc. The last term
contains any residual nth order correlated contribution of all input
variables. The basic conjecture underlying HDMR is that the
component functions arising in typical real problems are likely to
exhibit only low order l cooperativity among the input variables
such that the significant terms in the HDMR expansion are expected
to satisfy the relation: &lt;math&gt;l&lt;&lt;n&lt;/math&gt; for &lt;math&gt;n&gt;&gt;l&lt;/math&gt;
. An HDMR expansion to second order
</P><P>&lt;math&gt;f(x)=f_{0}+\sum_{i=1}^{n}f_{i}(x_{i})+\sum_{1\leq i&lt;j\leq n}
f_{ij}(x_{i}, x_{j})&lt;/math&gt;</P><P>often provides a satisfactory description of &lt;math&gt;f(x)&lt;/math&gt;  for
many high dimensional systems when the input variables are properly
chosen. This is also the formula we use to achieve HDMR expansion.
How to determine HDMR component functions? Practical formulations of
the HDMR component functions determine whether we can carry out this
algorithm. Theoretically, a component function
</P><P>&lt;math&gt;f_{i_{1}i_{2}...i_{l}}(x_{i1},x_{i2},...,x_{il})&lt;/math&gt;</P><P>is obtained by minimizing the functional,
</P><P>&lt;math&gt;\min_{f_{i_{1}i_{2}...i_{l}}(u_{i1},u_{i2},...,u_{il}}\int_{\Omega}\omega_{i_{1}i_{2}...i_{l}}(\hat{x},u)
[f(u)-f_{0}-\sum_{i=1}^{n}f_{i}(u_{i})-\sum_{1\leq i&lt;j\leq n} f_{ij}(u_{i}, u_{j})-...- \sum_{1\leq
i_{1}&lt;...&lt;i_{l}\leq
n}f_{i_{1}i_{2}...i_{l}}(u_{i1},u_{i2},...,u_{il})]^{2}du&lt;/math&gt;</P><P>under a suitable specified orhogonality condition which guarantees
that all the component functions are determined step by step.
Here,&lt;math&gt;\hat{x}=(x_{i_{1}},x_{i_{2}},...,x_{i_{n}}&lt;/math&gt;  ,
&lt;math&gt;\omega_{i_{1}i_{2}...i_{l}}(\hat{x},u)&lt;/math&gt; is a weight
function. Different weight functions will produce distinct, but
formally equivalent HDMR expansions. There are two commonly used
HDMR expansions: Cut- and RS(Random Sampling)-HDMR. Cut-HDMR
expresses &lt;math&gt;f(x)&lt;/math&gt;  in reference to a specified cut point,
while RS-HDMR depends on the average value of &lt;math&gt;f(x)&lt;/math&gt; over
the whole domain &lt;math&gt;\Omega&lt;/math&gt; . We adopt RS-HDMR to do global
sensitivity analysis. For RS-HDMR, we first rescale variables
&lt;math&gt;x_{i}&lt;/math&gt; such that &lt;math&gt;0&lt;x_{i}&lt;1&lt;/math&gt; for all i. The
output function &lt;math&gt;f(x)&lt;/math&gt;  is then defined in the unit
hypercube
</P><P>&lt;math&gt;K^{n}={(x_{1},x_{2},...x_{n})|0&lt;\leq x_{i}\leq 1,
i=1,2,...,n}&lt;/math&gt;</P><P>by suitable transformations. The component functions of RS-HDMR possess the following forms:
</P><P>&lt;math&gt;f_{0}=\int_{K^{n}}f(u)du&lt;/math&gt;</P><P>&lt;math&gt;f_{i}(x_{i})=\int_{K^{n-1}}f(x_{i},u^{i})du^{i}-f_{0}&lt;/math&gt;</P><P>&lt;math&gt;f_{ij}(x_{i},x_{j})=\int_{K^{n-2}}f(x_{i},x_{j},u^{ij})du^{ij}-f_{i}(x_{i})-f_{j}(x_{j})-f_{0}&lt;/math&gt;</P><P>Where &lt;math&gt;du^{i}&lt;/math&gt;  and &lt;math&gt;du^{ij}&lt;/math&gt;  are just the
product &lt;math&gt;du_{1}du_{2}...du_{n}&lt;/math&gt;  without
&lt;math&gt;du_{i}&lt;/math&gt; and &lt;math&gt;du_{ij}&lt;/math&gt; respectively.
Evaluation of the high dimensional integrals in the RS-HDMR
expansion is carried out by the Monte Carlo random sampling.
Orthogonality conditions are used to obtain the above formulas. The
integral of a component function of RS-HDMR with respect to any of
its own variables is zero, i.e.,
</P><P>&lt;math&gt;\int_{0}^{1}f_{i_{1}i_{2}...i_{n}}(x_{i_{1}},x_{i_{2}},...,x_{i_{l}})dx_{s}=0,
s\in{i_{1},i_{2},...,i_{l}}&lt;/math&gt;</P><P>Using the orthobonality property of the RS-HDMR component functions,
it can be proven that the total variance of &lt;math&gt;f(x)&lt;/math&gt; caused
by all input variables sampled uniformly over their full range may
be decomposed in to distinct input contributions in the following
manner.
</P><P>&lt;math&gt;\begin{align}
\sigma_{\bar{f}}^{2} &amp;= \int_{K^{n}}[f(x)-\bar{f}]^{2}dx \\
&amp;= \sum_{i=1}^{n}\int_{0}^{1}f_{i}^{2}(x_{i})dx_{i}+\sum_{1\leq i&lt; j \leq n} \int_{0}^{1}\int_{0}^{1}f_{ij}^{2}(x_{i},x_{j})dx_{i}dx_{j}+... \\
&amp;= \sum_{i=1}^{n}\sigma_{i}^{2}+\sum_{1\leq i&lt; j \leq n}\sigma_{ij}^{2}+... \\
\end{align}\,\!&lt;/math&gt;</P><P>Thus, the total variance &lt;math&gt;\sigma_{\bar{f}}^{2}&lt;/math&gt;  is the
sum of first-order variances &lt;math&gt;\sigma_{i}^{2}&lt;/math&gt; ,
second-order covariances &lt;math&gt;\sigma_{ij}^{2}&lt;/math&gt; , etc. The
magnitudes of the indices &lt;math&gt;\sigma_{i}^{2}&lt;/math&gt;,
&lt;math&gt;\sigma_{ij}^{2}&lt;/math&gt; reveal how the output uncertainty is
influenced by the input uncertainties and the nature of the
cooperativities that exist. The direct determination of the
component functions of RS-HDMR at different values in the unit
hypercube by Monte Carlo integration can require a large number of
random samples. To reduce the sampling effort, the RS-HDMR component
functions may be approximated to any desired level of accuracy by
either analytical approximation or numerical approximation. In our
code, we implement the former approximation, which approximate the
RS-HDMR component function by expansion in terms of a suitable set
of functions. Specifically, orthogonal polynomials \cite{paper7} are
used as a basis to approximate &lt;math&gt;f_{i}(x_{i})&lt;/math&gt; ,
&lt;math&gt;f_{ij}(x_{i},x_{j})&lt;/math&gt; as follows:
</P><P>&lt;math&gt;f_{i}(x_{i})\sum_{k=1}^{\infty}\alpha_{k}^{i}\phi_{k}(x_{i})&lt;/math&gt;</P><P>&lt;math&gt;f_{ij}(x_{i},x_{j})=\sum_{k,l=1}^{\infty}\beta_{kl}^{ij}\phi_{k}(x_{i})\phi_{l}(x_{j})&lt;/math&gt;</P><P>&lt;math&gt;...&lt;/math&gt;</P><P>In most cases, to achieve a desired accuracy using
&lt;math&gt;\phi_{1}(x)&lt;/math&gt; ,&lt;math&gt;\phi_{2}(x)&lt;/math&gt;  and
&lt;math&gt;\phi_{3}(x)&lt;/math&gt; is sufficient. Description of the use of
orthonormal polynomials in details can be found in ref\cite{paper7},
we only give some results here.
</P><P>&lt;math&gt;\begin{align}
\sigma_{i}^{2} &amp;= \int_{0}^{1}f_{i}^{2}(x_{i})dx_{i}\approx \int_{0}^{1}[\sum_{k=1}^{s_{1}}\alpha_{k}^{i}\phi_{k}(x_{i})]dx_{i} \\
&amp;= \sum_{k=1}^{s_{l}}(\alpha_{k}^{i})^{2} \\
\end{align}\,\!&lt;/math&gt;</P><P>&lt;math&gt;\begin{align}
\sigma_{ij}^{2} &amp;= \int_{0}^{1}\int_{0}^{1}f_{ij}^{2}(x_{i},x_{j})dx_{i}dx_{j} \\
&amp;\approx \int_{0}^{1}\int_{0}^{1}[\sum_{k=1}^{s_{1}}\sum_{k=1}^{s_{1}'}\beta_{kl}^{ij}\phi_{k}(x_{i})\phi_{l}(x_{j})]^{2}dx_{i}dx_{j} \\
&amp;= \sum_{k=1}^{s_{1}}\sum_{k=1}^{s_{1}'}(\beta_{kl}^{ij})^{2}\\
\end{align}\,\!&lt;/math&gt;</P><P>Only one random sampling set of &lt;math&gt;f(x)&lt;/math&gt;  is needed to
determine &lt;math&gt;f_{0}&lt;/math&gt;  and all the expansion coefficients and
consequently,&lt;math&gt;\sigma_{\bar{f}}^{2}&lt;/math&gt;,
&lt;math&gt;\sigma_{i}^{2}&lt;/math&gt; , &lt;math&gt;\sigma_{ij}^{2}&lt;/math&gt;  . This
dramatically reduces the sampling effort and makes global
sensitivity analysis of a model very efficient.
</P><H1><SPAN class="mw-headline" id="Local_Sensitivity_Analysis_.28LSA.29">Local Sensitivity Analysis (LSA)</SPAN></H1><P>Besides global sensitivity analysis, we also implement additional
local sensitivity analysis technique, which can measure time-varying
sensitivities along arbitrary trajectories. For local methods, the
basic measures of sensitivity are the partial derivatives.
</P><P>&lt;math&gt;\frac{\partial(Model Responses)}{\partial (Model
Parameters)}&lt;/math&gt;,
</P><P>called the sensitivity coefficients of the model\cite{paper8}. We
adopt the method proposed by \cite{paper9}, which can determine the
sensitivity to the perturbation throughout the time evolution of the
system, regardless of the nature of the trajectory. The network is
modeled by the ordinary differential equation
</P><P>&lt;math&gt;\frac{d}{dt}\mathbf{s}(t)=N\mathbf{v}(\mathbf{s}(t),\mathbf{p},t)&lt;/math&gt;</P><P>where the n-vector s is composed of the concentrations of each
species, the constant r-vector p is composed of the parameters of
interest in the model, the m-vector valued function
</P><P>&lt;math&gt;\mathbf{v}=\mathbf{v}(s, p, t)&lt;/math&gt;</P><P>describes the rate of each reaction as function of species
concentrations and parameter values.
</P><H2><SPAN class="mw-headline" id="Introduction_to_LSA">Introduction to LSA</SPAN></H2><P><B> What's LSA?</B></P><P><B> Why LSA?</B></P><H2><SPAN class="mw-headline" id="Basic_Definitions">Basic Definitions</SPAN></H2><P>Definition 1 Given an initial condition &lt;math&gt;s(0)=s_{0}&lt;/math&gt; and
a set of parameter values &lt;math&gt;p_{0}&lt;/math&gt; , which together for a
vector&lt;math&gt;q_{0}&lt;/math&gt; ,the time-varying concentration
sensitivity coefficients(or concentration response coefficients) are
define as the elements of the &lt;math&gt;n\times(n+r)&lt;/math&gt;  matrix
function &lt;math&gt;R_{q}^{s}(t)&lt;/math&gt;  given by
</P><P>&lt;math&gt;R_{q}^{s}(t):=\frac{\partial s(t,q)}{\partial q}|_{q=q_{0}}=\lim_{\Delta q\rightarrow0}
\frac{s(t,q_{0}+\Delta q)-s(t,q_{0})}{\Delta q}&lt;/math&gt;</P><P>for all &lt;math&gt;t\geq0&lt;/math&gt;
The response coefficient defined above provides a measure of the
difference between this &quot;perturbed trajectory&quot; and the &quot;nominal&quot;
trajectory at each time t. Definition 2 Given an initial condition
&lt;math&gt;s(0)=s_{0}&lt;/math&gt; and a set of parameter values
&lt;math&gt;p_{0}&lt;/math&gt; , which together for a vector &lt;math&gt;q_{0}&lt;/math&gt;
,the time-varying rate sensitivity coefficients(or rate response
coefficients) as the elements of the &lt;math&gt;m\times r&lt;/math&gt;  matrix
&lt;math&gt;R_{q}^{v}(t)&lt;/math&gt; function given by,
</P><P>&lt;math&gt;R_{q}^{v}(t)=\frac{\partial v(s(t,q),p,t)}{\partial
q}|_{q=q_{0}}&lt;/math&gt;</P><P>for all, where the derivatives are evaluated at  and The rate
response coefficients give the response in the rates at time t to a
perturbation at time zero.
</P><H2><SPAN class="mw-headline" id="Computation">Computation</SPAN></H2><P>The sensitivity coefficients are defined by a first-order linear
ordinary differential equation,
</P><P>&lt;math&gt;\frac{d}{dt}\frac{\partial s(t,q)}{\partial q}=N[\frac{\partial v(t)}{s}\frac{\partial s(t)}{\partial q}+
\frac{\partial v(t)}{\partial q}]&lt;/math&gt;</P><P>for all &lt;math&gt;t \geq 0&lt;/math&gt;. The choice of
&lt;math&gt;s(0)=s_{0}&lt;/math&gt; and &lt;math&gt;p_{0}&lt;/math&gt; fix a trajectory
&lt;math&gt;s(t)=s(t,s_{0},p_{0}&lt;/math&gt; , and hence determines the
function &lt;math&gt;v(t)=v(s(t),p,t)&lt;/math&gt; as well. The equation above
can be solved numerically for the concentration sensitivities. In
performing such a calculation, we must note that the right-hand side
depends on the time-varying value of the species concentration
vector s(t), so a simple computational strategy is to determine
&lt;math&gt;s_{i}(.)&lt;/math&gt; and &lt;math&gt;\partial s_{i}(.)/\partial q&lt;/math&gt;
simultaneously.
</P><DIV class="printfooter">
Retrieved from &quot;<A href="http://2009.igem.org/Team:USTC_Software/Algorithm">http://2009.igem.org/Team:USTC_Software/Algorithm</A>&quot;</DIV></DIV></DIV><DIV id="footer-box"><DIV id="footer"><UL id="f-list"><LI id="t-recentchanges"><A href="/Special:RecentChanges" title="Recent changes">Recent changes</A></LI><LI id="t-whatlinkshere"><A href="/Special:WhatLinksHere/Team:USTC_Software/Algorithm" title="List of all wiki pages that link here [j]" accesskey="j">What links here</A></LI><LI id="t-recentchangeslinked"><A href="/Special:RecentChangesLinked/Team:USTC_Software/Algorithm" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</A></LI><LI id="t-specialpages"><A href="/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</A></LI><LI><A href="/Special:Preferences">My preferences</A></LI></UL></DIV><DIV id="footer"><UL id="f-list"><LI id="t-print"><A href="/wiki/index.php?title=Team:USTC_Software/Algorithm&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</A></LI><LI id="t-permalink"><A href="/wiki/index.php?title=Team:USTC_Software/Algorithm&amp;oldid=114087" title="Permanent link to this revision of the page">Permanent link</A></LI><LI id="privacy"><A href="/2009.igem.org:Privacy_policy" title="2009.igem.org:Privacy policy">Privacy policy</A></LI><LI id="disclaimer"><A href="/2009.igem.org:General_disclaimer" title="2009.igem.org:General disclaimer">Disclaimers</A></LI></UL></DIV></DIV></DIV></BODY></HTML>