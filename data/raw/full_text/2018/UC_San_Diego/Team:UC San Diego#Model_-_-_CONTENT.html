<HTML lang="en" dir="ltr" class="client-nojs">
<style type="text/css">
A:before { content:' '; } 
A:after { content:' '; } 
SPAN:before { content:' '; } 
SPAN:after { content:' '; } 
</style>
<BODY class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Team_UC_San_Diego_Model skin-igem action-view"><DIV id="globalWrapper"><DIV id="content" class="mw-body" role="main"><DIV id="top_title"><H1 id="firstHeading" class="firstHeading"><SPAN dir="auto">Team:UC San Diego/Model</SPAN></H1></DIV><DIV id="HQ_page"><DIV id="bodyContent"><DIV id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><DIV id="navbar"><DIV class="navlink">People
      <DIV class="navdrop"><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Team">Our Team</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Collaborations">Collaborations</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Attributions">Attributions and Sponsors</A></DIV></DIV></DIV><DIV class="navlink">Application
      <DIV class="navdrop"><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Design">Product Design</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Entrepreneurship">Entrepreneurship</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Database">Database</A></DIV></DIV></DIV><DIV class="navlink">Human Practices
      <DIV class="navdrop"><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Human_Practices_Silver">Human Practices Silver</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Human_Practices">Integrated Human Practices</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Public_Engagement">Education and Public Engagement</A></DIV></DIV></DIV><DIV class="navlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Model">Modeling</A></DIV><DIV class="navlink firstnav">Our Project
      <DIV class="navdrop"><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Design_Philosophy">Our Design Philosophy</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Description">Description</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Applied_Design">Design</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/HC">Hepatocellular Carcinoma</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Notebook">Protocols and Notebook</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Results">Parts and Results</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Medal">Medal Criteria</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/InterLab">InterLab</A></DIV><DIV class="subnavlink"><A href="https://2018.igem.org/Team:UC_San_Diego/Safety">Safety</A></DIV></DIV></DIV></DIV><DIV class="igem_2018_team_content"><DIV class="igem_2018_team_column_wrapper"><DIV id="wrapper"><DIV class="section"><CENTER><H2>Model</H2></CENTER><H3>Introduction</H3><P> Our team generated mathematical models in order to better characterize our overall workflow. The primary task was to use these rigorous approaches in order to determine specific disease-specific markers (i.e. regions of promoter methylation that were consistent across all patients for a particular disease). Although literature provided some biomarkers, our team believed that due to the novelty of our approach, characterizing existing datasets and measurements of beta methylation values to derive a new panel of biomarkers would be more effective. </P><P> Cancer patients are known to show abnormally high levels of cell-free DNA in the plasma which is typically derived from cancer cells that have undergone apoptosis or programmed cell death. The optimal means to characterize a tumor through the methylation profile of the plasma DNA is to derive a set of genes that are all incurring high levels of hyper-methylation. Our team used measurements of <B> 485,000 unique CpG markers to guide our DNA probe design </B> and help <B> characterize our understanding of the disease state fundamentals</B>. Along the way, we expanded the modularity of our approach so that it can be used for other diseases and enabled this functionality within our biomarker discovery tool.  </P><P> Key achievements from our modeling were </P><OL><LI><P> Developed a model to inform DNA probe design for any methylome data </P></LI><LI><P> Created a primer design script to generate a list of all possible methylation positions </P></LI><LI><P> Made our Random Forest algorithms publicly available to other iGEM teams </P></LI><LI><P> Expanded cross-platform functionality for any TCGA methylome data set </P></LI><LI><P> Made a significant advance in using epigenetic parameters to guide wetlab circuit design </P></LI></OL><H3>Using Unsupervised Machine Learning to Inform our Project Design</H3><P>Using a large sample size of both affected and control tissue (n=2191), a HCC classifier was constructed with high specificity and sensitivity. A classifier determines which biomarkers within a specific tissue type can be used to predict the overall disease state of the sample. The goal of these algorithms was to identify a subset of hyper-methylated promoter regions that could accurately identify tumorous tissue samples based on the methylation readouts at these individual loci. To increase sensitivity and specificity, it was critical to select a set of markers that had high specificity to avoid including healthy patients in the diagnostic outcome and to ensure all diseased patients were being accounted for. </P><H3> Overview of dataset </H3><P> This study involved the analysis of 485,000 unique CpG markers to generate a final set of CpG markers that were enhanced in HCC patients. The following data represent the sample characteristics in the training and validation cohort used by the algorithm. <B> This data was integrated with bisulfite sequencing measurements from IlluminaSeq 450 microarray platform and formed the starting point of our data set</B>. To get an extract of the methylation dataset, please email ucsdigem@gmail.com. We are not making the dataset public at the moment due to a confidentiality agreement with one of our laboratory collaborators. The following screenshots show the first and last five rows respectively of the post-processed data. </P><P> PatientType - This is the binary classifier used to distinguish whether or not the tissue sample is HCC(1) or normal(0). The data is organized such that all of the HCC patient data (1221 samples) is first and is followed by the healthy patient data (970 samples). </P><H3>Explaining the Theory Behind our Model</H3><P> In order for our team to develop the model, <B> we had to make a foundational assumption that CpG markers that have a maximal difference in methylation between the two sample types would most likely demonstrate detectable methylation differences in the cfDNA of HCC patients. A second foundational assumption was that we would only consider hypermethylated markers and ignore hypomethylated markers as these are less abundant and overall global hypomethylation is difficult to ascertain with certain techniques. </B> Our team then used <B> Random Forest algorithms </B>  and <B> LASSO (Least Absolute Shrinkage and Selection Operator)</B> to further reduce the biomarker list. In selecting the markers, the t-statistic method with Empirical Bayes was used to shrink the variance and Benjamini-Hochberg procedure was used to control the false discovery rate at a significance level of 0.05. Using these methods and values, the data was processed to roughly 450 markers. By reducing the markers to a smaller subset, we can ensure that only the markers that are most differentially methylated and have a dynamic methylation range are used for the clustering.</P><P> The Random Forest algorithm is a supervised machine learning algorithm that can be used to solve both classification and regression problems. It is a robust algorithm that avoids over-fitting by creating decisions trees from randomly selected data samples and generating predictions from each tree. The algorithm then takes predictions from all of the trees to generate the relative feature importance for the model. It was essential to perform several data processing steps prior to running this algorithm to separate the data into feature variables (methylation markers) and target variables (HCC or healthy). The input and output of Random Forest are listed below. Note that the accuracy of the model improves if the number of estimators is increased, but this also introduces higher computational burden and the chance of overfitting the data. </P><H4> Input </H4><OL><LI><P> Number of Estimators - the number of decision trees in the forest</P></LI><LI><P> Number of Features - the total number of significant features to output from the resulting decision tree</P></LI></OL><H4> Output </H4><OL><LI><P> Summary Statistics - error metrics, confusion matrix, and classification report</P></LI><LI><P> Sensitivity Analysis - AUROC curve</P></LI><LI><P> Feature List - list of identified features and their respective weights </P></LI></OL><P> Similar to the Random Forest procedure, LASSO is used to shrink the total number of features being used to model the data. This algorithm uses a process called regularization which penalizes features in the data in order to keep only the most important factors. The most important parameter in a LASSO regression is the alpha value. When alpha=0, a simple linear regression is being performed on the data. When alpha approaches 1 however, most of the feature coefficients reach 0 which indicates they have no weight on the model. The only downside of LASSO is that it tends to overfit the data and has a lower predictive capability when compared to Random Forest. </P><H4> Input:</H4><P>Consider the alpha value - The regularization parameter, ranges from 0-1 where alpha = 0 is equivalent to a linear regression and alpha = 1 causes all features to be dropped from the model </P><H4> Output </H4><OL><LI><P> Test Statistics - for each value of alpha, the number of features used and the test/training score for the model</P></LI><LI><P> Feature List - list of identified features and their coefficients </P></LI></OL><P> By running both selection algorithms, we are able to derive a subset of biomarkers that strongly contribute to the classification model. <B> Taking an overlap of the two lists produced from each method yields a set of computationally likely disease-causing methylation loci. </B></P><H3>Outcomes and Analysis</H3><P> The two analyses yielded overlapping markers where used to test sensitivity and specificity against the training and validation data sets. </P><P><B> Random Forest Analysis </B>: # of training observations: 1752, # of test observations:  439 </P><P> In a Random Forest prediction model, feature importances give a sense of which variables have the most effect in these models. The sklearn package has a .feature_importances_ attribute that returns an array of each features importance in the model. <B> To identify the top 30 positions that contribute to the model, we take the top 30 feature weights and organize them in descending order below.</B></P><H3>Random Forest Methylation Biomarkers - (Marker, Feature Weight)</H3><TABLE><TBODY><TR><TD>Biomarker ID</TD><TD>Feature Weight</TD></TR><TR><TD>X4.1324870</TD><TD>0.09226192627385134</TD></TR><TR><TD>X17.80358809</TD><TD>0.059085931369960085</TD></TR><TR><TD>X5.171538557</TD><TD>0.057999353985870285</TD></TR><TR><TD>X4.1324877</TD><TD>0.03303274292587381</TD></TR><TR><TD>X11.47624801</TD><TD>0.03174275980572196</TD></TR><TR><TD>X3.101808857</TD><TD>0.02760567525917319</TD></TR><TR><TD>X17.49295615</TD><TD>0.02695179769504988</TD></TR><TR><TD>X10.135072960</TD><TD>0.026502243602910897</TD></TR><TR><TD>X7.151106060</TD><TD>0.025569011566135204</TD></TR><TR><TD>X10.103534546</TD><TD>0.017009344232785523</TD></TR><TR><TD>X6.31527889</TD><TD>0.016819558850001068</TD></TR><TR><TD>X10.88684020</TD><TD>0.013249692773994187</TD></TR><TR><TD>X2.113931518</TD><TD>0.012937163887263686</TD></TR><TR><TD>X7.151106022</TD><TD>0.011656198731186432</TD></TR><TR><TD>X21.36421467</TD><TD>0.011245567212164166</TD></TR><TR><TD>X10.134141823</TD><TD>0.01078311019638933</TD></TR><TR><TD>X8.53851151</TD><TD>0.010115600105707685</TD></TR><TR><TD>X10.14701815</TD><TD>0.00988707517481865</TD></TR><TR><TD>X1.226296852</TD><TD>0.00823810513451597</TD></TR><TR><TD>X17.47286802</TD><TD>0.008169863614495784</TD></TR><TR><TD>X4.1324849</TD><TD>0.007778442275209609</TD></TR><TR><TD>X6.41528449</TD><TD>0.007534845972666597</TD></TR><TR><TD>X7.27155000</TD><TD>0.007265779808743397</TD></TR><TR><TD>X11.69707285</TD><TD>0.007172146198231417</TD></TR><TR><TD>X8.2879889</TD><TD>0.0067021680826400585</TD></TR><TR><TD>X8.48656343</TD><TD>0.006613567990276688</TD></TR><TR><TD>X18.61143902</TD><TD>0.006435928106157796</TD></TR><TR><TD>X4.1324842</TD><TD>0.006322098138107917</TD></TR><TR><TD>X6.16729606</TD><TD>0.006154748779256597</TD></TR><TR><TD>X12.20522379</TD><TD>0.0061085520577785464</TD></TR></TBODY></TABLE><H3>Confusion Matrix Structure</H3><P> Confusion Matrix &amp; Classification Report: A confusion matrix is a summary of the prediction results generated from the model on a classification problem. </P><TABLE><TBODY><TR><TD>Negative Test</TD><TD>Positive Test</TD><TD>Total</TD></TR><TR><TD>Disease Absent</TD><TD>True Negative (TN)</TD><TD>False Positive (FP)</TD><TD>TN + FP</TD></TR><TR><TD>Disease Present</TD><TD>False Negative (FN)</TD><TD>True Positive (TP)</TD><TD>FN + TP</TD></TR><TR><TD>Total</TD><TD>TN + FN</TD><TD>FP + TP</TD><TD>FP + TP</TD></TR></TBODY></TABLE><P><B> TP </B>: Case where the disease is present and the test predicts the state accurately.</P><P><B>  FP </B> : Case where the disease is absent, but the test fails and predicts it as present.</P><P><B> FN </B> : Case where disease is present, but the test fails and predicts it as absent.</P><P><B> TN</B> : Case where the disease is absent and the test predicts the state accurately. </P><P><B> Accuracy </B> = ( (TP + TN) / Total) </P><P><B>Precision </B>  = (TP / (TP + FP) ) </P><P><B> Sensitivity </B> = (TP / (TP+FN) ) </P><P><B> Specificity </B> = (TN / (TN + FP) ) </P><P><B>  F1 Score </B> - Measure of a binary classificationâ€™s accuracy by taking a weighted average of the recall and precision values. </P><P><B>  Micro Average </B> - Aggregates the contributions of all classes to compute the average metric, used when class imbalance is suspected. (More examples of one class than another) </P><P><B>  Macro Average  </B> - Computes the metric independently for each class and then takes the average, thus treating all classes equally. </P><P><B>  Weighted Average </B> - Each class contribution to the average is weighted by the relative number of examples available for it. </P><H3>Confusion Matrix - Model Data</H3><TABLE><TBODY><TR><TD>Negative Test</TD><TD>Positive Test</TD><TD>Total</TD></TR><TR><TD>Disease Absent</TD><TD>167 (TN)</TD><TD>7 (FP)</TD><TD>174 (TN + FP)</TD></TR><TR><TD>Disease Present</TD><TD>15 (FN)</TD><TD>250 (TP)</TD><TD>265 (FN + TP)</TD></TR><TR><TD>Total</TD><TD>182 (TN  + FN)</TD><TD>257 (FP+TP)</TD><TD>439</TD></TR></TBODY></TABLE><H5> Accuracy = (167+250 / 439) = 0.949886 </H5><H5> Sensitivity = (250 / 250+15) = 0.943396 </H5><H5> Specificity = (167 / 167+7) = 0.959770 </H5><H3>Classification Report- Model Data</H3><TABLE><TBODY><TR><TD>Precision</TD><TD>Recall</TD><TD>F1-score</TD><TD>Support (# of Samples)</TD></TR><TR><TD>0 (Healthy)</TD><TD>0.92 (167/192)</TD><TD>0.96 ( 167/174)</TD><TD>0.94</TD><TD>174</TD></TR><TR><TD>1 (Disease)</TD><TD>0.97 (250/257)</TD><TD>0.94 (250/265)</TD><TD>0.96</TD><TD>265</TD></TR><TR><TD>micro average</TD><TD>0.94</TD><TD>0.94</TD><TD>0.94</TD><TD>439</TD></TR><TR><TD>macro average</TD><TD>0.93</TD><TD>0.94</TD><TD>0.94</TD><TD>439</TD></TR><TR><TD>weighted average</TD><TD>0.94</TD><TD>0.94</TD><TD>0.94</TD><TD>439</TD></TR></TBODY></TABLE><P><B> ROC Analysis </B>: Receiving Operator Characteristic (ROC) curves are typically used to study the output of a classifier for a binary classification problem. In this model, the outputs were labeled as 1 for HCC patient samples and 0 for healthy patient samples. The ROC curve maps the true positive rate (y-axis) against the false positive rate (x-axis) where the top left corner is the ideal point as this indicates a true positive rate of 1 and a false positive rate of 0. We often observe and try to optimize the area under this curve (AUC) which is some value between 0 and 1. In the ROC curve generated by this random forest model, the AUC = 0.9876165690739536. </P><P><B> Random Forest Feature Estimator </B>: As the name suggests, the Random Forest algorithm produces a unique forest of decision trees each time it is run. As a result, each run of the algorithm produces a different set of features with unique weights. The purpose of the feature estimator is to take the output of 50 random forest algorithm instances and generate a weighted list of top features/biomarkers. Since the features included in each individual output may not be the same and may be in different orders depending upon the decision tree that was generated, it is essential to weight all these outputs. The weighted and ranked features have been listed in the table below: </P><P> Note - The biomarkers that overlap with the ones produced in the above random forest analysis are labeled in bold </P><H3>Feature Estimator - (Rank, Marker ID)</H3><TABLE><TBODY><TR><TD>Rank</TD><TD>Biomarker ID</TD></TR><TR><TD>1</TD><TD>X4.1324870 (#1)</TD></TR><TR><TD>2</TD><TD>X4.1324849 (#21)</TD></TR><TR><TD>3</TD><TD>X4.1324842 (#28)</TD></TR><TR><TD>4</TD><TD>X11.64644496</TD></TR><TR><TD>5</TD><TD>X10.88684020 (#12)</TD></TR><TR><TD>6</TD><TD>X11.47624801 (#5)</TD></TR><TR><TD>7</TD><TD>X17.80358809 (#2)</TD></TR><TR><TD>8</TD><TD>X4.1324877 (#4)</TD></TR><TR><TD>9</TD><TD>X11.1102570</TD></TR><TR><TD>10</TD><TD>X7.151106060 (#9)</TD></TR><TR><TD>11</TD><TD>X7.151106022 (#14)</TD></TR><TR><TD>12</TD><TD>X21.36421467 (#15)</TD></TR><TR><TD>13</TD><TD>X5.171538557 (#3)</TD></TR><TR><TD>14</TD><TD>X6.41528449 (#22)</TD></TR><TR><TD>15</TD><TD>X3.101808857 (#6)</TD></TR><TR><TD>16</TD><TD>X7.35301161</TD></TR><TR><TD>17</TD><TD>X17.49295615 (#7)</TD></TR><TR><TD>18</TD><TD>X7.140267061</TD></TR><TR><TD>19</TD><TD>X10.6162175</TD></TR><TR><TD>20</TD><TD>X12.20522379 (#30)</TD></TR><TR><TD>21</TD><TD>X19.14993513</TD></TR><TR><TD>22</TD><TD>X10.135072960 (#8)</TD></TR><TR><TD>23</TD><TD>X6.170494251</TD></TR><TR><TD>24</TD><TD>X22.37813041</TD></TR><TR><TD>25</TD><TD>X15.55569496</TD></TR><TR><TD>26</TD><TD>X1.226296852 (#19)</TD></TR><TR><TD>27</TD><TD>X7.139929764</TD></TR><TR><TD>28</TD><TD>X8.2879889 (#25)</TD></TR><TR><TD>29</TD><TD>X8.124332861</TD></TR><TR><TD>30</TD><TD>X10.3823804</TD></TR></TBODY></TABLE><P> A majority of the biomarkers in the features estimator are bolded indicating they match the features identified in the example random forest run. This is expected since the estimator should encompass the most likely features that will be found in any given Random Forest. </P><H3> LASSO Feature List:</H3><P> The LASSO method was run on the dataset using several different alpha parameters including alpha = [0.0001, 0.01, 0.05, 0.1, and 1]. The training score, test score, and number of features used for each alpha level are indicated below. </P><TABLE><TBODY><TR><TD>Alpha Level</TD><TD>Training Score</TD><TD>Test Score</TD><TD>Number of Features</TD></TR><TR><TD>Alpha=1</TD><TD>0</TD><TD>-0.004419100127760922</TD><TD>0</TD></TR><TR><TD>Alpha=0.1</TD><TD>0.15677933815289924</TD><TD>0.13977060185368662</TD><TD>2</TD></TR><TR><TD>Alpha=0.05</TD><TD>0.4381200357272692</TD><TD>0.42151384889843724</TD><TD>8</TD></TR><TR><TD>Alpha=0.01</TD><TD>0.6807028476075634</TD><TD>0.6585068207316982</TD><TD>26</TD></TR><TR><TD>Alpha=0.0001</TD><TD>0.8291845505629482</TD><TD>0.6949522662970736</TD><TD>393</TD></TR></TBODY></TABLE><P> We can immediately notice that as the value of alpha approaches 0, the number of features used in the model increases and the prediction accuracy also improves. This is accurate with how the alpha parameter works as a value of Alpha=0 is equivalent to running a linear regression and including all of the features. The visualization below is generated by the LASSO analysis and shows the various features with different coefficient magnitudes at the varying alpha levels. The spread for alpha=1 is a straight line along zero coefficient magnitude because all of the features are dropped at this alpha level. The spread for alpha=0.0001 is very similar to the linear regression spread which is the lower bound of alpha. For further analysis, we sort the top 20 features produced from the Alpha=0.01 run and compare them to the results from Random Forest.</P><TABLE><TBODY><TR><TD>Biomarker ID</TD><TD>Alpha=0.0001</TD><TD>Alpha=0.01</TD><TD>Alpha=0.05</TD></TR><TR><TD><B>X17.80358809</B></TD><TD>0.066168</TD><TD>0.14503</TD><TD>0</TD></TR><TR><TD><B>X10.6162175</B></TD><TD>0.06678</TD><TD>0.123655</TD><TD>0</TD></TR><TR><TD><B>X11.1102570</B></TD><TD>0.105655</TD><TD>0.118012</TD><TD>0.082344</TD></TR><TR><TD><B>X21.36421467</B></TD><TD>0.382851</TD><TD>0.113295</TD><TD>0</TD></TR><TR><TD><B>X1.226296852</B></TD><TD>0.064078</TD><TD>0.099545</TD><TD>0.002631</TD></TR><TR><TD><B>X3.101808857</B></TD><TD>0.053619</TD><TD>0.096017</TD><TD>0.161699</TD></TR><TR><TD>X14.23291079</TD><TD>0.088821</TD><TD>0.093794</TD><TD>0.029928</TD></TR><TR><TD>X19.18874658</TD><TD>0.045012</TD><TD>0.057451</TD><TD>0.003233</TD></TR><TR><TD>X19.36233435</TD><TD>0.111286</TD><TD>0.053625</TD><TD>0</TD></TR><TR><TD><B>X8.2879889</B></TD><TD>0.017818</TD><TD>0.04995</TD><TD>0.014396</TD></TR><TR><TD><B>X6.41528449</B></TD><TD>0.070034</TD><TD>0.045128</TD><TD>0</TD></TR><TR><TD>X5.172982345</TD><TD>0.033804</TD><TD>0.039364</TD><TD>0</TD></TR><TR><TD><B>X11.69707285</B></TD><TD>0.021109</TD><TD>0.033732</TD><TD>0</TD></TR><TR><TD>X17.55456535</TD><TD>0.08954</TD><TD>0.022602</TD><TD>0</TD></TR><TR><TD>X1.53794245</TD><TD>0.093721</TD><TD>0.017271</TD><TD>0</TD></TR><TR><TD>X6.134491421</TD><TD>0.037699</TD><TD>0.005953</TD><TD>0</TD></TR><TR><TD>X2.127933601</TD><TD>0.023571</TD><TD>0.003084</TD><TD>0</TD></TR><TR><TD>X7.83479508</TD><TD>0.01037</TD><TD>0.002575</TD><TD>0</TD></TR><TR><TD>X6.3247680</TD><TD>-0.020572</TD><TD>0.000515</TD><TD>0</TD></TR><TR><TD>X3.150321178</TD><TD>0</TD><TD>0</TD><TD>0</TD></TR></TBODY></TABLE><P> It is important to note that as we increase the alpha level, some of the features that have a significant weight at a lower level drop out of the model. After sorting based on the alpha=0.01 column, we find an overlapping 9 biomarkers between the Random Forest and LASSO outputs.</P><H3> Making our tool accessible to researchers and iGEM community </H3><P> Although our original intent was to use this tool to aid in biomarker discovery for hepatocellular carcinoma markers, our team realized that the approach and theory behind our model were <B>universal </B> and could be <B> applicable to any existing methylome data </B>. As such, we expanded the modularity of our approach to take methylome data from the TCGA and generate a subsequent classifier. In the proposed state, methylation data from the TCGA or separate Illumina sequencing platform can be ingested into the platform which will output a list of biomarkers to evaluate further. This output will be linked to a primer design and ordering tool which will allow scientists to efficiently find biomarkers for patient samples. The TCGA database contains data from over 8500 tumor samples of 33 tumor types and the Gene Expression Omnibus database contains an additional 60000 samples including several thousand normal blood and tissue samples that can be abstracted as controls. The Illumina Human Methylation 450 microarray-based assay that is used to generate methylation datasets covers over 450,000 loci that include human CpG islands and gene promoter regions that are frequently aberrantly methylated in cancer cells. </P><P> The TCGA integration will make this machine-learning model applicable to cancers outside of HCC. <B> This enables other teams and researchers to rapidly identify potentially hyper-methylated markers for various cancer types and perform a further wet-lab investigation at these markers </B>. With the optimally designed set of biomarkers for these cancers, teams can design probes to assay patient methylation levels at these loci and perhaps implement targeted demethylation techniques. It is also likely that as new patient data is ingested into the platform, new markers that were initially not statistically significant to include will become more relevant across the datasets. Lastly, it is also possible to use the set of all markers across cancers to make predictions on whether a patient is at a predisposition for a combination of multiple common cancers. Overall the availability of these markers will enable more rapid cancer diagnostics alongside the advancement of liquid biopsy sampling </P><P> Once our algorithm generates a list of hypermethylated biomarkers, the primer design tool takes the output loci and allows the researcher to design primers for each position. The primers, which are typically 24-36 basepairs in length, will have a methylated cytosine at the position specified by algorithms. This methylated base must also be 5-10 base pairs within the 3' end of the primer. Due to those restrictions, the primer design tool enables the researcher to generate a variety of different primers for a given locus based on their own experimental design. Future applications of this tool would involve creating a recommendation system for optimal primer design based on the biomarker output. Lastly, this tool can be linked with a primer purchasing platform such as IDT to provide a step by step workflow for the researchers. </P><H3> Conclusion </H3><H3><P> We believe that this epigenetics-driven modeling approach will prove useful to many iGEM teams and the cancer research community as a whole going forward. Using the overlapping technique with LASSO and Random Forest help provide unparalleled discovery ability and its modularity for any disease with existing methylome data makes it a powerful tool for others in their research as well </P></H3><H3>References</H3><OL><LI> http://matthewrocklin.com/blog/work/2015/03/16/Fast-Serialization </LI><LI> https://ianlondon.github.io/blog/pickling-basics/</LI><LI>https://www.datacamp.com/community/tutorials/random-forests-classifier-python </LI><LI> https://medium.com/usf-msds/intuitive-interpretation-of-random-forest-2238687cae45 </LI><LI> https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/ </LI><LI>https://chrisalbon.com/machine_learning/trees_and_forests/random_forest_classifier_example/ </LI><LI> https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it </LI></OL></DIV></DIV></DIV></DIV></DIV></DIV></DIV></DIV></DIV></BODY></HTML>