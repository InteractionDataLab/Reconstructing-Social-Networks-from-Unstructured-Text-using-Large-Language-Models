<HTML lang="en" dir="ltr" class="client-nojs">
<style type="text/css">
A:before { content:' '; } 
A:after { content:' '; } 
SPAN:before { content:' '; } 
SPAN:after { content:' '; } 
</style>
<BODY class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Team_XMU-China_Hardware skin-igem action-view"><DIV id="globalWrapper"><DIV id="content" class="mw-body" role="main"><DIV id="top_title"><H1 id="firstHeading" class="firstHeading"><SPAN dir="auto">Team:XMU-China/Hardware</SPAN></H1></DIV><DIV id="HQ_page"><DIV id="bodyContent"><DIV id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><P><TITLE>Team:XMU-China/Hardware - 2018.igem.org</TITLE></P><DIV id="container"><HEADER><DIV class="wrapper cf"><NAV id="main-nav"><UL class="first-nav"><LI class="Project"><A href="#" target="_blank">Project</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Description">Description</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Design">Design</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Results">Results</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Demonstrate">Demonstrate</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Parts">Parts</A></LI></UL><UL class="second-nav"><LI class="Hardware"><A href="#">Hardware</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Hardware">Overview</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Hardware#Microfluidic_Chips">Microfluidic Chips</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Hardware#Fluorescence_Detection">Fluorescence Detection</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Hardware#Raspberry_Pi">Raspberry Pi</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Hardware#Application">Application</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Software">Software</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Applied_Design">Product Design</A></LI></UL><LI class="Model"><A href="#">Model</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Model#Summary">Summary</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Model#Thermodynamic_model">Thermodynamic Model</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Model#Fluid_dynamics_model">Fluid dynamics Model</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Model#Molecular_docking_model">Molecular Docking Model</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Model#The_dynamic_model">Derivation of Rate Equation</A></LI><LI class="Human_Practice"><A href="#">Social Works</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Human_Practices">Human Practice</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Public_Engagement">Engagement</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Collaborations">Collaborations</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Entrepreneurship">Entrepreneurship</A></LI><LI class="Other_Works"><A href="#">Other Works</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/InterLab">InterLab</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Improve">Improve</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Safety">Safety</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Measurement">Measurement</A></LI><LI class="Notebook"><A href="#">Notebook</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Notebook">Notebook</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Experiments">Experiments</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Engineering">Engineering</A></LI><LI class="Team"><A href="#">Team</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Team">Members</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Attributions">Attributions</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Judging">Judging</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/After_iGEM">After iGEM</A></LI></NAV></DIV></HEADER></DIV><DIV class="header"><DIV class="nav"><DIV id="Team"><DIV class="nav-word">Team</DIV><UL><LI><A href="https://2018.igem.org/Team:XMU-China/Team">Members</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Attributions">Attributions</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Judging">Judging</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/After_iGEM">After iGEM</A></LI></UL></DIV><DIV id="Notebook"><DIV class="nav-word">Notebook</DIV><UL><LI><A href="https://2018.igem.org/Team:XMU-China/Notebook">Notebook</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Experiments">Experiments</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Engineering">Engineering</A></LI></UL></DIV><DIV id="Other_Works"><DIV class="nav-word">Other Works</DIV><UL><LI><A href="https://2018.igem.org/Team:XMU-China/InterLab">InterLab</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Improve">Improve</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Safety">Safety</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Measurement">Measurement</A></LI></UL></DIV><DIV id="Human_Practice"><DIV class="nav-word">Social Works</DIV><UL><LI><A href="https://2018.igem.org/Team:XMU-China/Human_Practices">Human Practice</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Public_Engagement">Engagement</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Collaborations">Collaborations</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Entrepreneurship">Entrepreneurship</A></LI></UL></DIV><DIV id="Model"><DIV class="nav-word">Model</DIV><UL><LI><A href="https://2018.igem.org/Team:XMU-China/Model">Summary</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Model#Thermodynamic_model">Thermodynamic Model</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Model#Fluid_dynamics_model">Fluid dynamics Model</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Model#Molecular_docking_model">Molecular Docking Model</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Model#The_dynamic_model">Derivation of Rate Equation</A></LI></UL></DIV><DIV id="Hardwork"><DIV class="nav-word">Hardware</DIV><UL><LI><A href="https://2018.igem.org/Team:XMU-China/Hardware">Overview</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Hardware#Microfluidic_Chips">Microfluidic Chips</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Hardware#Fluorescence_Detection">Fluorescence Detection</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Hardware#Raspberry_Pi">Raspberry Pi</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Hardware#Application">Application</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Software">Software</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Applied_Design">Product Design</A></LI></UL></DIV><DIV id="Project"><DIV class="nav-word">Project</DIV><UL><LI><A href="https://2018.igem.org/Team:XMU-China/Description">Description</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Design">Design</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Results">Results</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Demonstrate">Demonstrate</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Parts">Parts</A></LI></UL></DIV></DIV></DIV><DIV class="description_banner"><DIV class="word">Hardware</DIV></DIV><NAV class="Quick-navigation"><DIV class="Quick-navigation_word"><A href="#Overview" id="Quick_A">Overview</A><A href="#Microfluidic_Chips" id="Quick_B">Microfluidic Chips</A><A href="#Fluorescence_Detection" id="Quick_C">Fluorescence Detection</A><A href="#Raspberry_Pi" id="Quick_D">Raspberry Pi</A><A href="#Application" id="Quick_F">Application</A></DIV></NAV><DIV class="main Entrepreneurship"><SECTION id="Overview" class="js-scroll-step"><DIV class="headline">
                    Overview
                </DIV><H1>Background</H1><P>Nowadays, most disease-diagnosing methods are confined to specific delicate testing apparatus, which are expensive, time-consuming and low sensitivity. The study of <I>Point-of-care testing</I> (POCT), also called bedside testing (with the definition of medical diagnostic testing at or near the time and place of patient care), has become very heated because of its convenience, simplicity and highly efficiency. <I>Internet of things</I> (IoT) is the network of physical devices, vehicles, home appliances, and other items embedded with electronics, software, sensors, actuators, and connectivity which enables the connection and exchange of data.</P><P>Here we came up with a design. We combined the idea of our project <I>Aptamer-based Cell-free Detection system</I> (ABCD system), IoT, and the above concept of POCT so as to develop a microfluidic device, which is small while convenient for real-time detection of cancer. </P><P>Given that the biomarkers of different cancers are overlapping and the time was limited, we just took the pancreatic cancer as an example to certify the feasibility of our testing theory as well as our device.</P><H1>Designs</H1><P>We gave our testing device a name, &quot;<I>Fang</I>&quot;. &quot;<I>Fang</I>&quot; in Chinese means &quot;Cubic&quot; which can describe our hardware's outlook vividly.
                </P><P>In general,<I>Fang</I> consists of four parts, namely the Microfluidic chip, the fluorescence detection apparatus, <I>Raspberry Pi</I> (RPi) and a mobile-phone application(Software). Among these four parts, Raspberry Pi is the main operating system of the entire device, and its functions include chip-driving controlling, image capturing and server-client data transmission.</P><P>At the beginning, the sample, which is added into our designed microfluidic chip based on ABCD system, will react to Cas12a and give out the fluorescence signal. Then RPi controlled camera captures the image that would be transmitted to App through Socket. In the meantime, the App will convert the image results into visual and readable analysis reports based on its internal machine learning sample database. Finally, App would also achieve Information Sharing between User and Doctor.</P><P class="Figure_word"><STRONG>Figure 1.</STRONG> The draft of our hardware –<I>Fang</I>.</P><P>The overwhelming advantage of <I>Fang</I> is that it gives the best interpretation of POCT. <I>Fang</I> can overcome the drawback that some tumor/cancer detections could only be achieved in the big clinical laboratory by large and expensive equipment. It can be more widely used in the community hospitals, especially in remote areas which are short of necessary medical resources. In addition, <I>Fang</I> can also be used for risk-monitoring by people who have familial-hereditary disease at their home.</P><P class="Figure_word"><STRONG>Figure 2.</STRONG> The physical figure of our hardware –<I>Fang</I>.</P></SECTION><SECTION id="Microfluidic_Chips" class="js-scroll-step"><DIV class="headline">
                    Microfluidic chips
                </DIV><P> According to the model of Aptamer Based Cell Free testing system(ABCD System), we put forward the following design using traditional material for hardware—PMMA as the layers.</P><P> There are two rooms and two pipelines in our microfluidic chips. The first room is Incubation Room, where we has already covered with BAS systems(Biotin-Avidin System). Inspired by the project of <A class="click_here" href="https://2017.igem.org/Team:EPFL/Description/Aptamers">2017 EPFL</A>, we used <STRONG>ternary affinity coating(TERACOAT) method</STRONG><SUP>[1]</SUP> to pre-treat our microfluidic chip, aiming to coat the aptamers which will compete with target protein. The second room is Detection Room, and we put Cas12a (Cfp1) and DNase Alert in advance to achieve it that we convert the protein signal to fluorescence signal. What's more, there is a pipeline we named Pneumatic Valve between Incubation Room and Detection Room. It could control liquid flow through changing the motor speed, which would generate differential air pressure caused by barometric pressure and centrifugal force.</P><P> When the samples are added into loading slot, they would flow into the first room (Incubation Room).The biomarker would compete the complementary sequences in the aptamer, which would flow to the second room (Detection Room) by increasing rotate speed. In the second room, the complementary sequence would activate the Cas12a (Cfp1) to cut DNase Alert. The short sequence between Quencher and Fluorophore is cut and then the quencher wouldn't restrain the fluorophore anymore. Above all, it would give out the green fluorescence we want in detecting room.</P><P class="Figure_word"><STRONG>Figure 3.</STRONG> The Microfluidic Chips Cutaway View(up panel) and the Sandwich Structure of BAS(Biotin-Avidin System)(low panel).</P><H1>References</H1><P class="reference">[1] Piraino F, Volpetti F, Watson C, et al. A Digital–Analog Microfluidic Platform for Patient-Centric Multiplexed Biomarker Diagnostics of Ultralow Volume Samples. <I>Acs Nano</I>, <STRONG>2016</STRONG>, 10(1).</P></SECTION><SECTION id="Fluorescence_Detection" class="js-scroll-step"><DIV class="headline">
                    Fluorescence detection
                </DIV><P> Inspired by the principle of the ultraviolet gel tester and <A class="click_here" href="https://2017.igem.org/Team:EPFL/Description/Aptamers">2014 Aachen</A>, we came up with the fluorescence detection design. Given that the fluorophore of the DNaseAlert (<I>IDT</I>) would be excited by green light with 535 nm and give out light of 565 nm, we selected the green LED lamp beads (which give out light around 535 nm) as light source. The camera with specific optical filter is in the same level with the light. It will get the Emission light from sample and then take a photo of the whole microfluidic chip.</P><P class="Figure_word"><STRONG>Figure 4.</STRONG> The Flow Path of Fluorescence Detection.</P><P> As showed in the figure below (Figure 5), after combination of EpCam and Cas12a protein, the detection room gave out significant green fluorescence compared to other wells which didn’t coat corresponding sequence. This proves that our experimental method of converting protein signals into fluorescent signals is feasible.</P><P class="Figure_word"><STRONG>Figure 5.</STRONG> The group of pictures photographed by our fluorescence detection.</P></SECTION><SECTION id="Raspberry_Pi" class="js-scroll-step"><DIV class="headline">
                    Raspberry Pi
                </DIV><P> The Raspberry Pi (RPi) plays an important role as server, which can receive the command from the APP and apply it to the peripherals (motor and camera). To achieve it, all you need is just clicking the button on the mobile phone. Besides, it's worth mentioning that it can execute the whole series detection processes automatically. We used SSH to make a connection between RPi and IP address, and programmed in RPi. We called <I>Picamera</I> Function by Python to achieve imaging-capture, and called <I>Wringpi</I> by C-language. We made Speed-controlling come to realize by PID speed mode. </P><P class="Figure_word"><STRONG>Figure 6.</STRONG> Circuit Diagram of Raspberry Pi Operating System.</P><P class="Figure_word"><STRONG>Figure 7.</STRONG> The physical map of the Raspberry Pi Operating System.</P><P> The speed acquisition is realized by the encoder and the call interrupt-function. The RPi detects the waveform (square wave) returned by the encoder through the pin. It generates an external trigger when electrical level changes, and then it enters the interrupt function so as to realize counting and come to null. It could convert to the current motor's speed by a pulse count for a fixed time. The maximum output voltage of PWM is only 3.3 V. But the voltage will be applied to the signal input through 12 V battery resistance transformer mode, which can realize 12-0 V voltage regulation, that is, speed regulation in the range of 500-0 rpm. <I>NodeJS</I> calls the corresponding file and executes the corresponding program by receiving signals and parameters, which makes RPi's corresponding to mobile phone's command be realized. More details about the principle of the Raspberry Pi, please switch to our code on <A class="click_here" href="https://github.com/igemsoftware2018/Team_XMU_China">our GitHub page.</A></P><P> From our experimental results, the time required for the sample flowing from the Sample Well to Incubation Room is 15-20 seconds, while the corresponding threshold rotation speed is 280 rpm. After the sample competed for 30 minutes in Incubation Room, we increased the speed to 320 rpm, allowing the sample to flow from Incubation Room to Detection Room, which takes around 30 seconds. (Our experimental speed design is calculated in  our <A class="click_here" href="https://2018.igem.org/Team:XMU-China/Model#Fluid_dynamics_model"> Fluid Dynamics Model page.</A>. Under the guidance of the results, we also consider other practical conditions obtained from experiments)</P></SECTION><SECTION id="Application" class="js-scroll-step"><DIV class="headline ">
                    Application
                </DIV><P> In order to conveniently control our hardware for users, we have designed our own App which can be used with <I>Fang</I>. It is worth mentioning that our App gathers AIT (Artificial Intelligence Technology) and blocks chain technology, equipped with functions providing excellent user experience. According to the navigation bar, we can easily find that App consists of four function modules: Control, Analysis, Interaction and About.</P><P class="Figure_word"><STRONG>Figure 8.</STRONG> Interfacial Design of our Application.</P><P> The first function module named Control, which is used to control the operation of <I>Fang</I>. There is a switch which can control the power-driven machine to turn on or off. And the PHOTO button can be used to control the camera to capture the fluorescent signal, and display it on the mobile phone. In addition, users are free to control the speed of the revolution (Auto/Seton). What's more, users can press the SET button to change the speed and the REQUIRE button to require the current speed.</P><P> The second function module named Analysis aims at taking advantage of AIT to analyze the images that <I>Fang</I> has already photographed. The technology we chose is Tensorflow-the second generationartificial intelligent learning system developed by Google based on DistBelief. When an user takes a photo, the photo will be sent to our cloud server. In the cloud server, there are some modules which have been trained by large amounts of samples with CNN (Convolutional Neural Network). Intelligently, the photo uploaded will match the model by machine learning and user can obtain the information containing analysis results accurately and efficiently from App.</P><P> The third function module named Interaction. It is based on block chain technology. In our cloud server, we have already established a private chain of Ethereum, and we will issue our own digital currency provided for users to deploy smart contract of their own, including all theirdiagnostic messages. There is a One-to-One correspondence between a user and a doctor. When a smart contract is deployed, it allows only one doctor to confirm. Later the doctor can write the therapeutic methods or suggestions into the smart contract to which the patient can refer. On account of its transparency, openness and immutable property, a medical certificate (smart contract) can be corresponded to one specific patient and one specific doctor. Thus, it can effectively avoid medical dispute such as misdiagnose and unscrupulous disavowing.</P><P> The last function module named About. contains introduction of our teamand some users' information.</P><P> Following the function recommended above comes the introduction about the communication mechanism of our App. It is based on three-party communication among Raspberry Pi, App, and cloud server. The instruction sent from App will first arrive cloud server, and will be transmit to Raspberry Pi by server. Actually, the instruction sent from Raspberry will be in a similar way. What's more, the machine learning and mining mechanism of block chain will be operating in the cloud server, thus we can enhance operational efficiency and real-time performance.</P></SECTION></DIV><DIV class="footer"><DIV class="footer_top"><UL><LI><A href="https://2018.igem.org/Team:XMU-China">Home</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Model">Model</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Parts">Parts</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Entrepreneurship">Entrepreneurship</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Attributions">Attributions</A></LI></UL><UL><LI><A href="https://2018.igem.org/Team:XMU-China/Design">Design</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Hardware">Hardware</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Human_Practices">Human Practices</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Notebook">Notebook</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Judging">Judging</A></LI></UL><UL><LI><A href="https://2018.igem.org/Team:XMU-China/Results">Results</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Software">Software</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Collaborations">Collaborations</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/Members">Members</A></LI><LI><A href="https://2018.igem.org/Team:XMU-China/After_iGEM">After iGEM</A></LI></UL></DIV></DIV></DIV></DIV></DIV></DIV></DIV></BODY></HTML>