<HTML lang="en" dir="ltr" class="client-nojs">
<style type="text/css">
A:before { content:' '; } 
A:after { content:' '; } 
SPAN:before { content:' '; } 
SPAN:after { content:' '; } 
</style>
<BODY class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Team_HFUT-China_Model skin-igem action-view"><DIV id="globalWrapper"><DIV id="content" class="mw-body" role="main"><DIV id="top_title"><H1 id="firstHeading" class="firstHeading"><SPAN dir="auto">Team:HFUT-China/Model</SPAN></H1></DIV><DIV id="HQ_page"><DIV id="bodyContent"><DIV id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><P><TITLE>Team:HFUT-China</TITLE></P><DIV class="header" style="z-index:999; position:fixed;left:0px;top:16px;"><UL class="nav"><LI><A href="https://2017.igem.org/Team:HFUT-China">  Main page  </A></LI><LI><A href="https://2017.igem.org/Team:HFUT-China/Description">  Project   </A></LI><LI class="selfnav hef"><A href="https://2017.igem.org/Team:HFUT-China/Description"><FONT class="hef">Description</FONT></A></LI><LI class="selfnav hef"><A href="https://2017.igem.org/Team:HFUT-China/Design"><FONT class="hef">Design</FONT></A></LI><LI class="selfnav hef"><A href="https://2017.igem.org/Team:HFUT-China/Contribution"><FONT class="hef">Contribution</FONT></A></LI><LI class="selfnav hef"><A href="https://2017.igem.org/Team:HFUT-China/Demonstrate"><FONT class="hef">Demonstrate</FONT></A></LI></UL><LI><A href="https://2017.igem.org/Team:HFUT-China/Software">  Software   </A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/Software"><FONT class="hef" color="#000">Software</FONT></A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/Model"><FONT class="hef" color="#000">Model</FONT></A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/Improve"><FONT class="hef" color="#000">Improve</FONT></A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/Unit_Test"><FONT class="hef" color="#000">Unit Test</FONT></A></LI><LI><A href="https://2017.igem.org/Team:HFUT-China/Notebook">  Documents   </A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/Notebook"><FONT class="hef" color="#000">Notebook</FONT></A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/Medals"><FONT class="hef" color="#000">Medals</FONT></A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/Safety"><FONT class="hef" color="#000">Safety</FONT></A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/User_guide"><FONT class="hef" color="#000">User guide</FONT></A></LI><LI><A href="https://2017.igem.org/Team:HFUT-China/Team">Team </A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/Team"><FONT class="hef" color="#000">Members</FONT></A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/Attributions"><FONT class="hef" color="#000">Attributions</FONT></A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/Collaborations"><FONT class="hef" color="#000">Collaborations</FONT></A></LI><LI><A href="https://2017.igem.org/Team:HFUT-China/HP/Silver">  Human practice   </A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/HP/Silver"><FONT class="hef" color="#000">Silver HP</FONT></A></LI><LI class="selfnav"><A href="https://2017.igem.org/Team:HFUT-China/HP/Gold_Integrated"><FONT class="hef" color="#000">Integratedand Gold</FONT></A></LI><LI><A href="https://igem.org/2017_Judging_Form?id=2466" target="blank">  Judging Form  </A></LI></DIV><DIV class="title"><B><FONT color="#555555">Model</FONT><DIV style="width: 76%"><DIV style="margin-top: 90px;font-family: Light; text-align: left;"><B><FONT color="#333" size="6px" weight="bold">1. Latent Dirichlet Allocation (LDA) model</FONT></B></DIV></DIV><DIV class="p"><DIV class="q" style="line-height: 2.5;">For the information of all teams under each track, we tried to let our computers “understand” it, and
                    automatically classify it into groups. Conventional LDA model is used to explore keywords of themes among
                    documents, but here we regard it as an unsupervised classifier, and unsupervised means we don’t have
                    to provide any manually labeled data. As a result it can give us clusters of documents, and documents
                    in the same cluster have the same theme. The picture below better explains how LDA works.
                    <P style="font-size:16px">
                        (1) α represents the key parameter to generate a theme. (2) β stands for the word given theme distribution (p(word|theme)).
                        (3) θ is the theme distribution for the document (p(theme)). (4) z is the theme for words in a document.
                        (5) w is specific words.
                    </P></DIV></DIV><DIV style="width: 76%"><DIV style="margin-top: 90px;font-family: Light;text-align: left;"><B><FONT color="#333" size="6px" weight="bold">2. TF-IDF model</FONT></B></DIV></DIV><DIV class="p"><DIV class="q" style="line-height: 2.5;">TF-IDF refers to Term Frequency–Inverse Document Frequency. It is used in our system to excavate the
                    keywords of a document. It consists of two parts TF value and IDF value. Primarily, we calculate the
                    TF value for each document by simply counting how many times a word appears in the document. As for IDF
                    value of word w_i, it is calculated according to the following formula:
                    The IDF value represents how general a word is, and the higher it is, the less the word is commonly seen.
                    Finally, we combine the TF and IDF value by multiplying them. By doing this,
                    we can filter off some the general words, and keywords are left as we expected.
                </DIV></DIV><DIV style="width: 76%;"><DIV style="margin-top: 90px;font-family: Light; text-align: left;"><B><FONT color="#333" size="6px" family="he" weight="bold">3. Word2Vec</FONT></B></DIV></DIV><DIV class="p"><DIV class="q" style="line-height: 2.5;">Word2Vec plays an important role in our system. Word Vector is an effective and promising substitute
                    for the conventionally used one-hot encoding method in NLP (Natural Language Processing). About one-hot
                    encoding, take a sentence “I love you so much” for example. We want to find a vector to represent each
                    word in this sentence. What one-hot does is that it assigns “1” to the entry in this vector according
                    to the word’s position in the sentence. For example, “love” in one-hot is “0 1 0 0 0” and “so” is “0
                    0 0 1 0”. Nonetheless, this kind of encoding does not contain the semantic meaning of a word. If we want
                    to measure the semantic similarity between two words, unless these words are identical, the similarity
                    will be zero. So researchers proposed “Word Vector”, which is a vector representing the word’s semantic
                    meaning. It takes the context of the word into consideration, and the effect turns out to be really excellent.
                    Thus, the similarity between two words can be easily measured using L2 norm. The whole process of calculating
                    word vectors is through neural networks, and the detailed structure of it can be found 
                    <A href="https://en.wikipedia.org/wiki/Word2vec">here.</A></DIV></DIV><DIV style="width: 76%"><DIV style="margin-top: 90px;font-family: Light; text-align: left;"><B><FONT color="#333" size="6px" weight="bold">4. LSI (Latent Semantic Indexing)</FONT></B></DIV></DIV><DIV class="p"><DIV class="q" style="line-height: 2.5;">Word Vector can only be used to explore the semantic meaning for a word, and if we try to measure the
                    semantic distance between two documents, we will have to find another way. And that’s why we introduced
                    LSI model. LSI uses SVD (Singular Value Decomposition) to find the latent similarity between documents.
                    SVD can be thought as factorization in matrices version. For example, the number 12 can be decomposed
                    to 2×2×3, and SVD does the same thing for matrices. Suppose that we have m documents and n total words.
                    We decompose it as follows:
                    Where A_(i,j) stands for the feature value, which is TF-IDF value of word j in document i generally.
                    We regard U_i, which is the row vector of the matrix U to be the semantic value of document i. The similarity
                    among documents i and j can be calculated using cosine similarity as the following expression:
    </DIV></DIV><DIV style="width: 76%"><DIV style="margin-top: 90px;font-family: Light; text-align: left;"><B><FONT color="#333" size="6px" weight="bold">Reference</FONT></B></DIV></DIV><DIV class="p"><DIV class="q" style="line-height: 2.5;">    <A href="https://commons.wikimedia.org/w/index.php?curid=3610403">1.By Bkkbrad - Own work, GFDL,</A>    </DIV></DIV></B></DIV></DIV></DIV></DIV></DIV></DIV></BODY></HTML>