<HTML lang="en" dir="ltr" class="client-nojs">
<style type="text/css">
A:before { content:' '; } 
A:after { content:' '; } 
SPAN:before { content:' '; } 
SPAN:after { content:' '; } 
</style>
<BODY class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Team_Cambridge-JIC_MicroMaps skin-igem action-view"><DIV id="globalWrapper"><DIV id="content" class="mw-body" role="main"><H1 id="firstHeading" class="firstHeading"><SPAN dir="auto">Team:Cambridge-JIC/MicroMaps</SPAN></H1><DIV id="bodyContent"><DIV id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><DIV class="cam-container"><DIV class="navbar navbar-inverse navbar-fixed-top navbar-pad"><DIV class="container"><DIV class="navbar-header"><A class="navbar-brand" href="//2015.igem.org/Team:Cambridge-JIC">CAMBRIDGE-JIC</A></DIV><DIV class="navbar-collapse collapse"><UL class="nav navbar-nav navbar-right"><LI><A href="//2015.igem.org/Team:Cambridge-JIC">HOME</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Project">PROJECT</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Description">DESCRIPTION</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Design">APPLIED DESIGN</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Make_Your_Own">MAKE YOUR OWN</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Downloads">DOWNLOADS</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Gallery">GALLERY</A></LI></UL></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Hardware">HARDWARE</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Tech_Specs">TECHNICAL SPECIFICATIONS</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Modeling">MODELING</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/3D_Printing">3D PRINTING</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Stretch_Goals">STRETCH GOALS</A></LI></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Software">SOFTWARE</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Webshell">WEBSHELL</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/MicroMaps">MICROMAPS</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Autofocus">AUTOFOCUS</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/ImageJ">IMAGEJ</A></LI></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Practices">COMMUNITY</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Outreach">OUTREACH</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/OpenHardwareRevolution">OPEN HARDWARE LICENSING</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Collaborations">COLLABORATION</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Wiki_Design">WIKI DESIGN</A></LI></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Lab">OUR LAB</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Notebook">NOTEBOOK</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Safety">SAFETY</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Achievements">ACHIEVEMENTS</A></LI></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/People">ABOUT US</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Team">MEET THE TEAM</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Attributions">ATTRIBUTION</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Sponsors">SPONSORS</A></LI></DIV></DIV></DIV><SECTION style="background-color:#fff"><DIV class="slide" style="min-height:0px"><DIV style="width: 100%; padding:0% 10%; margin: 10px 0px;color:#000"><CENTER><H1 style="line-height:1.295em"> MicroMaps </H1></CENTER><CENTER><P><I>The future of microscopy is (almost) here! Catch a sneakpeak of MicroMaps and play around with our early alpha by getting hold of your own OpenScope.</I></P></CENTER></DIV></DIV></SECTION><SECTION style="background-color:#444"><DIV class="slide" style="min-height:0px"><DIV style="width: 100%; padding:0% 10%; margin: 30px 0px;color:#fff"><H3>What is MicroMaps?</H3><P>MicroMaps is the new way to interact with microscopes. MicroMaps combines the simplicity of a Google Maps-like web interface (based on <A href="http://openlayers.org/" class="blue">OpenLayers</A>) with the power of a motorised microscope and automated image annotation. Navigate your slide with ease and never worry about losing focus or your bearings ever again (check out our <A href="//2015.igem.org/Team:Cambridge-JIC/Autofocus" class="blue">Autofocus</A> algorithm)! Say goodbye to tedious cell counting and phenotype searches!</P><P>MicroMaps features:</P><UL><LI><P><B>Freely pan around</B>: automates image-taking and stitching to provide a seamless map of your slide - pan, zoom and rotate your samples</P></LI><LI><P>Using calibration data, <B>measure</B> features with ease, regardless of their orientation or the position of your reticle</P></LI><LI><P>See something you like? <B>capture a raw unprocessed image</B> for later, or <B>drop a pin</B> to return to later!</P></LI><LI><P>Need data? use an <B>extensive automated annotation toolkit</B> to measure and characterise your sample. Looking for a specific phenotype? Want to count your cells? Look no further - all of this with the comfort of knowing that you can manually intervene if the computer gets it wrong!</P></LI></UL><CENTER><P><I>Note: Unfortunately, though most of the groundwork is in place for these features, technical difficulties currently impede our ability to bring all these to you at present. Stay tuned for future updates or read on to see what has already been done behind the scene.</I></P></CENTER><CENTER><P>Can't wait? Try out our <A href="//2015.igem.org/Team:Cambridge-JIC/ImageJ" class="blue">ImageJ plugin</A>.</P></CENTER></DIV></DIV></SECTION><SECTION style="background-color:#fff"><DIV class="slide" style="min-height:0px"><DIV style="width: 100%; padding:0% 10%; margin: 30px 0px;color:#000"><H3>Image Stitching</H3><P>This is the technology that makes MicroMaps all possible...and the reason it is in early alpha. Though the translation mechanism of our microscope allows panning control as fine as a single micron (see our <A href="//2015.igem.org/Team:Cambridge-JIC/Tech_Specs" class="blue">Tech Specs</A> page), the material used for 3D-printing and the quality of the motors used impairs the accuracy with which translation can be achieved. A subtle point of the flexure mechanism, used for stage movement, is that shifting in one direction also causes a small angle twist of the frame of view. This in turn makes it difficult to know which parts of the slide go where in our interface. Luckily, image stitching algorithms have been created to find where two or more images match and combine them together (such as in the panorama feature on modern smartphones). Using these algorithms we can determine precisely how the microscope imagery should be shown on screen and eliminate the seams between them. We can also use the position information derived to correct for translational inaccuracies so we can know with confidence where you are on your slide and enable you to drop pins on features you like. In addition, the overlaying of the stitched images actually removes the black artifacts deposited by dirt on the CCD or imperfections of the optics.</P><P>There are many <B>stitching algorithms</B> that have been developed, though unfortunately some of the best are proprietary. As an open-source alternative, our algorithm is significantly less robust and this has been a major roadblock in the development of MicroMaps. Eventually, we were forced to disable the free panning mechanism on MicroMaps alpha. It has also meant that some of the other features that we had in mind have not been integrated yet.</P><P>Algorithms required for MicroMaps:</P><DL><DT><P>feature detecting algorithms: (these find interesting things in images)</P></DT><DD><P>proprietary: SIFT, SURF, FAST</P></DD><DD><P>free: ORB - actually this algorithm is also pretty good, according to its developers [1]</P></DD><DT><P>feature MATCHING algorithms: (these try to match the same interesting things in two images)</P></DT><DD><P>proprietary: FLANN - fast and reliable</P></DD><DD><P>free: BF - Brute-Force, unreliable</P></DD></DL><P>Our research in the area indicates that the SIFT + FLANN combination is very good. Further understanding of the subject might be gained from Google's <A href="http://www.google.co.uk/maps/about/contribute/photosphere/" class="blue">PhotoSphere</A> project.</P><P><B>Examples:</B></P><CENTER><P><I><B>Figure 1</B>: First successful stitching of two images (Nigerian liane). <B>Figure 2</B>: Stitching implemented on macroscopic images of Marchantia polymorpha as part of our <A href="//2015.igem.org/Team:Cambridge-JIC/Stretch_Goals" class="blue">Stretch Goals</A>. Note the accuracy of the stitching. <B>Figure 3</B>: Pretend stitching (performed manually) - shows how MicroMaps is ultimately intended to work.</I></P></CENTER><P><B>How it works:</B> More concretely, MicroMaps keeps a collection of images it has taken along with the corresponding expected physical coordinates. MicroMaps will request small regions (tiles) of the slide one-by-one to fill up its field of view. When a tile is requested the software will look through its collection to see if it has already captured that region, and will join any seams it finds if multiple images match that tile. If no images match that tile, it will take a series of overlapping images between a nearby (in terms of expected coordinates) image and the desired tile. For each image, it will use the stitching algorithm to determine accurate coordinates representing the image and compare them to the expected coordinates. This is essential to correct for hardware noise and inaccuracies, and will allow a seamless image to be constructed from these small tiles. The accuracy obtained, combined with calibration data, will then allow  for precise measurements to be made. The accurate positioning information will also allow pins to be dropped so interesting features can be returned to later.</P><P><B>Problems:</B> This works well for fixed samples, but what about live samples? With the current difficulties, we are not prepared to apply MicroMaps logic to motile samples. Moving samples are infeasible with current processing delays. For now, we recommend using the <A href="//2015.igem.org/Team:Cambridge-JIC/Webshell" class="blue">WebShell</A>. We are still working on this issue, expect the ability to follow moving specimens in WebShell v2, and perhaps in MicroMaps v2 with some speed improvements.</P><CENTER><P><I>Image stitching, and the whole MicroMaps construct, were developed by Will, with useful feedback and advice from the rest of the Software team.</I></P></CENTER><P style="font-size:80%">References:  [1] Ethan Rublee, Vincent Rabaud, Kurt Konolige, Gary Bradski. ORB: an efficient alternative to SIFT or SURF, <I>Computer Vision (ICCV)</I>, 2011 IEEE International Conference on. IEEE, 2011.</P></DIV></DIV></SECTION><SECTION style="background-color:#444"><DIV class="slide" style="min-height:0px"><DIV style="width: 100%; padding:0% 10%; margin: 30px 0px;color:#fff"><H3>Image Processing</H3><P>The purpose of microscopy is to extract some useful information about the specimen: screen for a particular phenotype, examine fluorescence, measure sizes, count cells, recognize distinctive features, eg. nuclei. Focusing on a specimen is just a small part of the art of microscopy. The actual scientific challenge is to interpret the image. Imagine a program that does this for you. This is what we had in mind when creating MicroMaps. To achieve this, we had to implement different types of image processing algorithms. Image recognition is still work in progress, but we believe that we have laid out the framework for a new, smarter, approach to digital microscopy.</P><P><B>The Method:</B> We tested our image processing software on some images of <I>Marchantia</I> gemma on a Petri dish with agar, This was intended to be a step towards our <A href="https://2015.igem.org/Team:Cambridge-JIC/Stretch_Goals" class="blue">Stretch Goal</A> - an automated screening desktop system. To write the software, the <A href="http://opencv.org/" class="blue">OpenCV</A> library was used. Two types of image processing algorithms were implemented:</P><UL><LI><P><B>Standard thresholding</B>This makes an image grey-scale and searches for the dark areas. We started off with a basic contrast increase to isolate the darker areas of the image, which we assume would correspond to samples. We then followed the steps in a paper [2] which was supposed to yield much better sample isolation for samples which look faint, and are hard to distinguish from their background. This ended up detecting dents in the agar gel along with the samples. To resolve this issue we came up with the next idea...</P></LI><LI><P><B>Colour detection</B>An eye dropper was added to select the upper and lower colour darknesses to search for (the user would click to select these colours). These colours correspond to areas of the sample with better and worse illumination respectively.Also, a slider that allows you to change the 'darkness' of the sample colour was added. This generally varies depending on room lighting conditions. With this implementation, the program performed much better, detecting the <I>Marchantia</I> gemma before the agar dents.</P></LI></UL><CENTER><P><I><B>Figure 4</B>: Sample recognition working on Petri dish with Marchantia gemma. The program highlights the samples it finds in red. Note that the agar dent is not included in the final output. This was achieved using the color detection algorithm.</I></P></CENTER><P><B>Microscopic image processing:</B> The colour detection used above can theoretically be easily adapted to work with fluorescent samples â€“ this would prove useful for sample counting and detection of, for example, samples that successfully express a specific fluorescent protein. A similar strategy can be applied to stained samples with interesting coloured features: for example to recognize stained nuclei (eg. with toluidine blue) and in this way distinguish eukaryotic cells.</P><P>However, we have not implemented sample recognition into MicroMaps Alpha, mostly due to lack of time and difficulties for coping with multicolour images. Still, the script for image recognition is in the <A href="https://github.com/sourtin/igem15-sw/blob/master/img_processing/identificationTesting/marchantiaIdentification_gui.py" class="blue">software package</A> for you to try out (and improve, also in the source code download on the <A href="//2015.igem.org/Team:Cambridge-JIC/Downloads" class="blue">Downloads</A> page under img_processing/identificationTesting/marchantiaIdentification_gui.py).
</P><CENTER><P><I>Image recognition was developed by Ocean, with useful feedback and advice from the rest of the Software team.</I></P></CENTER><P style="font-size:80%">References:  [2] Chen, L., Chien, C. and Nguyen, X. (2013). An effective image segmentation method for noisy low-contrast unbalanced background in Mura defects using balanced discrete-cosine-transfer (BDCT). <I>Precision Engineering</I>, 37(2), pp.336-344.</P></DIV></DIV></SECTION><SECTION id="footer-sec"><DIV class="container"><DIV class="row  pad-bottom"><DIV class="col-md-3"><H4><STRONG>ABOUT US</STRONG></H4><P>
                    We are a team of Cambridge undergraduates, competing in the Hardware track in iGEM 2015.
                    </P><A href="//2015.igem.org/Team:Cambridge-JIC/Team">read more</A></DIV><DIV class="col-md-3"><H4><STRONG>FOLLOW US ON</STRONG></H4></DIV><DIV class="col-md-3"><H4><STRONG>LOCATION</STRONG></H4><P>
                    Department of Plant Sciences, 
                    University of Cambridge  
                    Downing Street 
                    CB2 3EA
                    </P></DIV><DIV class="col-md-3"><H4><STRONG>CONTACT US</STRONG></H4><P>
                    Email: <A href="mailto:igemcambridge2015@gmail.com">igemcambridge2015@gmail.com</A>
                    Tel: <A href="tel:+447721944314">+447721944314</A></P></DIV></DIV></DIV></SECTION></BODY></HTML>