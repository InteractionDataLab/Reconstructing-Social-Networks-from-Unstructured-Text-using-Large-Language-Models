<HTML lang="en" dir="ltr" class="client-nojs">
<style type="text/css">
A:before { content:' '; } 
A:after { content:' '; } 
SPAN:before { content:' '; } 
SPAN:after { content:' '; } 
</style>
<BODY class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Team_Cambridge-JIC_Autofocus skin-igem action-view"><DIV id="globalWrapper"><DIV id="content" class="mw-body" role="main"><H1 id="firstHeading" class="firstHeading"><SPAN dir="auto">Team:Cambridge-JIC/Autofocus</SPAN></H1><DIV id="bodyContent"><DIV id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><DIV class="cam-container"><DIV class="navbar navbar-inverse navbar-fixed-top navbar-pad"><DIV class="container"><DIV class="navbar-header"><A class="navbar-brand" href="//2015.igem.org/Team:Cambridge-JIC">CAMBRIDGE-JIC</A></DIV><DIV class="navbar-collapse collapse"><UL class="nav navbar-nav navbar-right"><LI><A href="//2015.igem.org/Team:Cambridge-JIC">HOME</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Project">PROJECT</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Description">DESCRIPTION</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Design">APPLIED DESIGN</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Make_Your_Own">MAKE YOUR OWN</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Downloads">DOWNLOADS</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Gallery">GALLERY</A></LI></UL></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Hardware">HARDWARE</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Tech_Specs">TECHNICAL SPECIFICATIONS</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Modeling">MODELING</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/3D_Printing">3D PRINTING</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Stretch_Goals">STRETCH GOALS</A></LI></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Software">SOFTWARE</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Webshell">WEBSHELL</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/MicroMaps">MICROMAPS</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Autofocus">AUTOFOCUS</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/ImageJ">IMAGEJ</A></LI></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Practices">COMMUNITY</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Outreach">OUTREACH</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/OpenHardwareRevolution">OPEN HARDWARE LICENSING</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Collaborations">COLLABORATION</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Wiki_Design">WIKI DESIGN</A></LI></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Lab">OUR LAB</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Notebook">NOTEBOOK</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Safety">SAFETY</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Achievements">ACHIEVEMENTS</A></LI></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/People">ABOUT US</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Team">MEET THE TEAM</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Attributions">ATTRIBUTION</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Sponsors">SPONSORS</A></LI></DIV></DIV></DIV><SECTION style="background-color:#fff"><DIV class="slide" style="min-height:0px"><DIV style="width: 100%; padding: 0% 10%; margin: 10px 0px;color:#000"><CENTER><H1>Autofocus</H1></CENTER></DIV></DIV></SECTION><SECTION style="background-color:#444"><DIV class="slide" style="min-height:0px"><DIV style="width: 100%; padding: 0% 10%; margin: 30px 0px;color:#fff"><H3>The Problem</H3><P>Since the start of the OpenScope project, the objective has been to turn our microscope into a fully automated and autonomously operable system. As such, an <B>autofocus</B> feature was essential. Autofocus is especially useful in certain experimental setups including:</P><UL><LI><P>Time lapse imaging - to account for stage drift/settling with time</P></LI><LI><P>Monitoring of motile samples</P></LI><LI><P>Focus stacking - for imaging 3-dimensional objects</P></LI></UL><P>Last, but not least, autofocus simply facilitates the work of the person operating OpenScope. Manual focusing, although not very challenging, might still be fiddly, particularly to inexperienced users. This is where the autofocus algorithm comes in. Various methods for performing this task were tried, and the best methods were used in the final version (although the rest of the code is present in our source code).</P></DIV></DIV></SECTION><SECTION style="background-color:#fff"><DIV class="slide" style="min-height:0px"><DIV style="width: 100%; padding: 0% 10%; margin: 30px 0px;color:#000"><H3>The Solution</H3><P>The strategy of the autofocus algorithm is to calculate the <B>focus score</B> for each frame imaged while gradually changing the sample-objective distance. The most sharply focused image corresponds to the maximised focus score. The physics behind the process is very simple: the image of an object, viewed through a lens, is produced at a particular distance from the lens. At any shorter/longer distance from the lens, the light rays from a single point on the object do not converge into a point, causing the image to be blurred. The focus score can be simply regarded as a mathematical function with a single maximum.</P><CENTER><P><I><B>Fig, 1:</B> The plot shows the focus score of an image as a function of the distance between the objective and the sample: it is clear that the function has a single maximum. The corresponding images recorded at these distances are also shown. Note that most of the time during the search, the sample remains significantly out-of-focus.</I></P></CENTER><P>There are, however, several issues to consider and multiple methods exist for calculating the focus score. In addition, there are many ways to scan around for the best focus score. The challenge was to find the most robust solutions. Below is an outline of the development process:
</P><P>Tested out focus scores based on variance, as this was recommended by both sources [1][2] on artificially blurred out images (Gaussian blur with different standard deviations). This worked well. <B>Variance was chosen as the focus measure to implement since it is quick to evaluate</B> (speed being an important priority), and numpy (a Python library) has a function that does this already.</P><LI><P>Implemented an autofocus method for multi-resolution search (described in source [2], section 16.3.3).</P></LI><LI><P>Implemented DWT (<A href="https://en.wikipedia.org/wiki/Discrete_wavelet_transform" class="blue">Discrete Wavelet Transform</A>) to decompose image to low resolution versions. This was expected to allow for faster computation and autofocusing. In practice, there was no significant computational advantage to this, so low-res images were not used in the final algorithm.</P></LI><P>Having been able to determine a focus score for an image, the next step was to develop methods for finding the most in-focus position. This involves several important issues:</P><UL><LI><P><B>Delays from moving the motor to receiving an updated frame</B> - this is a common issue with feedback loops, everywhere from biological systems to adjusting the temperature of the shower. Failure to deal with this can result in <B>oscillations</B> around the set point, with the image remaining just out of focus most of the time. We dealt with this by introducing a delay between sending a &quot;move motor&quot; command and a &quot;read focus score&quot; command to attempt to match the two. Reading a focus score when the motors were no longer moving is not an issue, but reading a value before the desired position has been reached is. Thus, it is beneficial to err on the side of an increased counter-delay, though at the expense of the time for autofocus to take place.</P></LI><LI><P><B>Unreliability of motor positioning</B> - when moving +400 steps and then -400 steps, there is no guarantee of returning to exactly the same position.</P></LI><LI><P><B>Local maxima not being the global maxima</B> - A more theoretical issue than the practical ones above. Once reaching an area of high focus value, there is little guarantee that this is the most focused plane in the image. This is in contrast to the issue above - if we &quot;give up&quot; an area of high focus, we may find it difficult to retrieve it. We ignored this issue in favour of the more practical ones above - attempting to account for global maxima of focus scores would require the autofocus procedure to be far longer than desirable. We also implemented a naive dynamic threshold to stop moving the z axis once the focus was &quot;good enough&quot;, a value for which is empirically derived.</P></LI><LI><P><B>We settled for a hill-climbing approach</B>. Imagining the theoretical focus score for all possible z values on a graph may look like a hill. This algorithm attempts to &quot;climb&quot; that hill of focus scores. This algorithm assesses the focus scores of individual frames, moving in the direction where the focus is increasing. If the focus decreases, the motors are reversed in smaller increments. This process is repeated several times until an exit condition is reached.</P></LI></UL><P>We tried a variety of other approaches to tackle these issues:</P><UL><LI><P>Implemented the Gaussian approximation to predict in-focus position from the interval. This assumes that the image is formed from paraxial rays only, which is a good approximation in the case of microscopic imaging.</P></LI><LI><P>Implemented the Fibonacci search algorithm. It turns out that this is the optimal search algorithm for the situation of unimodal functions [2].</P></LI><LI><P>Implemented the parabola approximation to find in focus position. </P></LI></UL><P>Gradient search, which is a simple technique to locate local extrema, was also attempted but did not give satisfactory results, as the learning rate required for convergence was slow. Also, noise from images drastically disrupted the calculations. This was a significant problem even when the gradient was calculated by taking multiple pictures around the point of interest. </P><CENTER><P><I><B>Fig. 2:</B> Autofocus algorithm in action: the plot shows the increase of the variance (i.e. the focus score) of the image with each iteration.</I></P></CENTER><DIV style="width:220px;float:right"><CENTER><P style=""><I><B>Fig. 3:</B> Indicative proportion of time spent in each phase of one implementation of an autofocus search</I></P></CENTER></DIV><P>The performance of the final autofocus algorithm varies, depending on the starting point of the search. A typical processing time is around 5s, which allows for autofocus during live-stream imaging through the <A href="//2015.igem.org/Team:Cambridge-JIC/Webshell" class="blue">Webshell</A>.</P><P>The autofocus algorithm we have implemented is open for further improvements and performance enhancement. Ideas for future development include:</P><UL><LI><P>A better hill climbing algorithm for finer focus after reaching the &quot;almost focused stage&quot;</P></LI><LI><P>Automatic  measurement of the actual height to the CCD</P></LI><LI><P>More reliable motors use to improve precision</P></LI></UL><CENTER><P><I>The Autofocus algorithm we have developed can be found in the <A href="https://2015.igem.org/Team:Cambridge-JIC/Downloads" class="blue">Downloads</A> page. The whole Software team contributed to its development.</I></P></CENTER><P style="font-size:80%">References:[1] Firestone, L., Cook, K., Culp, K., Talsania, N. and Preston, K. (1991). Comparison of autofocus methods for automated microscopy. <I>Cytometry</I>, 12(3), pp.195-206[2] Wu, Q., Merchant, F. and Castleman, K. (2008). <I>Microscope image processing</I>. Amsterdam: Elsevier/Academic Press.</P></DIV></DIV></SECTION><SECTION id="footer-sec"><DIV class="container"><DIV class="row  pad-bottom"><DIV class="col-md-3"><H4><STRONG>ABOUT US</STRONG></H4><P>
                    We are a team of Cambridge undergraduates, competing in the Hardware track in iGEM 2015.
                    </P><A href="//2015.igem.org/Team:Cambridge-JIC/Team">read more</A></DIV><DIV class="col-md-3"><H4><STRONG>FOLLOW US ON</STRONG></H4></DIV><DIV class="col-md-3"><H4><STRONG>LOCATION</STRONG></H4><P>
                    Department of Plant Sciences, 
                    University of Cambridge  
                    Downing Street 
                    CB2 3EA
                    </P></DIV><DIV class="col-md-3"><H4><STRONG>CONTACT US</STRONG></H4><P>
                    Email: <A href="mailto:igemcambridge2015@gmail.com">igemcambridge2015@gmail.com</A>
                    Tel: <A href="tel:+447721944314">+447721944314</A></P></DIV></DIV></DIV></SECTION></BODY></HTML>