<HTML lang="en" dir="ltr" class="client-nojs">
<style type="text/css">
A:before { content:' '; } 
A:after { content:' '; } 
SPAN:before { content:' '; } 
SPAN:after { content:' '; } 
</style>
<BODY class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Team_Cambridge-JIC_Stretch_Goals skin-igem action-view"><DIV id="globalWrapper"><DIV id="content" class="mw-body" role="main"><H1 id="firstHeading" class="firstHeading"><SPAN dir="auto">Team:Cambridge-JIC/Stretch Goals</SPAN></H1><DIV id="bodyContent"><DIV id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><DIV class="cam-container"><DIV class="navbar navbar-inverse navbar-fixed-top navbar-pad"><DIV class="container"><DIV class="navbar-header"><A class="navbar-brand" href="//2015.igem.org/Team:Cambridge-JIC">CAMBRIDGE-JIC</A></DIV><DIV class="navbar-collapse collapse"><UL class="nav navbar-nav navbar-right"><LI><A href="//2015.igem.org/Team:Cambridge-JIC">HOME</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Project">PROJECT</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Description">DESCRIPTION</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Design">APPLIED DESIGN</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Make_Your_Own">MAKE YOUR OWN</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Downloads">DOWNLOADS</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Gallery">GALLERY</A></LI></UL></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Hardware">HARDWARE</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Tech_Specs">TECHNICAL SPECIFICATIONS</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Modeling">MODELING</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/3D_Printing">3D PRINTING</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Stretch_Goals">STRETCH GOALS</A></LI></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Software">SOFTWARE</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Webshell">WEBSHELL</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/MicroMaps">MICROMAPS</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Autofocus">AUTOFOCUS</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/ImageJ">IMAGEJ</A></LI></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Practices">COMMUNITY</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Outreach">OUTREACH</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/OpenHardwareRevolution">OPEN HARDWARE LICENSING</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Collaborations">COLLABORATION</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Wiki_Design">WIKI DESIGN</A></LI></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Lab">OUR LAB</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Notebook">NOTEBOOK</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Safety">SAFETY</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Achievements">ACHIEVEMENTS</A></LI></DIV><LI><A href="//2015.igem.org/Team:Cambridge-JIC/People">ABOUT US</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Team">MEET THE TEAM</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Attributions">ATTRIBUTION</A></LI><LI><A href="//2015.igem.org/Team:Cambridge-JIC/Sponsors">SPONSORS</A></LI></DIV></DIV></DIV><SECTION style="background-color: #fff"><DIV class="slide"><DIV style="width: 100%; padding: 0% 10%; margin: 30px 0px;color:#000"><H1>Screening System - A Potential use for OpenScope</H1><H3>The Concept</H3><P>To integrate OpenScope onto a desktop translation system (CNC), such as the <A href="http://www.shapeoko.com/" class="blue">Shapeoko</A>, implementing an automated, high-throughput screening system for Enhancer Trap, Forward Mutagenesis and Reporter screens in <I>Marchantia</I> and potentially yeast. This involves sample scanning, detection and labelling, after macroscopic and microscopic imaging in brightfield and fluorescence.</P><H3>The Approach</H3><P>Using the CNC for coarse positioning (0.1mm accuracy) and OpenScope flexure-type plastic mechanisms for fine positioning (accuracy of the order of 1 μm). The Raspberry Pi camera is used for imaging. Exploit the image recognition software we have developed for labelling, and <A href="//2015.igem.org/Team:Cambridge-JIC/MicroMaps" class="blue">MicroMaps</A> for image stitching to map the sample field. The original (unmodified) Raspberry Pi camera is used for macroscopic imaging, and the OpenScope optics cube (Raspberry Pi camera with inverted lens) - for microscopy.</P><H3>Problems Encountered</H3><UL><LI><P>Poor repeatability of CNC head positioning</P></LI><LI><P>Significant memory required for image storage</P></LI><LI><P>Full screening time of the order of 1 day</P></LI></UL><P>Theoretically, the calculations go as follows:</P><UL><LI><P>One image: 1024x768px, size on disk: approx. 400KB</P></LI><LI><P>Maximal reliable travel speed of Shapeoko head: 900cm/min</P></LI><LI><P>Field of view (Raspberry Pi camera with inverted lens, as in the OpenScope): approx. 50x40μm, hence area = 2000μm<SUP>2</SUP>=2x10<SUP>-5</SUP>cm<SUP>2</SUP></P></LI><LI><P>Standard Petri dish: 90mm diameter, hence area ≈ 64cm<SUP>2</SUP></P></LI><LI><P>Number of images required to cover whole Petri: 64cm<SUP>2</SUP>/2x10<SUP>-5</SUP>cm<SUP>2</SUP>≈3x10<SUP>6</SUP> images, i.e. 10<SUP>9</SUP> KB memory (approx. 1TB)</P></LI><LI><P>Time to scan whole area of 1 Petri: 17hrs at max speed (3x10<SUP>6</SUP> squares with 50μm sides); in practice constant movement with maximal speed is not possible – the head needs to start/stop/focus, so the estimated time is 24hrs or more (but this might still be decreased by a faster moving head on a more reliable CNC)</P></LI></UL><P><I>In short, after doing this math, we almost gave up on the idea. However, after some more maths, we came up with a...</I></P><H3>Solution</H3><P>Preliminary scanning can be carried out with a macroscopic (normal) camera. The image recognition software can then scan for whole colonies or <I>Marchantia</I> plants within the recorded images. This turns out to be much more feasible:</P><UL><LI><P>Camera field of view: 3.5x2.5cm, hence area ≈ 8.5cm<SUP>2</SUP></P></LI><LI><P>Images required to cover whole Petri dish: 64cm<SUP>2</SUP>/8.5cm<SUP>2</SUP>=8 images</P></LI><LI><P>Accounting for overlaps required for stitching at x4 coverage (each vertex appears in 4 neighbouring images), this gives 32 images</P></LI><LI><P>Total memory usage: 32 x 400KB ≈ 13 MB</P></LI></UL><P>This approach significantly reduces the number of images and time required to cover a whole plate. When image recognition software is implemented, it is possible to label areas of interest within the whole sample field. These specific areas can then be imaged by the microscope head attachment independently.</P><H3>Software Architecture</H3><P>We began to develop some python libraries to automatically manage this process, with an architecture as seen in the image to the right. The software in its current state is available within the 'lib' and 'hw' directories in our Github repository, which can be found on our <A href="//2015.igem.org/Team:Cambridge-JIC/Downloads" class="blue">Downloads</A> page (or go <A href="//github.com/sourtin/igem15-sw" class="blue">here</A> to look for future revisions). It is based around a series of abstraction layers with the ultimate goal of hiding any underlying hardware and allowing easy automation of experiments. Our example use case is an automated screening system as described above, whereby a macroscopic camera images a large sample set, software is then used to identify individual samples which are independently imaged by a microscopic camera, finally these images are screened (using a range of image processing algorithms) to identify different phenotypes.</P><UL><LI><P>We start with the hardware itself. An xyz translation system, such as a Shapeoko or other CNC machine, is fitted with at least one 'head'. The most important head for our use is our OpenScope microscope. In our example we would also fit the Shapeoko with a macroscopic camera (or an overhead camera with a sufficiently large field of view). Further, we intend to have a marker. 
</P></LI><LI><P>The next layer up is the driver software which interacts directly with this hardware.</P></LI><LI><P>The Shapeoko itself is controlled by an Arduino Mega with a G-code interpreter installed which we can interface with to send complex movement commands (such as precision arcs) and to recalibrate at any time. Our driver can handle unexpected events such as cable disconnection with ease, recalibrating afterwards in case of unaccounted-for interruptions.</P></LI><LI><P>The OpenScope software can be directly leveraged for the camera heads.</P></LI><LI><P>In our calibration tests we attached a pen to the Shapeoko head. This was much simpler as we could control the z axis to apply the pen at will, without any additional Arduinos to interface with.</P></LI><LI><P>We also have some virtual hardware in our examples and tests to demonstrate the abilities of the software, such as a camera which can navigate a gigapixel image of the Andromeda galaxy</P></LI></UL><P>These drivers use a common software class to unite their differences under a common language. In this way we can abstract away from all the intricacies of each hardware head and control the hardware with ease. There are a few key classes we use here. The Head class (and Camera subclass) are used to refer to the hardware heads. The xyz stage is interacted with via the Stage class which allows for calibration, xyz movement, and head switching. Finally we unite everything under the same roof with the Workspace class. This class manages all the software commands we might send, utilising a smart queue to optimise the order in which actions are taken to reduce movement and time (and respecting the order of commands when necessary, e.g. if you’re transferring liquids with a pipette!). The workspace is the middleman through which user experiments communicate to the hardware. The last class of interest is the Canvas class. This class is responsible for collecting, and managing the stitching together of, the images that constitute an individual sample, as well as updating it periodically as necessary in case a time-lapse image is desired.</P><P>Finally comes the interesting part, your experiment! Though we don't yet have examples of this, thanks to the library abstractions provided in the lower layers, writing these experiments should be unprecedentedly easy. Python is one of the easiest languages to learn for those who are new to programming, and so should make even very complex experiments easy. See our <A href="//2015.igem.org/Team:Cambridge-JIC/MicroMaps" class="blue">MicroMaps</A> page for more information on the image processing algorithms that we have been developing to characterise specimens. We have also been developing an extensive and powerful Graphical User Interface (GUI) to manage and observe your experiment, checkout our WebShell and MicroMaps interfaces under our <A href="//2015.igem.org/Team:Cambridge-JIC/Software" class="blue">Software</A> section!</P><CENTER><P><I>The automated experimental abstraction layers were developed by Will, with useful feedback and advice from the rest of the Software team.</I></P></CENTER><H3>Proof of Concept (Experiment)</H3><P>Macroscopic imaging of a Petri dish containing mature <I>Marchantia</I> samples was carried out. Images were captured manually through the Webshell during 1-2 minutes of operation. A Raspberry Pi camera was fixed to the CNC (in our case a Shapeoko v1, 150GBP second-hand) with a 3D-printed static mount. The motorised z-axis driven by the python program allowed for focusing of the sample. Seven images were successfully stitched, in a remarkable stitching time of 1.5sec. It is notable that the stitching algorithm copes well even with frames which are rotated at a small angle relative to each other.
</P><P><CENTER><I>Desktop translation system (CNC) Shapeoko v.1, the setup used for imaging and resultant stitched image</I></CENTER></P><H3>Future opportunities</H3><P>Incorporate of OpenScope onto the CNC head.</P><P>Develop colony screening for yeast/bacterial colonies on Petri dishes. Criteria such as distinct colour or fluorescence can be incorporated into the image recognition software. However, the fluorescence imaging achieved on OpenScope is not currently reliable enough to perform fluorescence screening.</P><P>Implement focus-stacking software for 3D samples (like <I>Marchantia</I>). This facilitates the development of programs which track the growth of samples.</P></DIV></DIV></SECTION><SECTION id="footer-sec"><DIV class="container"><DIV class="row  pad-bottom"><DIV class="col-md-3"><H4><STRONG>ABOUT US</STRONG></H4><P>
                    We are a team of Cambridge undergraduates, competing in the Hardware track in iGEM 2015.
                    </P><A href="//2015.igem.org/Team:Cambridge-JIC/Team">read more</A></DIV><DIV class="col-md-3"><H4><STRONG>FOLLOW US ON</STRONG></H4></DIV><DIV class="col-md-3"><H4><STRONG>LOCATION</STRONG></H4><P>
                    Department of Plant Sciences, 
                    University of Cambridge  
                    Downing Street 
                    CB2 3EA
                    </P></DIV><DIV class="col-md-3"><H4><STRONG>CONTACT US</STRONG></H4><P>
                    Email: <A href="mailto:igemcambridge2015@gmail.com">igemcambridge2015@gmail.com</A>
                    Tel: <A href="tel:+447721944314">+447721944314</A></P></DIV></DIV></DIV></SECTION></BODY></HTML>