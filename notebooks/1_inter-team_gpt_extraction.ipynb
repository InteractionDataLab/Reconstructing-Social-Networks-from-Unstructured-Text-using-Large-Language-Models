{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb947d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the necessary libraries\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import tiktoken\n",
    "import cleantext\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import ast\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#from ipynb.fs.full.gpt_curation_custom_functions import send_batch_request\n",
    "#from ipynb.fs.full.gpt_curation_custom_functions import string_process_no_stp_words, process_team_names\n",
    "#from ipynb.fs.full.gpt_curation_custom_functions import fuzzy_match, fuzzy_search\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#Load the api key from being an environment variable\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "stats = pd.DataFrame()\n",
    "\n",
    "stp_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3cceb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc18b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Version is custom - based on my prompt styles. Specific is one-shot with particular explanation on the task\n",
    "version = \"two-step\"\n",
    "\n",
    "#The text file to the prompt\n",
    "f = open(\"prompts/prompt_inter-team_version_specific_lol\", \"r\")\n",
    "prompt = f.read().rstrip()\n",
    "\n",
    "meta = pd.read_csv(\"../data/raw/team_meta.csv\")\n",
    "\n",
    "#batches = os.listdir(\"../data/processed/text_batches_inter-team_filtered/\")\n",
    "#batches = [batch.replace(\".csv\", \"\") for batch in batches]\n",
    "#batches = sorted(batches)\n",
    "\n",
    "#The first 2 collaboration batches are the ones manually curated\n",
    "batches = [\"1\", \"2\"]\n",
    "\n",
    "#model = \"gpt-4-turbo\"\n",
    "#model = \"gpt-4-0125-preview\"\n",
    "model = \"gpt-3.5-turbo-16k\"\n",
    "\n",
    "#The level of creativity in the gpt response - for the extraction task set at 0.3\n",
    "temperature = 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed3fd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request_chat_competion(arguments, model = \"gpt-3.5-turbo-16k\", temperature = 0.7, output_format = \"lol\", columns = ['team', 'context', 'target']):\n",
    "    \n",
    "    message = arguments[0]\n",
    "    variables = arguments[1]\n",
    "    chunk = arguments[2]\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(model = model, messages = message, temperature = temperature, request_timeout = 60)\n",
    "        final_response = response.choices[0].message[\"content\"].strip()\n",
    "    \n",
    "    except:\n",
    "        return(pd.DataFrame(), 0, chunk)\n",
    "    \n",
    "    var = final_response\n",
    "    df = pd.DataFrame([[var]], columns=[\"text\"]) \n",
    "                \n",
    "    for key, value  in variables.items():\n",
    "        df[key] = value              \n",
    "\n",
    "    return(df, 1, chunk)\n",
    "\n",
    "\n",
    "def process_request_regex(text, output_format = \"lol\", columns = [\"team\", \"context\", \"target\"]):\n",
    "    \n",
    "    if (output_format == \"lol\"):\n",
    "        \n",
    "        var = text\n",
    "            \n",
    "        try:\n",
    "\n",
    "            var = text\n",
    "            \n",
    "            var = re.findall(r'\\[[\"].*?[\"],[ ]?[\"].*?[\"],[ ]?[\"].*?[\"]\\]', var)\n",
    "            var = [ast.literal_eval(x) for x in var]\n",
    "\n",
    "            var = [x if len(x) == 3 else None for x in var]\n",
    "            var = [x for x in var if x is not None]\n",
    "\n",
    "            if len(var) == 0:\n",
    "                return(pd.DataFrame(), 2)\n",
    "\n",
    "            processed_df = pd.DataFrame(var, columns = columns)   \n",
    "                \n",
    "        except:\n",
    "                \n",
    "            return(pd.DataFrame(), 0)\n",
    "            \n",
    "    return(processed_df, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9108bb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started batch 1\n",
      "Completed batch 1 in 33.790950536727905 seconds\n",
      "Started batch 2\n",
      "Completed batch 2 in 7.660872220993042 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for batch_no in batches:\n",
    "    \n",
    "    if (not os.path.isfile(\"../data/processed/gpt_curated/inter-team/\" + batch_no + \"_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\")):\n",
    "\n",
    "        print(\"Started batch \" + batch_no)\n",
    "        start = time.time()\n",
    "\n",
    "        batch = pd.read_csv(\"../data/processed/text_batches_inter-team_collab/\" + batch_no + \".csv\")\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        output_format_mistakes = []\n",
    "        gpt_mistakes = []\n",
    "\n",
    "        #Loop over all chunks of batches\n",
    "        all_messages = []\n",
    "        val = []\n",
    "        \n",
    "        for i in range(0,len(batch)):\n",
    "            \n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            messages.append({\"role\": \"user\", \"content\": \"\\nWe are team \" + batch['team'].iloc[i] + \"\\n.\" + \"The following text describes our activities in a scientific competition called iGEM and could also describe our relationships with other iGEM teams:\"})\n",
    "            messages.append({\"role\": \"user\", \"content\": batch['text'].iloc[i]})\n",
    "            all_messages.extend([[messages, {\"source_team\": batch['team'].iloc[i], \"year\": batch['year'].iloc[i]}, i]])\n",
    "               \n",
    "        \n",
    "        #for message in all_messages:\n",
    "        #    \n",
    "        #    output, fail, chunk = send_request_chat_competion(message)\n",
    "        #    output['chunk'] = chunk\n",
    "        #    df = df.append(output)\n",
    "        #    \n",
    "        #    if fail == 0:\n",
    "        #        gpt_mistakes.extend([chunk])\n",
    "                    \n",
    "        with Pool(processes = 8) as pool:\n",
    "    \n",
    "            for output, fail, chunk in pool.map(send_request_chat_competion, all_messages):\n",
    "                \n",
    "                output['chunk'] = chunk\n",
    "                df = df.append(output)\n",
    "\n",
    "                if fail == 0:\n",
    "                    gpt_mistakes.extend([chunk])\n",
    "                \n",
    "                #time.sleep(0.01)\n",
    "            \n",
    "        end = time.time()\n",
    "        time_elapsed = (end-start)\n",
    "\n",
    "        stats = stats.append(pd.DataFrame([[\"extraction\", batch_no, time_elapsed, str(gpt_mistakes)]], columns=[\"type\", \"batch_no\", \"time_secs\", \"failed_chunks\"]))\n",
    "\n",
    "        print(\"Completed batch \" + batch_no + \" in \" + str(time_elapsed) + \" seconds\")\n",
    "\n",
    "        \n",
    "        df.to_csv(\"../data/processed/gpt_curated/inter-team/\" + batch_no + \"_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\", index=False)\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0039d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 1\n",
      "Completed batch 2\n"
     ]
    }
   ],
   "source": [
    "stats_processing = pd.DataFrame()\n",
    "\n",
    "for batch_no in batches:\n",
    "    \n",
    "    df_interactions = pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_csv(\"../data/processed/gpt_curated/inter-team/\" + batch_no + \"_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\")\n",
    "    \n",
    "    for loop_var in range(len(df)):\n",
    "    \n",
    "        t, flag = process_request_regex(str(df['text'][loop_var]))\n",
    "        \n",
    "        if (len(t) > 0):\n",
    "        \n",
    "            t['source_team'] = df['source_team'][loop_var]\n",
    "            t['year'] = df['year'][loop_var]\n",
    "            t['batch_id'] = batch_no\n",
    "            t['chunk_id'] = df['chunk'][loop_var]\n",
    "            \n",
    "            df_interactions = df_interactions.append(t)                \n",
    "        \n",
    "        stats_processing = stats_processing.append(pd.DataFrame([[batch_no, loop_var, flag]], columns=[\"batch_no\", \"chunk_no\", \"status\"]))\n",
    "    \n",
    "    df_interactions.to_csv(\"../data/processed/gpt_curated/inter-team/processed_\" + batch_no + \"_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\", index=False)\n",
    "    print(\"Completed batch \" + batch_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eef6696",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.to_csv(\"../data/processed/gpt_curated/inter-team/stats_extraction_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\")\n",
    "stats_processing.to_csv(\"../data/processed/gpt_curated/inter-team/stats_processing_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae7e0722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>batch_no</th>\n",
       "      <th>time_secs</th>\n",
       "      <th>failed_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extraction</td>\n",
       "      <td>1</td>\n",
       "      <td>33.790951</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extraction</td>\n",
       "      <td>2</td>\n",
       "      <td>7.660872</td>\n",
       "      <td>[3, 7, 10, 11, 14, 15, 19, 21, 22, 26, 29, 30,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         type batch_no  time_secs  \\\n",
       "0  extraction        1  33.790951   \n",
       "0  extraction        2   7.660872   \n",
       "\n",
       "                                       failed_chunks  \n",
       "0                                                 []  \n",
       "0  [3, 7, 10, 11, 14, 15, 19, 21, 22, 26, 29, 30,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "caa6e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions = pd.DataFrame()\n",
    "\n",
    "batches = [\"1\", \"2\"]\n",
    "\n",
    "for batch_no in batches:\n",
    "    \n",
    "    try:\n",
    "        t = pd.read_csv(\"../data/processed/gpt_curated/inter-team/processed_\" + batch_no + \"_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\")\n",
    "        df_interactions = df_interactions.append(t)\n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6dc2d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all batches into one\n",
    "df_interactions.to_csv(\"../data/processed/gpt_curated/inter-team/processed_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec1933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
