---
title: "R Notebook"
output: html_notebook
---

Performance Evaluation of GPT v Fuzzy v Manual

a. Edge level
b. Category (context) level

```{r}

library(ggplot2)
library(dplyr)
library(wesanderson)

df_stats = data.frame()
df_stats_context = data.frame()

jackknife_se = function(estimates, actual)
{
  se = sqrt(sum((estimates-actual)**2)*((length(estimates)-1)/length(estimates)))
  return(se)
}

```


```{r}

#Matching and Curation with 3.5-turbo-16k

load("../data/processed/gpt_curated/inter-team/comparison_manual_gpt_fuzzy_two-step_10_gpt-3.5-turbo-16k_0-3_0-3.RData")

precision = sum(df_merged$in_manual == TRUE & df_merged$in_gpt == TRUE, na.rm = TRUE)/sum(df_merged$in_gpt == TRUE, na.rm = TRUE)

recall = sum(df_merged$in_manual == TRUE & df_merged$in_gpt == TRUE, na.rm = TRUE)/sum(df_merged$in_manual == TRUE, na.rm = TRUE)

df_stats = rbind(df_stats, data.frame(model = "gpt-3.5-turbo-16k", metric = "precision", value = precision, se = jackknife_se(df_stats_jackknife$precision[df_stats_jackknife$type == "gpt"], precision)))

df_stats = rbind(df_stats, data.frame(model = "gpt-3.5-turbo-16k", metric = "recall", value = recall, se = jackknife_se(df_stats_jackknife$recall[df_stats_jackknife$type == "gpt"], recall)))

#Contexts

temp = df_merged_context[,c("context_gpt", "context_manual")]
temp$model = "gpt-3.5-turbo-16k"

df_stats_context = rbind(df_stats_context, temp)

```

```{r}

#Matching and Curation with Fuzzy

precision = sum(df_merged$in_manual == TRUE & df_merged$in_fuzzy == TRUE, na.rm = TRUE)/sum(df_merged$in_fuzzy == TRUE, na.rm = TRUE)

recall = sum(df_merged$in_manual == TRUE & df_merged$in_fuzzy == TRUE, na.rm = TRUE)/sum(df_merged$in_manual == TRUE, na.rm = TRUE)

df_stats = rbind(df_stats, data.frame(model = "fuzzy-matching", metric = "precision", value = precision, se = jackknife_se(df_stats_jackknife$precision[df_stats_jackknife$type == "fuzzy"], precision)))

df_stats = rbind(df_stats, data.frame(model = "fuzzy-matching", metric = "recall", value = recall, se = jackknife_se(df_stats_jackknife$recall[df_stats_jackknife$type == "fuzzy"], recall)))

```

```{r}

#Matching and Curation with 3.5-turbo-instruct

load("../data/processed/gpt_curated/inter-team/comparison_manual_gpt_fuzzy_two-step_10_gpt-3.5-turbo-instruct_0-3_0-3.RData")

precision = sum(df_merged$in_manual == TRUE & df_merged$in_gpt == TRUE, na.rm = TRUE)/sum(df_merged$in_gpt == TRUE, na.rm = TRUE)

recall = sum(df_merged$in_manual == TRUE & df_merged$in_gpt == TRUE, na.rm = TRUE)/sum(df_merged$in_manual == TRUE, na.rm = TRUE)

df_stats = rbind(df_stats, data.frame(model = "gpt-3.5-turbo-instruct", metric = "precision", value = precision, se = jackknife_se(df_stats_jackknife$precision[df_stats_jackknife$type == "gpt"], precision)))

df_stats = rbind(df_stats, data.frame(model = "gpt-3.5-turbo-instruct", metric = "recall", value = recall, se = jackknife_se(df_stats_jackknife$recall[df_stats_jackknife$type == "gpt"], recall)))

temp = df_merged_context[,c("context_gpt", "context_manual")]
temp$model = "gpt-3.5-turbo-instruct"

df_stats_context = rbind(df_stats_context, temp)

```

```{r}

#Matching and Curation with 4-turbo

load("../data/processed/gpt_curated/inter-team/comparison_manual_gpt_fuzzy_two-step_10_gpt-4-turbo_0-3_0-3.RData")

precision = sum(df_merged$in_manual == TRUE & df_merged$in_gpt == TRUE, na.rm = TRUE)/sum(df_merged$in_gpt == TRUE, na.rm = TRUE)

recall = sum(df_merged$in_manual == TRUE & df_merged$in_gpt == TRUE, na.rm = TRUE)/sum(df_merged$in_manual == TRUE, na.rm = TRUE)

df_stats = rbind(df_stats, data.frame(model = "gpt-4-turbo", metric = "precision", value = precision, se = jackknife_se(df_stats_jackknife$precision[df_stats_jackknife$type == "gpt"], precision)))

df_stats = rbind(df_stats, data.frame(model = "gpt-4-turbo", metric = "recall", value = recall, se = jackknife_se(df_stats_jackknife$recall[df_stats_jackknife$type == "gpt"], recall)))

temp = df_merged_context[,c("context_gpt", "context_manual")]
temp$model = "gpt-4-turbo"

df_stats_context = rbind(df_stats_context, temp)

```

```{r}

#Matching and Curation with gpt 4

load("../data/processed/gpt_curated/inter-team/comparison_manual_gpt_fuzzy_two-step_10_gpt-4_0-3_0-3.RData")

precision = sum(df_merged$in_manual == TRUE & df_merged$in_gpt == TRUE, na.rm = TRUE)/sum(df_merged$in_gpt == TRUE, na.rm = TRUE)

recall = sum(df_merged$in_manual == TRUE & df_merged$in_gpt == TRUE, na.rm = TRUE)/sum(df_merged$in_manual == TRUE, na.rm = TRUE)

df_stats = rbind(df_stats, data.frame(model = "gpt-4", metric = "precision", value = precision, se = jackknife_se(df_stats_jackknife$precision[df_stats_jackknife$type == "gpt"], precision)))

df_stats = rbind(df_stats, data.frame(model = "gpt-4", metric = "recall", value = recall, se = jackknife_se(df_stats_jackknife$recall[df_stats_jackknife$type == "gpt"], recall)))

temp = df_merged_context[,c("context_gpt", "context_manual")]
temp$model = "gpt-4"

df_stats_context = rbind(df_stats_context, temp)

```

```{r}

#Matching and Curation with 4 preview

load("../data/processed/gpt_curated/inter-team/comparison_manual_gpt_fuzzy_two-step_10_gpt-4-0125-preview_0-3_0-3.RData")

precision = sum(df_merged$in_manual == TRUE & df_merged$in_gpt == TRUE, na.rm = TRUE)/sum(df_merged$in_gpt == TRUE, na.rm = TRUE)

recall = sum(df_merged$in_manual == TRUE & df_merged$in_gpt == TRUE, na.rm = TRUE)/sum(df_merged$in_manual == TRUE, na.rm = TRUE)

df_stats = rbind(df_stats, data.frame(model = "gpt-4-0125-preview", metric = "precision", value = precision, se = jackknife_se(df_stats_jackknife$precision[df_stats_jackknife$type == "gpt"], precision)))

df_stats = rbind(df_stats, data.frame(model = "gpt-4-0125-preview", metric = "recall", value = recall, se = jackknife_se(df_stats_jackknife$recall[df_stats_jackknife$type == "gpt"], recall)))

temp = df_merged_context[,c("context_gpt", "context_manual")]
temp$model = "gpt-4-0125-preview"

df_stats_context = rbind(df_stats_context, temp)

```

```{r}

load("../data/processed/gpt_curated/inter-team/comparison_manual_gpt_fuzzy_old_0-3_0-3.RData")

precision = sum(df_merged$in_manual == TRUE & df_merged$in_gpt == TRUE, na.rm = TRUE)/sum(df_merged$in_gpt == TRUE, na.rm = TRUE)

recall = sum(df_merged$in_manual == TRUE & df_merged$in_gpt == TRUE, na.rm = TRUE)/sum(df_merged$in_manual == TRUE, na.rm = TRUE)

df_stats = rbind(df_stats, data.frame(model = "turbo-instruct and turbo-16k", metric = "precision", value = precision, se = jackknife_se(df_stats_jackknife$precision[df_stats_jackknife$type == "gpt"], precision)))

df_stats = rbind(df_stats, data.frame(model = "turbo-instruct and turbo-16k", metric = "recall", value = recall, se = jackknife_se(df_stats_jackknife$recall[df_stats_jackknife$type == "gpt"], recall)))

temp = df_merged_context[,c("context_gpt", "context_manual")]
temp$model = "turbo-instruct and turbo-16k"

df_stats_context = rbind(df_stats_context, temp)

```


Make the Plots - Edge Level for Precision/Recall

```{r}

#color_palette = wes_palette("Royal2")

vir = viridis::viridis(6)

color_palette = c("grey", vir[1:6])

 
#Library to check the visibility of palettes under various colorblindness conditions 
#colorBlindness::displayAllColors(viridis::viridis(6))

df_stats$model = factor(df_stats$model, levels = c("fuzzy-matching", "gpt-3.5-turbo-instruct", "turbo-instruct and turbo-16k", "gpt-3.5-turbo-16k", "gpt-4-0125-preview", "gpt-4-turbo", "gpt-4"))

plt = ggplot(df_stats) + 
    geom_bar(aes(fill = model, y=value, x=metric), position="dodge", stat="identity") +
    scale_fill_manual(breaks =  c("fuzzy-matching", "gpt-3.5-turbo-instruct", "turbo-instruct and turbo-16k", "gpt-3.5-turbo-16k", "gpt-4-0125-preview",  "gpt-4-turbo", "gpt-4"), values = (color_palette)) +
    geom_errorbar(aes(fill = model, ymin = value-se, ymax = value+se, x = metric), width = 0.1, position = position_dodge(0.9)) +
    ggtitle("") +
    theme_bw(base_size = 25) + coord_cartesian(ylim = c(0.0, 1)) + 
    xlab("") + ylab("") + theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) + theme(legend.position="none")

ggsave(plt, filename = "../figures/precision_recall_gpt_models_without_legend.png", width = 5, height = 5)

plt = ggplot(df_stats) + 
    geom_bar(aes(fill = model, y=value, x=metric), position="dodge", stat="identity") +
    scale_fill_manual(breaks =  c("fuzzy-matching", "gpt-3.5-turbo-instruct", "turbo-instruct and turbo-16k", "gpt-3.5-turbo-16k", "gpt-4-0125-preview",  "gpt-4-turbo", "gpt-4"), values = (color_palette)) +
    geom_errorbar(aes(fill = model, ymin = value-se, ymax = value+se, x = metric), width = 0.1, position = position_dodge(0.9)) +
    ggtitle("") +
    theme_bw(base_size = 25) + coord_cartesian(ylim = c(0.0, 1)) + 
    xlab("") + ylab("") + theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) 

ggsave(plt, filename = "../figures/precision_recall_gpt_models.png", width = 7, height = 5)

```

Confusion Matrix of Categories

```{r}

temp = df_stats_context %>% group_by(context_gpt, context_manual, model) %>% summarise(count = n())
temp = temp[!is.na(temp$context_gpt) & !is.na(temp$context_manual) & temp$context_gpt %in% c("work", "advice", "material transfer", "meetup", "other"),]

temp$context_gpt = factor(temp$context_gpt, levels = rev(c("work", "advice", "material transfer", "meetup", "other")))
temp$context_manual = factor(temp$context_manual, levels = (c("work", "advice", "material transfer", "meetup", "other")))

temp = temp %>% group_by(context_gpt, model) %>% mutate(proportion = count/sum(count), zscore = (count - mean(count))/sd(count))

#pdf("../figures/categories_matching_confusion_matrix.pdf")
#dev.off()

temp$model = factor(temp$model, levels = c("gpt-3.5-turbo-instruct", "turbo-instruct and turbo-16k", "gpt-3.5-turbo-16k", "gpt-4-0125-preview",  "gpt-4-turbo", "gpt-4"))


plt = ggplot(temp, aes(y = context_gpt, x = context_manual, fill = zscore)) + geom_tile(lwd = 1.5, linetype = 1) + theme_bw(base_size = 20) + 
  #scale_x_discrete(position = "top") +
  geom_text(aes(x = context_manual, y = context_gpt, label = count), color = "white") + 
  ggtitle("") + 
  scale_fill_gradient(low = "yellow", high = "blue", na.value = "white") +
  #scale_fill_gradientn(colors = hp(n = 7, option = "Always"), na.value = "white") +
  #scale_fill_gradient(low = "yellow", high = "deepblue", na.value = "white") +
  facet_wrap("model", nrow = 1) + 
  theme_bw(base_size = 17) + xlab("") + ylab("") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+ theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) 

ggsave(plt, filename = "../figures/category_matching_confusion_matrix.png", width = 20, height = 5)


```

Category Matching only for one of the models (for main figure)

With Dendogram

```{r}

temp_resh <- reshape2::acast(temp[temp$model == "gpt-3.5-turbo-16k",], formula = context_gpt~context_manual, value.var = "zscore")

temp_resh[is.na(temp_resh)] = 0

dist_mat <- dist(temp_resh, method = 'euclidean')
hc = hclust(dist_mat)
ord = hc$labels[hc$order]

temp$context_gpt = factor(temp$context_gpt, levels = rev(ord))
temp$context_manual = factor(temp$context_manual, levels = ord)

plt = ggplot(temp[temp$model == "gpt-3.5-turbo-16k",], aes(y = context_gpt, x = context_manual, fill = zscore)) + geom_tile(lwd = 1.5, linetype = 1) + theme_bw(base_size = 25) + 
  #scale_x_discrete(position = "top") +
  geom_text(aes(x = context_manual, y = context_gpt, label = count), color = "white") + 
  ggtitle("") + 
  scale_fill_gradient(low = "yellow", high = "blue", na.value = "white") +
  theme_bw(base_size = 20) + xlab("") + ylab("") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) 

ggsave(plt, filename = "../figures/category_matching_confusion_matrix_main_ordered.png", width = 6, height = 5)



```

```{r}

df_stats_context

```

