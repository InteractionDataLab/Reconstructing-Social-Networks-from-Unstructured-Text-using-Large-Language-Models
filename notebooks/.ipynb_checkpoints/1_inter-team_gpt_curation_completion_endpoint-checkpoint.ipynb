{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b844ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the necessary libraries\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import tiktoken\n",
    "import cleantext\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import ast\n",
    "from fuzzywuzzy import fuzz\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#Load the api key from being an environment variable\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "stats = pd.DataFrame()\n",
    "\n",
    "stp_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e59e08a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc18b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Version is custom - based on my prompt styles. Specific is one-shot with particular explanation on the task\n",
    "version = \"two-step\"\n",
    "\n",
    "#Setting the output format as a list of list\n",
    "output_format = \"lol\"\n",
    "\n",
    "\n",
    "#The text file to the prompt\n",
    "f = open(\"prompts/prompt_inter-team_version_specific_lol\" , \"r\")\n",
    "prompt = f.read().rstrip()\n",
    "\n",
    "meta = pd.read_csv(\"../data/raw/team_meta.csv\")\n",
    "\n",
    "#batches = os.listdir(\"../data/processed/batches_inter-team_collab/\") #all_batches_inter-team_fuzzy_select_freq/\")\n",
    "#batches = [batch.replace(\".csv\", \"\") for batch in batches]\n",
    "#batches = sorted(batches)\n",
    "batches = [\"1\", \"2\"]\n",
    "\n",
    "model = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "#The level of creativity in the gpt response - for the extraction task set at 0.3\n",
    "temperature = 0.3\n",
    "\n",
    "#The number of parallel chunks to evaluate at the same time: gpt-instruct limit is 20\n",
    "jump = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3fd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_batch_request(arguments, model = \"gpt-3.5-turbo-instruct\", temperature = 0.3, output_format = \"lol\", columns = [\"team\", \"context\", \"target\"]):\n",
    "    \n",
    "    \n",
    "    chunks = arguments[0]\n",
    "    variables = arguments[1]\n",
    "    ids = arguments[2]\n",
    "    \n",
    "    #print(ids + \" here\")\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    flag = []\n",
    "\n",
    "    try:\n",
    "    \n",
    "        response = openai.Completion.create(                                \n",
    "                    prompt = chunks ,\n",
    "                    engine = model,\n",
    "                    temperature = temperature,\n",
    "                    max_tokens = 2000,  \n",
    "                    top_p = 1,\n",
    "                    frequency_penalty = 0,\n",
    "                    presence_penalty = 0,\n",
    "                    timeout = 200\n",
    "                    )\n",
    "        \n",
    "    except:\n",
    "        #print(ids)\n",
    "        return(df, 0)\n",
    "    \n",
    "    for j in range(0, len(chunks)):\n",
    "        \n",
    "        var = response.choices[j].text.strip()\n",
    "        \n",
    "        t = pd.DataFrame([[var]], columns=[\"text\"]) \n",
    "                \n",
    "        for key, value  in variables.items():\n",
    "            t[key] = value[j]\n",
    "                    \n",
    "        df = df.append(t)\n",
    "    \n",
    "    return(df, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55bea12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_request_regex(text, output_format = \"lol\", columns = [\"team\", \"context\", \"target\"]):\n",
    "    \n",
    "    if (output_format == \"lol\"):\n",
    "        \n",
    "        var = text\n",
    "\n",
    "            \n",
    "        try:\n",
    "\n",
    "            var = text\n",
    "\n",
    "            #var = re.findall(r'\\[[\"].*?[\"], [\"].*?[\"], [\"].*?[\"]\\]', var)\n",
    "            var = re.findall(r'\\[[\"].*?[\"],[ ]?[\"].*?[\"],[ ]?[\"].*?[\"]\\]', var)\n",
    "            var = [ast.literal_eval(x) for x in var]\n",
    "\n",
    "            var = [x if len(x) == 3 else None for x in var]\n",
    "            var = [x for x in var if x is not None]\n",
    "\n",
    "            if len(var) == 0:\n",
    "                return(pd.DataFrame(), 2)\n",
    "\n",
    "            processed_df = pd.DataFrame(var, columns = columns)   \n",
    "                \n",
    "        except:\n",
    "                \n",
    "            return(pd.DataFrame(), 0)\n",
    "            \n",
    "    return(processed_df, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7786ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0507b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started batch 2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Completed batch 2 in 75.57405734062195 seconds\n"
     ]
    }
   ],
   "source": [
    "#Token limit = 1,000,000 per minute (Tier 4 account)\n",
    "#Each chunk = 500 tokens (max)\n",
    "#Chunk + Prompt ~ 1000 tokens\n",
    "#Expected output token size ~ 1000 tokens (max)\n",
    "\n",
    "#Each request = 2000 tokens. Max requests per minute = 200 \n",
    "#Number of requests allowed by gpt-instruct in parallel = 20 (20*2000 = 40000 tokens)\n",
    "#Using multiprocessing - starting with 8 processes (GPT limit not breached)\n",
    "\n",
    "\n",
    "for batch_no in batches:\n",
    "    \n",
    "    if (not os.path.isfile(\"../data/processed/gpt_curated/inter-team/\" + batch_no + \"_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\")):\n",
    "\n",
    "        print(\"Started batch \" + batch_no)\n",
    "        start = time.time()\n",
    "\n",
    "        batch = pd.read_csv(\"../data/processed/text_batches_inter-team_collab/\" + batch_no + \".csv\")\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        all_messages = []\n",
    "\n",
    "        batch['processed_text'] =  prompt + \"We are team \" + batch['team'] + \"\\n.\" + \"The following text describes our activities in a scientific competition called iGEM including our relationships with other teams:\" +  batch['text']\n",
    "        \n",
    "        for i in range(0,len(batch), jump):\n",
    "            \n",
    "            temp = batch['processed_text'][i:min(i+jump,len(batch))]\n",
    "            temp = list(temp)\n",
    "            all_messages.extend([[temp, {\"source_team\": list(batch['team'][i:min(i+jump,len(batch))]), \"year\": list(batch['year'][i:min(i+jump,len(batch))])}, i]])\n",
    "         \n",
    "        for message in all_messages:\n",
    "            \n",
    "            output, miss = send_batch_request(message)\n",
    "            print(miss)\n",
    "            df = df.append(output)\n",
    "            \n",
    "        #After support for the old endpoint ended early 2024, parallel processing seems to fail much more than before.\n",
    "        \n",
    "        \"\"\"with Pool(processes = 8) as pool:\n",
    "            \n",
    "            zipped_output = pool.map(send_batch_request, all_messages)\n",
    "                \n",
    "            for output, miss in list(zipped_output):    \n",
    "                df = df.append(output)\n",
    "                print(miss)\n",
    "                \n",
    "            time.sleep(0.01)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        end = time.time()\n",
    "        time_elapsed = (end-start)\n",
    "\n",
    "        stats = stats.append(pd.DataFrame([[batch_no, time_elapsed]], columns=[\"batch_no\", \"time_secs\"]))\n",
    "\n",
    "        print(\"Completed batch \" + batch_no + \" in \" + str(time_elapsed) + \" seconds\")\n",
    "\n",
    "        df.to_csv(\"../data/processed/gpt_curated/inter-team/\" + batch_no + \"_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\", index=False)\n",
    "        time.sleep(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be003aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "031fdaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 1\n",
      "Completed batch 2\n"
     ]
    }
   ],
   "source": [
    "stats_processing = pd.DataFrame()\n",
    "\n",
    "for batch_no in batches:\n",
    "    \n",
    "    df_interactions = pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_csv(\"../data/processed/gpt_curated/inter-team/\" + batch_no + \"_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\")\n",
    "    \n",
    "    for loop_var in range(len(df)):\n",
    "    \n",
    "        t, flag = process_request_regex(str(df['text'][loop_var]))\n",
    "        \n",
    "        if (len(t) > 0):\n",
    "        \n",
    "            t['source_team'] = df['source_team'][loop_var]\n",
    "            t['year'] = df['year'][loop_var]\n",
    "            t['batch_id'] = batch_no\n",
    "            t['chunk_id'] = loop_var #df['chunk'][loop_var]\n",
    "            \n",
    "            df_interactions = df_interactions.append(t)                \n",
    "        \n",
    "        stats_processing = stats_processing.append(pd.DataFrame([[batch_no, loop_var, flag]], columns=[\"batch_no\", \"chunk_no\", \"status\"]))\n",
    "    \n",
    "    df_interactions.to_csv(\"../data/processed/gpt_curated/inter-team/processed_\" + batch_no + \"_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\", index=False)\n",
    "    print(\"Completed batch \" + batch_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b91745c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.to_csv(\"../data/processed/gpt_curated/inter-team/stats_extraction_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\")\n",
    "stats_processing.to_csv(\"../data/processed/gpt_curated/inter-team/stats_processing_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b488b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions = pd.DataFrame()\n",
    "\n",
    "for batch_no in batches:\n",
    "    \n",
    "    t = pd.read_csv(\"../data/processed/gpt_curated/inter-team/processed_\" + batch_no + \"_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\")\n",
    "    \n",
    "    df_interactions = df_interactions.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43dc064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions.to_csv(\"../data/processed/gpt_curated/inter-team/processed_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae93e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
