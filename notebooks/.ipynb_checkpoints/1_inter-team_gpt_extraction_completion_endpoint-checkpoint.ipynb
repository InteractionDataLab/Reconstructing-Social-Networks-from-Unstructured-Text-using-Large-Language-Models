{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b844ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the necessary libraries\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import tiktoken\n",
    "import cleantext\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import ast\n",
    "from fuzzywuzzy import fuzz\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#Load the api key from being an environment variable\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "stats = pd.DataFrame()\n",
    "\n",
    "stp_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e59e08a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc18b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Version is custom - based on my prompt styles. Specific is one-shot with particular explanation on the task\n",
    "version = \"two-step\"\n",
    "\n",
    "#Setting the output format as a list of list\n",
    "output_format = \"lol\"\n",
    "\n",
    "\n",
    "#The text file to the prompt\n",
    "f = open(\"prompts/prompt_inter-team_version_specific_lol\", \"r\")\n",
    "prompt = f.read().rstrip()\n",
    "\n",
    "meta = pd.read_csv(\"../data/raw/team_meta.csv\")\n",
    "\n",
    "batches = os.listdir(\"../data/processed/text_batches_inter-team_collab/\") #all_batches_inter-team_fuzzy_select_freq/\")\n",
    "batches = [batch.replace(\".csv\", \"\") for batch in batches]\n",
    "batches = sorted(batches)\n",
    "\n",
    "model = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "#The level of creativity in the gpt response - for the extraction task set at 0.3\n",
    "temperature = 0.3\n",
    "\n",
    "#The number of parallel chunks to evaluate at the same time: gpt-instruct limit is 20\n",
    "jump = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3fd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_batch_request(arguments, model = \"gpt-3.5-turbo-instruct\", temperature = 0.3, output_format = \"lol\", columns = [\"team\", \"context\", \"target\"]):\n",
    "    \n",
    "    \n",
    "    chunks = arguments[0]\n",
    "    variables = arguments[1]\n",
    "    ids = arguments[2]\n",
    "    \n",
    "    #print(ids + \" here\")\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    flag = []\n",
    "\n",
    "    try:\n",
    "    \n",
    "        response = openai.Completion.create(                                \n",
    "                    prompt = chunks ,\n",
    "                    engine = model,\n",
    "                    temperature = temperature,\n",
    "                    max_tokens = 2000,  \n",
    "                    top_p = 1,\n",
    "                    frequency_penalty = 0,\n",
    "                    presence_penalty = 0,\n",
    "                    timeout = 200\n",
    "                    )\n",
    "        \n",
    "    except:\n",
    "        #print(ids)\n",
    "        return(df, 0)\n",
    "    \n",
    "    for j in range(0, len(chunks)):\n",
    "        \n",
    "        var = response.choices[j].text.strip()\n",
    "        \n",
    "        t = pd.DataFrame([[var]], columns=[\"text\"]) \n",
    "                \n",
    "        for key, value  in variables.items():\n",
    "            t[key] = value[j]\n",
    "                    \n",
    "        df = df.append(t)\n",
    "    \n",
    "    return(df, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bea12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7786ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0507b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started batch 1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "Completed batch 1 in 49.08243250846863 seconds\n",
      "Started batch 2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "Completed batch 2 in 24.289158582687378 seconds\n"
     ]
    }
   ],
   "source": [
    "#Token limit = 1,000,000 per minute (Tier 4 account)\n",
    "#Each chunk = 500 tokens (max)\n",
    "#Chunk + Prompt ~ 1000 tokens\n",
    "#Expected output token size ~ 1000 tokens (max)\n",
    "\n",
    "#Each request = 2000 tokens. Max requests per minute = 200 \n",
    "#Number of requests allowed by gpt-instruct in parallel = 20 (20*2000 = 40000 tokens)\n",
    "#Using multiprocessing - starting with 8 processes (GPT limit not breached)\n",
    "\n",
    "batches = [\"1\", \"2\"]\n",
    "\n",
    "for batch_no in batches:\n",
    "    \n",
    "    if (not os.path.isfile(\"../data/processed/gpt_curated/inter-team/\" + batch_no + \"_\" + version + \"_\" + output_format + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\")):\n",
    "\n",
    "        print(\"Started batch \" + batch_no)\n",
    "        start = time.time()\n",
    "\n",
    "        batch = pd.read_csv(\"../data/processed/text_batches_inter-team_collab/\" + batch_no + \".csv\")\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        all_messages = []\n",
    "\n",
    "        batch['processed_text'] =  prompt + \"We are team \" + batch['team'] + \"\\n.\" + \"The following text describes our activities in a scientific competition called iGEM including our relationships with other teams:\" +  batch['text']\n",
    "        \n",
    "        for i in range(0,len(batch), jump):\n",
    "            \n",
    "            temp = batch['processed_text'][i:min(i+jump,len(batch))]\n",
    "            temp = list(temp)\n",
    "            all_messages.extend([[temp, {\"source_team\": list(batch['team'][i:min(i+jump,len(batch))]), \"year\": list(batch['year'][i:min(i+jump,len(batch))])}, i]])\n",
    "         \n",
    "        for message in all_messages:\n",
    "            \n",
    "            output, miss = send_batch_request(message)\n",
    "            print(miss)\n",
    "            df = df.append(output)\n",
    "            time.sleep(0.01)\n",
    "        \n",
    "        #After the new updates - parallel processing seems to falter with the complete endpoint, with many chunks not executed\n",
    "        \n",
    "        \"\"\"with Pool(processes = 8) as pool:\n",
    "            \n",
    "            zipped_output = pool.map(send_batch_request, all_messages)\n",
    "                \n",
    "            for output, miss in list(zipped_output):    \n",
    "                df = df.append(output)\n",
    "                print(miss)\n",
    "                \n",
    "            time.sleep(0.01)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        end = time.time()\n",
    "        time_elapsed = (end-start)\n",
    "\n",
    "        stats = stats.append(pd.DataFrame([[batch_no, time_elapsed]], columns=[\"batch_no\", \"time_secs\"]))\n",
    "\n",
    "        print(\"Completed batch \" + batch_no + \" in \" + str(time_elapsed) + \" seconds\")\n",
    "\n",
    "        df.to_csv(\"../data/processed/gpt_curated/inter-team/\" + batch_no + \"_\" + version + \"_\" + model + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\", index=False)\n",
    "        time.sleep(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be003aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031fdaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b91745c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b488b819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relationships: [[\"CCU_Taiwan\",\"collaborated\",\"BIT iGEM Team\"],[\"CCU_Taiwan\",\"collaborated\",\"MINGDAO iGEM team\"],[\"CCU_Taiwan\",\"exchanged ideas with\",\"BIT iGEM Team\"],[\"CCU_Taiwan\",\"exchanged ideas with\",\"MINGDAO iGEM team\"],[\"BIT iGEM Team\",\"provided suggestions to\",\"CCU_Taiwan\"],[\"MINGDAO iGEM team\",\"held debate with\",\"CCU_Taiwan\"],[\"MINGDAO iGEM team\",\"provided venue for debate\",\"CCU_Taiwan\"]]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"../data/processed/curated/fuzzy_select_inter_team_raw/stats_\" + version + \"_\" + output_format + \"_\" + str(temperature).replace(\".\",\"-\") + \".csv\"\n",
    "\n",
    "if os.path.isfile(fn):\n",
    "    \n",
    "    stats_old = pd.read_csv(fn)\n",
    "    stats = stats_old.append(stats)\n",
    "    \n",
    "stats.to_csv(fn, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df97a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157fc310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e490b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
